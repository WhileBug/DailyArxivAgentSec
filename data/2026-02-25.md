<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 53]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.AI](#cs.AI) [Total: 46]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ReportLogic: Evaluating Logical Quality in Deep Research Reports](https://arxiv.org/abs/2602.18446)
*Jujia Zhao,Zhaoxin Huan,Zihan Wang,Xiaolu Zhang,Jun Zhou,Suzan Verberne,Zhaochun Ren*

Main category: cs.CL

TL;DR: ReportLogic：一个评估LLM生成报告逻辑质量的基准，通过可审计性视角量化报告级逻辑质量，包含宏观、阐述和结构三个逻辑层次，并训练了开源LogicJudge进行可扩展评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估框架主要关注报告的流畅性和信息量，而忽视了逻辑质量这一关键要求。用户依赖LLM进行深度研究，需要报告具有明确的逻辑支持关系，确保结论可验证且可信，能够作为下游应用的基础。现有评估方法无法满足这一需求。

Method: 提出ReportLogic基准，采用分层分类法评估报告逻辑质量：1) 宏观逻辑：报告结构是否具有统一的分析弧线；2) 阐述逻辑：读者能否理解进展并获取必要背景；3) 结构逻辑：结论能否通过明确的声明-支持关系进行验证。基于此分类法构建人工标注的规则指导数据集，并训练开源LogicJudge进行可扩展评估。通过对抗性攻击评估评估者的鲁棒性。

Result: 研究发现现成的LLM评估者经常受到表面线索（如冗长）的影响，推理模式可能掩盖断裂的支持关系。训练的开源LogicJudge能够更可靠地评估逻辑质量。研究结果为构建更鲁棒的逻辑评估器和提高LLM生成报告的逻辑可靠性提供了可操作的指导。

Conclusion: ReportLogic基准填补了当前评估框架在逻辑质量评估方面的空白，通过读者中心的可审计性视角量化报告级逻辑质量。研究揭示了现有LLM评估者在逻辑评估中的局限性，并提供了改进逻辑评估器和提高LLM生成报告逻辑可靠性的具体路径。

Abstract: Users increasingly rely on Large Language Models (LLMs) for Deep Research, using them to synthesize diverse sources into structured reports that support understanding and action. In this context, the practical reliability of such reports hinges on logical quality: whether the report's claims and arguments are explicitly supported and can be trusted as a basis for downstream use, rather than merely appearing fluent or informative. However, current evaluation frameworks largely overlook this requirement. To bridge this gap, we introduce ReportLogic, a benchmark that quantifies report-level logical quality through a reader-centric lens of auditability. Specifically, ReportLogic adopts a hierarchical taxonomy that evaluates whether readers can (1) trace an on-topic report structure with a unified analytical arc (Macro-Logic), (2) understand the progression with necessary context (Expositional-Logic), and (3) verify conclusions via explicit claim--support (Structural-Logic). Based on this taxonomy, we construct a human-annotated rubric-guided dataset and train an open-source LogicJudge for scalable evaluation. We further evaluate judge robustness via adversarial attacks, showing that off-the-shelf LLM judges are frequently influenced by superficial cues (e.g., verbosity), and reasoning modes can mask broken support relations. Overall, our results provide actionable guidance for building more robust logic evaluators and improving the logical reliability of LLM-generated reports.

</details>


### [2] [ConfSpec: Efficient Step-Level Speculative Reasoning via Confidence-Gated Verification](https://arxiv.org/abs/2602.18447)
*Siran Liu,Cyril Y. He*

Main category: cs.CL

TL;DR: ConfSpec：基于置信度门控的级联验证框架，通过小型草稿模型进行高置信度推理步骤验证，选择性升级不确定案例到大型目标模型，解决推理速度与准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理显著提升大语言模型在复杂任务上的性能，但长生成轨迹导致高推理延迟。现有的步骤级推测推理方法面临准确性、推理速度和资源效率之间的长期权衡。

Method: 提出ConfSpec置信度门控级联验证框架：利用生成与验证的不对称性，小型草稿模型在其能力范围内对步骤级验证任务具有良好校准性，高置信度的草稿决策可直接接受，不确定案例选择性升级到大型目标模型验证。

Result: 在多样化工作负载评估中，ConfSpec实现最高2.24倍端到端加速，同时匹配目标模型准确性。无需外部判断模型，且与令牌级推测解码正交，可实现进一步乘法加速。

Conclusion: ConfSpec通过置信度门控级联验证框架有效解决了步骤级推测推理中准确性、速度和资源效率的权衡问题，为链式思维推理提供了高效且准确的加速方案。

Abstract: Chain-of-Thought reasoning significantly improves the performance of large language models on complex tasks, but incurs high inference latency due to long generation traces. Step-level speculative reasoning aims to mitigate this cost, yet existing approaches face a long-standing trade-off among accuracy, inference speed, and resource efficiency. We propose ConfSpec, a confidence-gated cascaded verification framework that resolves this trade-off. Our key insight is an asymmetry between generation and verification: while generating a correct reasoning step requires substantial model capacity, step-level verification is a constrained discriminative task for which small draft models are well-calibrated within their competence range, enabling high-confidence draft decisions to be accepted directly while selectively escalating uncertain cases to the large target model. Evaluation across diverse workloads shows that ConfSpec achieves up to 2.24$\times$ end-to-end speedups while matching target-model accuracy. Our method requires no external judge models and is orthogonal to token-level speculative decoding, enabling further multiplicative acceleration.

</details>


### [3] [Prompt Optimization Via Diffusion Language Models](https://arxiv.org/abs/2602.18449)
*Shiyu Wang,Haolin Chen,Liangwei Yang,Jielin Qiu,Rithesh Murthy,Ming Zhu,Zixiang Chen,Silvio Savarese,Caiming Xiong,Shelby Heinecke,Huan Wang*

Main category: cs.CL

TL;DR: 提出基于扩散语言模型的提示优化框架，通过掩码去噪迭代优化系统提示，无需梯度访问或修改下游语言模型


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法通常需要梯度访问或模型微调，限制了其通用性和可扩展性。需要一种模型无关、无需修改目标模型的方法来优化系统提示

Method: 使用扩散语言模型进行迭代提示优化：1) 基于交互轨迹（用户查询、模型响应、可选反馈）进行条件化；2) 通过掩码去噪实现灵活的span级提示更新；3) 无需梯度访问或修改下游语言模型；4) 探索不同扩散步数以平衡优化质量与稳定性

Result: 在多个基准测试（τ-bench、SST-2、SST-5）上，DLM优化的提示能持续提升冻结目标LLM（如GPT-4o-mini）的性能。适中的扩散步数在优化质量和稳定性之间达到最佳平衡

Conclusion: 扩散基提示优化是一种通用、模型无关且可扩展的方法，通过迭代提示精炼来增强LLM性能，为无需模型修改的提示优化提供了新途径

Abstract: We propose a diffusion-based framework for prompt optimization that leverages Diffusion Language Models (DLMs) to iteratively refine system prompts through masked denoising. By conditioning on interaction traces, including user queries, model responses, and optional feedback, our method enables flexible, span-level prompt updates without requiring gradient access or modifying the downstream language model. Across diverse benchmarks (e.g., $τ$-bench, SST-2, SST-5), DLM-optimized prompts consistently improve the performance of a frozen target LLM (e.g., GPT-4o-mini). We further show that moderate diffusion step counts provide the best balance between refinement quality and stability. These results highlight diffusion-based prompt optimization as a general, model-agnostic, and scalable approach for enhancing LLM performance through iterative prompt refinement.

</details>


### [4] [Asymptotic Semantic Collapse in Hierarchical Optimization](https://arxiv.org/abs/2602.18450)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.CL

TL;DR: 研究多智能体语言系统中的渐进语义坍缩现象，即共享主导语境逐渐吸收个体语义，导致智能体行为趋同


<details>
  <summary>Details</summary>
Motivation: 研究多智能体语言系统中存在的一种失效模式：共享主导语境渐进吸收个体语义，导致智能体行为几乎完全一致。这种现象被称为层次优化中的渐进语义坍缩

Method: 1. 在封闭语言环境中建立模型，包含具有无限语义惯性的主导锚节点和外围智能体节点
2. 将语义状态建模为黎曼流形上的点，分析诱导的投影动力学
3. 研究优化历史对最终语义配置的影响
4. 分析语境依赖程度对信息内容的影响
5. 使用RWKV-7 13B GGUF检查点进行轻量级无数据集基准测试

Result: 1. 极限语义配置对优化历史不敏感：无论是平滑梯度更新还是随机噪声更新都收敛到相同的拓扑端点，建立了收敛时的路径独立性
2. 语境依赖程度控制信息内容：从原子（独立）表示到完全纠缠（语境绑定）表示，节点熵（解释为可用自由度）在极限中趋于零
3. 基准测试结果：零哈希碰撞，贪婪解码的平均依从度为0.50，随机解码为0.531，最终Jaccard相似度分别为0.295和0.224

Conclusion: 该理论将信息论量与微分几何结构联系起来，并解释为一种不可变的共识规则，将智能体约束在共享语义语法中。渐进语义坍缩现象揭示了多智能体语言系统中语义多样性的丧失机制

Abstract: Multi-agent language systems can exhibit a failure mode where a shared dominant context progressively absorbs individual semantics, yielding near-uniform behavior across agents. We study this effect under the name Asymptotic Semantic Collapse in Hierarchical Optimization. In a closed linguistic setting with a Dominant Anchor Node whose semantic state has effectively infinite inertia, we show that repeated interactions with Peripheral Agent Nodes drive an asymptotic alignment that minimizes a global loss. We model semantic states as points on a Riemannian manifold and analyze the induced projection dynamics. Two consequences follow. First, the limiting semantic configuration is insensitive to the optimization history: both smooth gradient-style updates and stochastic noisy updates converge to the same topological endpoint, establishing path independence at convergence. Second, the degree of context dependence controls information content: moving from atomic (independent) representations to fully entangled (context-bound) representations forces the node entropy, interpreted as available degrees of freedom, to vanish in the limit. The theory connects information-theoretic quantities with differential-geometric structure and suggests an interpretation as an immutable consensus rule that constrains agents to a shared semantic grammar. A lightweight dataset-free benchmark on an RWKV-7 13B GGUF checkpoint complements the analysis, reporting zero hash collisions, mean compliance of 0.50 under greedy decoding and 0.531 under stochastic decoding, and final Jaccard-to-anchor similarity values of 0.295 and 0.224, respectively.

</details>


### [5] [The Million-Label NER: Breaking Scale Barriers with GLiNER bi-encoder](https://arxiv.org/abs/2602.18487)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov*

Main category: cs.CL

TL;DR: GLiNER-bi-Encoder是一种新型命名实体识别架构，通过双编码器设计将标签编码与上下文编码解耦，解决了原始GLiNER联合编码的二次复杂度问题，实现了工业级效率的零样本实体识别。


<details>
  <summary>Details</summary>
Motivation: 原始GLiNER框架虽然具有强大的泛化能力，但其联合编码方法在实体标签数量增加时面临二次复杂度问题，限制了在工业规模应用中的效率。

Method: 提出双编码器架构，将过程解耦为专用标签编码器和上下文编码器，消除了上下文窗口瓶颈。通过预计算标签嵌入实现高效推理，并基于此架构开发了GLiNKER框架用于大规模知识库的实体链接。

Result: 在CrossNER基准测试中达到61.5%的Micro-F1分数，实现最先进的零样本性能。在1024个标签时，相比单编码器前身实现了高达130倍的吞吐量提升。

Conclusion: GLiNER-bi-Encoder成功地将零样本灵活性与工业级效率相结合，能够同时识别数千甚至数百万个实体类型，为大规模知识库实体链接提供了高效解决方案。

Abstract: This paper introduces GLiNER-bi-Encoder, a novel architecture for Named Entity Recognition (NER) that harmonizes zero-shot flexibility with industrial-scale efficiency. While the original GLiNER framework offers strong generalization, its joint-encoding approach suffers from quadratic complexity as the number of entity labels increases. Our proposed bi-encoder design decouples the process into a dedicated label encoder and a context encoder, effectively removing the context-window bottleneck. This architecture enables the simultaneous recognition of thousands, and potentially millions, of entity types with minimal overhead. Experimental results demonstrate state-of-the-art zero-shot performance, achieving 61.5 percent Micro-F1 on the CrossNER benchmark. Crucially, by leveraging pre-computed label embeddings, GLiNER-bi-Encoder achieves up to a 130 times throughput improvement at 1024 labels compared to its uni-encoder predecessors. Furthermore, we introduce GLiNKER, a modular framework that leverages this architecture for high-performance entity linking across massive knowledge bases such as Wikidata.

</details>


### [6] [DP-RFT: Learning to Generate Synthetic Text via Differentially Private Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.18633)
*Fangyuan Xu,Sihao Chen,Zinan Lin,Taiwei Shi,Sydney Graham,Pei Zhou,Mengting Wan,Alex Stein,Virginia Estellers,Charles Chen,Morris Sharp,Richard Speyer,Tadas Baltrusaitis,Jennifer Neville,Eunsol Choi,Longqi Yang*

Main category: cs.CL

TL;DR: DP-RFT是一种差分隐私强化学习算法，利用隐私保护的最近邻投票作为奖励信号，训练LLM生成高质量合成文本，无需直接访问原始私有数据。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私合成数据生成面临两难：DP微调方法需要访问原始私有数据内容，而避免直接暴露的方法受限于未经微调的现成模型，其输出缺乏领域保真度。需要一种既能保护隐私又能生成高质量合成文本的方法。

Method: 提出DP-RFT（差分隐私强化微调），这是一种在线强化学习算法。使用DP保护的最近邻投票（来自"eyes-off"私有语料库）作为LLM生成合成样本的奖励信号。通过近端策略优化（PPO）迭代训练LLM，使其生成的合成数据最大化预期的DP投票。

Result: 实验评估了长文本和领域特定合成数据生成（如新闻文章、会议记录、医学摘要）。DP-RFT在保真度和下游效用方面缩小了私有评估与DP微调方法之间的差距，同时尊重私有数据边界。

Conclusion: DP-RFT能够在无需直接访问个体私有示例的情况下，训练LLM生成高质量的差分隐私合成文本，解决了现有方法在隐私保护和数据质量之间的权衡问题。

Abstract: Differentially private (DP) synthetic data generation plays a pivotal role in developing large language models (LLMs) on private data, where data owners cannot provide eyes-on access to individual examples. Generating DP synthetic data typically involves a difficult trade-off. On one hand, DP finetuning methods train an LLM as a synthetic data generator with formal privacy guarantees, yet it still requires the raw content of private examples for model training. However, methods that avoid direct exposure to private data are bounded by an off-the-shelf, un-finetuned model, whose outputs often lack domain fidelity. Can we train an LLM to generate high-quality synthetic text without eyes-on access to individual private examples? In this work, we introduce Differentially Private Reinforcement Fine-Tuning (DP-RFT), an online reinforcement learning algorithm for synthetic data generation with LLMs. DP-RFT leverages DP-protected nearest-neighbor votes from an eyes-off private corpus as a reward signal for on-policy synthetic samples generated by an LLM. The LLM iteratively learns to generate synthetic data to maximize the expected DP votes through Proximal Policy Optimization (PPO). We evaluate DP-RFT for long-form and domain-specific synthetic data generation, such as news articles, meeting transcripts, and medical article abstracts. Our experiments show that DP-RFT closes the gap between private evolution and DP finetuning methods in terms of the fidelity and downstream utility of the generated synthetic data, while respecting the private data boundary.

</details>


### [7] [PolyFrame at MWE-2026 AdMIRe 2: When Words Are Not Enough: Multimodal Idiom Disambiguation](https://arxiv.org/abs/2602.18652)
*Nina Hosseini-Kivanani*

Main category: cs.CL

TL;DR: PolyFrame系统通过轻量级模块增强多模态模型对习语的理解能力，在15种语言的多模态习语消歧任务中取得显著效果，无需微调大型多模态编码器。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在处理非组合性含义的习语表达时存在困难，特别是在多语言环境下，这一问题更加突出。需要开发有效的方法来解决多模态习语消歧问题。

Method: 提出PolyFrame统一流水线，包含冻结的CLIP风格视觉-语言编码器和多语言BGE M3编码器，仅训练轻量级模块：逻辑回归和LLM句子类型预测器、习语同义词替换、干扰项感知评分和Borda排序融合。

Result: 从CLIP基线（英语开发集Top-1 26.7%，英语测试集6.7%）提升至英语Top-1 60.0%，葡萄牙语零样本迁移Top-1 60.0%（NDCG@5 0.822）。在15种语言的多模态盲测中，Subtask A平均Top-1/NDCG为0.35/0.73，Subtask B为0.32/0.71。

Conclusion: 习语感知重写是性能提升的主要贡献者，句子类型预测和多模态融合增强了鲁棒性。研究表明无需微调大型多模态编码器即可实现有效的习语消歧。

Abstract: Multimodal models struggle with idiomatic expressions due to their non-compositional meanings, a challenge amplified in multilingual settings. We introduced PolyFrame, our system for the MWE-2026 AdMIRe2 shared task on multimodal idiom disambiguation, featuring a unified pipeline for both image+text ranking (Subtask A) and text-only caption ranking (Subtask B). All model variants retain frozen CLIP-style vision--language encoders and the multilingual BGE M3 encoder, training only lightweight modules: a logistic regression and LLM-based sentence-type predictor, idiom synonym substitution, distractor-aware scoring, and Borda rank fusion. Starting from a CLIP baseline (26.7% Top-1 on English dev, 6.7% on English test), adding idiom-aware paraphrasing and explicit sentence-type classification increased performance to 60.0% Top-1 on English and 60.0% Top-1 (0.822 NDCG@5) in zero-shot transfer to Portuguese. On the multilingual blind test, our systems achieved average Top-1/NDCG scores of 0.35/0.73 for Subtask A and 0.32/0.71 for Subtask B across 15 languages. Ablation results highlight idiom-aware rewriting as the main contributor to performance, while sentence-type prediction and multimodal fusion enhance robustness. These findings suggest that effective idiom disambiguation is feasible without fine-tuning large multimodal encoders.

</details>


### [8] [From Trial by Fire To Sleep Like a Baby: A Lexicon of Anxiety Associations for 20k English Multiword Expressions](https://arxiv.org/abs/2602.18692)
*Saif M. Mohammad*

Main category: cs.CL

TL;DR: 构建首个大规模英语多词表达焦虑关联词典，包含2万+多词表达，研究焦虑关联的可靠性、不同类型焦虑/平静表达分布、组合性，支持心理学、NLP、公共卫生等领域研究。


<details>
  <summary>Details</summary>
Motivation: 焦虑作为对可能负面结果的不安，与健康、福祉、行为等密切相关。现有研究主要集中在词汇层面的焦虑关联，但对更大文本单位如多词表达的研究很少，缺乏相关资源。

Method: 构建首个大规模英语多词表达焦虑关联词典，包含超过2万个多词表达。通过描述性规范方法收集焦虑关联数据，评估可靠性，分析不同类型焦虑/平静表达在不同长度序列中的分布，研究多词表达焦虑关联的组合性程度。

Result: 焦虑关联数据具有高度可靠性。分析了不同类型焦虑和冷静相关多词表达的流行程度及其在二词、三词、四词序列中的变化。研究发现多词表达的焦虑关联具有一定的组合性，但并非完全由其组成词汇决定。

Conclusion: 该词典填补了多词表达层面焦虑关联研究的空白，为心理学、自然语言处理、公共卫生和社会科学等领域的焦虑相关研究提供了重要资源，支持广泛的应用研究。

Abstract: Anxiety is the unease about a possible future negative outcome. In recent years, there has been growing interest in understanding how anxiety relates to our health, well-being, body, mind, and behaviour. This includes work on lexical resources for word-anxiety association. However, there is very little anxiety-related work on larger units of text such as multiword expressions (MWE). Here, we introduce the first large-scale lexicon capturing descriptive norms of anxiety associations for more than 20k English MWEs. We show that the anxiety associations are highly reliable. We use the lexicon to study prevalence of different types of anxiety- and calmness-associated MWEs; and how that varies across two-, three-, and four-word sequences. We also study the extent to which the anxiety association of MWEs is compositional (due to its constituent words). The lexicon enables a wide variety of anxiety-related research in psychology, NLP, public health, and social sciences. The lexicon is freely available: https://saifmohammad.com/worrylex.html

</details>


### [9] [Contradiction to Consensus: Dual Perspective, Multi Source Retrieval Based Claim Verification with Source Level Disagreement using LLM](https://arxiv.org/abs/2602.18693)
*Md Badsha Biswas,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 提出基于大语言模型的多视角证据检索与跨源分歧分析的开放领域声明验证系统，通过聚合多个知识源证据提升验证性能与透明度


<details>
  <summary>Details</summary>
Motivation: 现有自动声明验证系统通常依赖单一知识源，忽略不同知识源之间的分歧，导致知识覆盖有限且透明度不足，无法反映真实世界信息的复杂性

Method: 提出开放领域声明验证系统，采用新颖检索策略同时收集原始声明及其否定形式的证据，从Wikipedia、PubMed和Google等多源获取支持与矛盾信息；对证据进行过滤、去重和跨源聚合形成统一知识库；利用LLMs进行声明验证，并通过模型置信度分析量化可视化跨源分歧

Result: 在四个基准数据集和五个LLMs上的广泛评估表明，知识聚合不仅提升了声明验证性能，还揭示了不同知识源在推理上的差异

Conclusion: 证据的多样性、矛盾性和聚合对于构建可靠透明的声明验证系统至关重要，多源知识聚合能更好地反映真实世界信息的复杂性

Abstract: The spread of misinformation across digital platforms can pose significant societal risks. Claim verification, a.k.a. fact-checking, systems can help identify potential misinformation. However, their efficacy is limited by the knowledge sources that they rely on. Most automated claim verification systems depend on a single knowledge source and utilize the supporting evidence from that source; they ignore the disagreement of their source with others. This limits their knowledge coverage and transparency. To address these limitations, we present a novel system for open-domain claim verification (ODCV) that leverages large language models (LLMs), multi-perspective evidence retrieval, and cross-source disagreement analysis. Our approach introduces a novel retrieval strategy that collects evidence for both the original and the negated forms of a claim, enabling the system to capture supporting and contradicting information from diverse sources: Wikipedia, PubMed, and Google. These evidence sets are filtered, deduplicated, and aggregated across sources to form a unified and enriched knowledge base that better reflects the complexity of real-world information. This aggregated evidence is then used for claim verification using LLMs. We further enhance interpretability by analyzing model confidence scores to quantify and visualize inter-source disagreement. Through extensive evaluation on four benchmark datasets with five LLMs, we show that knowledge aggregation not only improves claim verification but also reveals differences in source-specific reasoning. Our findings underscore the importance of embracing diversity, contradiction, and aggregation in evidence for building reliable and transparent claim verification systems

</details>


### [10] [Semantic Substrate Theory: An Operator-Theoretic Framework for Geometric Semantic Drift](https://arxiv.org/abs/2602.18699)
*Stephen Russell*

Main category: cs.CL

TL;DR: 提出一个统一框架将语义漂移的多种信号（嵌入位移、邻居变化、分布差异、递归轨迹不稳定）形式化为单一时间索引基板，包含几何嵌入与局部扩散，引入桥质量作为未来邻居重连的预测指标。


<details>
  <summary>Details</summary>
Motivation: 现有语义漂移研究缺乏统一解释理论，多种信号（嵌入位移、邻居变化、分布差异、递归轨迹不稳定）各自独立报告，需要共享的理论框架来关联这些信号。

Method: 提出形式化框架 $S_t=(X,d_t,P_t)$，结合嵌入几何与局部扩散。定义节点级邻域漂移（局部条件分布变化）、粗Ricci曲率（语义扩散局部收缩性）、递归漂移（迭代语义算子稳定性）。引入桥质量作为入射负曲率的节点级聚合，用于预测未来邻居重连。

Result: 提供了理论框架和可证伪的测试合约，建立了语义漂移信号的统一形式化模型，但实证性能评估留待后续研究。

Conclusion: 提出了一个统一的语义漂移理论框架，将多种观测信号关联起来，为语义变化研究提供了形式化基础，并提出了可测试的预测指标（桥质量），为后续实证研究奠定了基础。

Abstract: Most semantic drift studies report multiple signals e.g., embedding displacement, neighbor changes, distributional divergence, and recursive trajectory instability, without a shared explanatory theory that relates them. This paper proposes a formalization of these signals in one time-indexed substrate, $S_t=(X,d_t,P_t)$, combining embedding geometry with local diffusion. Within this substrate, node-level neighborhood drift measures changes in local conditional distributions, coarse Ricci curvature measures local contractivity of semantic diffusion, and recursive drift probes stability of iterated semantic operators. This manuscript specifies the formal model, assumptions, and tests that can refute the model. Herein, the paper introduces bridge mass, a node-level aggregate of incident negative curvature, as a predictor of future neighborhood rewiring. This paper provides the theory and test contracts; empirical performance is deferred to subsequent studies.

</details>


### [11] [ReHear: Iterative Pseudo-Label Refinement for Semi-Supervised Speech Recognition via Audio Large Language Models](https://arxiv.org/abs/2602.18721)
*Zefang Liu,Chenyang Zhu,Sangwoo Cho,Shi-Xiong Zhang*

Main category: cs.CL

TL;DR: ReHear：一种用于ASR半监督学习的迭代伪标签精炼框架，通过集成指令调优的音频感知大语言模型来纠正伪标签错误，缓解确认偏差和错误传播问题。


<details>
  <summary>Details</summary>
Motivation: 传统ASR半监督学习中的伪标签方法存在确认偏差和错误累积问题，因为噪声监督会导致错误传播。需要一种能够纠正伪标签错误、提高监督质量的机制。

Method: 提出ReHear框架，在自训练循环中集成指令调优的音频感知大语言模型。该模型同时以ASR假设和源音频为条件，能够从严重识别错误中恢复语音准确的转录文本。精炼后的伪标签作为高质量目标用于迭代微调ASR模型。

Result: 在多个基准测试上的实验结果表明，ReHear能有效缓解错误传播，在监督学习和伪标签基线方法上均取得一致性的性能提升。

Conclusion: ReHear通过集成音频感知LLM进行迭代伪标签精炼，为ASR半监督学习提供了一种有效解决方案，能够显著改善伪标签质量并提升模型性能。

Abstract: Semi-supervised learning in automatic speech recognition (ASR) typically relies on pseudo-labeling, which often suffers from confirmation bias and error accumulation due to noisy supervision. To address this limitation, we propose ReHear, a framework for iterative pseudo-label refinement that integrates an instruction-tuned, audio-aware large language model (LLM) into the self-training loop. Unlike conventional text-based correctors, our approach conditions the LLM on both the ASR hypothesis and the source audio, allowing it to recover phonetically accurate transcripts even from severe recognition errors. These refined pseudo-labels serve as high-fidelity targets for fine-tuning the ASR model in an iterative cycle. Experimental results across diverse benchmarks demonstrate that ReHear effectively mitigates error propagation, consistently outperforming both supervised and pseudo-labeling baselines.

</details>


### [12] [Rethinking Retrieval-Augmented Generation as a Cooperative Decision-Making Problem](https://arxiv.org/abs/2602.18734)
*Lichang Song,Ting Long,Yi Chang*

Main category: cs.CL

TL;DR: CoRAG将RAG重新定义为协作多智能体决策问题，通过让重排序器和生成器作为对等决策者协同工作，而非传统的不对称依赖管道，从而提升生成稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统大多基于排名中心的不对称依赖范式，生成器的质量高度依赖重排序器的结果，这种局限性需要克服。作者希望打破这种不对称依赖，让重排序和生成能够协同工作。

Method: 提出协作检索增强生成（CoRAG）框架，将RAG重新表述为协作多智能体决策问题。重排序器和生成器作为对等决策者，通过联合优化它们的行为来实现共享任务目标，鼓励两者协作，确保文档重排序和生成协同工作以改进最终响应。

Result: 实验结果表明CoRAG具有良好的泛化能力和改进的生成稳定性，即使在仅使用约10K PopQA样本进行训练的情况下也能取得良好效果。模型已在指定链接中发布。

Conclusion: CoRAG通过将RAG重新定义为协作多智能体决策问题，成功克服了传统RAG系统中重排序器和生成器之间的不对称依赖限制，实现了更好的协同工作和性能提升。

Abstract: Retrieval-Augmented Generation (RAG) has demonstrated strong effectiveness in knowledge-intensive tasks by grounding language generation in external evidence. Despite its success, many existing RAG systems are built based on a ranking-centric, asymmetric dependency paradigm, where the generation quality of the generator is highly dependent on reranking results of the reranker. To overcome this limitation, we reformulate RAG as a cooperative multi-agent decision-making problem and propose Cooperative Retrieval-Augmented Generation (CoRAG), a framework in which the reranker and the generator act as peer decision-makers rather than being connected through an asymmetric dependency pipeline. By jointly optimizing their behaviors toward a shared task objective, the reranker and generator are encouraged to cooperate, ensuring that document reranking and generation work in concert to improve the final response. Experimental results demonstrate good generalization and improved generation stability of CoRAG, even when the model is trained on only around 10K PopQA samples. Our model released in https://anonymous.4open.science/r/CoRAG-D63F

</details>


### [13] [ArabicNumBench: Evaluating Arabic Number Reading in Large Language Models](https://arxiv.org/abs/2602.18776)
*Anas Alhumud,Abdulaziz Alhammadi,Muhammad Badruddin Khan*

Main category: cs.CL

TL;DR: ArabicNumBench是一个用于评估大语言模型在阿拉伯数字阅读任务上的综合基准，涵盖东方阿拉伯-印度数字和西方阿拉伯数字，通过多种提示策略在210个任务上测试71个模型，发现数值准确性和指令遵循是两种不同的能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在阿拉伯数字处理方面的能力评估不足，特别是对阿拉伯语数字阅读任务缺乏系统评估。需要建立基准来评估模型在阿拉伯数字理解、结构化输出生成和指令遵循方面的表现，为阿拉伯语NLP系统的模型选择提供指导。

Method: 构建包含210个阿拉伯数字阅读任务的基准，涵盖纯数字、地址、日期、数量、价格等六个上下文类别。评估71个模型使用四种提示策略（零样本、零样本思维链、少样本、少样本思维链），共59,010个测试用例。跟踪提取方法以测量结构化输出生成。

Result: 模型性能差异显著，准确率从14.29%到99.05%不等。少样本思维链提示比零样本方法准确率高2.8倍（80.06% vs 28.76%）。关键发现：高准确率模型（98-99%）通常产生非结构化输出，缺乏阿拉伯语思维链标记。仅6个模型在所有测试用例中一致生成结构化输出，多数模型需要备用提取方法。

Conclusion: 数值准确性和指令遵循是两种不同的能力，需要分别评估。该基准为阿拉伯数字理解建立了基线，为生产环境中的阿拉伯语NLP系统模型选择提供了可操作的指导。模型在数值准确性上的表现不一定反映其指令遵循和结构化输出生成能力。

Abstract: We present ArabicNumBench, a comprehensive benchmark for evaluating large language models on Arabic number reading tasks across Eastern Arabic-Indic numerals (0-9 in Arabic script) and Western Arabic numerals (0-9). We evaluate 71 models from 10 providers using four prompting strategies (zero-shot, zero-shot CoT, few-shot, few-shot CoT) on 210 number reading tasks spanning six contextual categories: pure numerals, addresses, dates, quantities, and prices. Our evaluation comprises 59,010 individual test cases and tracks extraction methods to measure structured output generation. Evaluation reveals substantial performance variation, with accuracy ranging from 14.29\% to 99.05\% across models and strategies. Few-shot Chain-of-Thought prompting achieves 2.8x higher accuracy than zero-shot approaches (80.06\% vs 28.76\%). A striking finding emerges: models achieving elite accuracy (98-99\%) often produce predominantly unstructured output, with most responses lacking Arabic CoT markers. Only 6 models consistently generate structured output across all test cases, while the majority require fallback extraction methods despite high numerical accuracy. Comprehensive evaluation of 281 model-strategy combinations demonstrates that numerical accuracy and instruction-following represent distinct capabilities, establishing baselines for Arabic number comprehension and providing actionable guidance for model selection in production Arabic NLP systems.

</details>


### [14] [BURMESE-SAN: Burmese NLP Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2602.18788)
*Thura Aung,Jann Railey Montalan,Jian Gang Ngui,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: BURMESE-SAN是首个系统评估缅语大语言模型在理解、推理和生成三大核心NLP能力上的综合基准，包含7个子任务，采用本地化构建方法，评估显示缅语性能更多取决于架构设计、语言表示和指令微调而非模型规模。


<details>
  <summary>Details</summary>
Motivation: 缅语作为低资源语言，缺乏系统性的评估基准来全面衡量大语言模型在理解、推理和生成方面的能力。现有基准要么不完整，要么存在翻译引入的伪影，无法准确反映模型在真实缅语环境下的表现。

Method: 1. 构建包含7个子任务的综合基准：问答、情感分析、毒性检测、因果推理、自然语言推理、抽象摘要和机器翻译；2. 采用严格的本地说话者驱动流程，确保语言自然性、流畅性和文化真实性；3. 对开源和商业LLM进行大规模评估，分析缅语建模面临的挑战；4. 建立公开排行榜支持持续评估。

Result: 1. 缅语性能更多取决于架构设计、语言表示和指令微调，而非单纯模型规模；2. 东南亚区域微调和较新模型代际带来显著性能提升；3. 揭示了缅语建模面临的挑战：预训练覆盖有限、丰富的形态学和句法变异。

Conclusion: BURMESE-SAN为缅语NLP提供了首个系统性评估基准，揭示了缅语LLM性能的关键影响因素，为低资源语言建模提供了重要见解，并通过公开排行榜促进缅语及其他低资源语言的持续进步。

Abstract: We introduce BURMESE-SAN, the first holistic benchmark that systematically evaluates large language models (LLMs) for Burmese across three core NLP competencies: understanding (NLU), reasoning (NLR), and generation (NLG). BURMESE-SAN consolidates seven subtasks spanning these competencies, including Question Answering, Sentiment Analysis, Toxicity Detection, Causal Reasoning, Natural Language Inference, Abstractive Summarization, and Machine Translation, several of which were previously unavailable for Burmese. The benchmark is constructed through a rigorous native-speaker-driven process to ensure linguistic naturalness, fluency, and cultural authenticity while minimizing translation-induced artifacts. We conduct a large-scale evaluation of both open-weight and commercial LLMs to examine challenges in Burmese modeling arising from limited pretraining coverage, rich morphology, and syntactic variation. Our results show that Burmese performance depends more on architectural design, language representation, and instruction tuning than on model scale alone. In particular, Southeast Asia regional fine-tuning and newer model generations yield substantial gains. Finally, we release BURMESE-SAN as a public leaderboard to support systematic evaluation and sustained progress in Burmese and other low-resource languages. https://leaderboard.sea-lion.ai/detailed/MY

</details>


### [15] [EvalSense: A Framework for Domain-Specific LLM (Meta-)Evaluation](https://arxiv.org/abs/2602.18823)
*Adam Dejl,Jonathan Pearson*

Main category: cs.CL

TL;DR: EvalSense是一个用于构建领域特定大语言模型评估套件的灵活可扩展框架，通过交互式指南和自动元评估工具帮助用户选择合适的评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统统计指标不适用于开放式生成任务，而基于LLM的评估方法虽然更灵活，但依赖于精心选择的模型、提示、参数和评估策略，容易产生配置错误和偏见，需要更可靠的评估框架。

Method: EvalSense框架提供对多种模型提供商和评估策略的开箱即用支持，包含两个核心组件：(1)交互式指南帮助用户选择评估方法；(2)自动元评估工具使用扰动数据评估不同评估方法的可靠性。

Result: 通过在临床笔记生成案例研究中使用公开数据集，证明了EvalSense的有效性，所有代码、文档和资源均已开源。

Conclusion: EvalSense提供了一个系统化的框架来解决LLM评估中的复杂性和可靠性问题，有助于在敏感领域部署LLM时进行更稳健和全面的评估。

Abstract: Robust and comprehensive evaluation of large language models (LLMs) is essential for identifying effective LLM system configurations and mitigating risks associated with deploying LLMs in sensitive domains. However, traditional statistical metrics are poorly suited to open-ended generation tasks, leading to growing reliance on LLM-based evaluation methods. These methods, while often more flexible, introduce additional complexity: they depend on carefully chosen models, prompts, parameters, and evaluation strategies, making the evaluation process prone to misconfiguration and bias. In this work, we present EvalSense, a flexible, extensible framework for constructing domain-specific evaluation suites for LLMs. EvalSense provides out-of-the-box support for a broad range of model providers and evaluation strategies, and assists users in selecting and deploying suitable evaluation methods for their specific use-cases. This is achieved through two unique components: (1) an interactive guide aiding users in evaluation method selection and (2) automated meta-evaluation tools that assess the reliability of different evaluation approaches using perturbed data. We demonstrate the effectiveness of EvalSense in a case study involving the generation of clinical notes from unstructured doctor-patient dialogues, using a popular open dataset. All code, documentation, and assets associated with EvalSense are open-source and publicly available at https://github.com/nhsengland/evalsense.

</details>


### [16] [DeepInnovator: Triggering the Innovative Capabilities of LLMs](https://arxiv.org/abs/2602.18920)
*Tianyu Fan,Fengji Zhang,Yuxiang Zheng,Bei Chen,Xinyao Niu,Chengen Huang,Junyang Lin,Chao Huang*

Main category: cs.CL

TL;DR: DeepInnovator是一个训练框架，旨在激发大语言模型的创新能力，通过构建自动化的研究知识提取管道和"下一个想法预测"训练范式，使模型能够自主生成新颖且有价值的研究想法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖复杂的提示工程，缺乏系统化的训练范式来培养大语言模型的创新能力。需要一种能够触发模型真正创新能力的训练框架，使其能够自主生成新颖且有价值的研究想法。

Method: 方法包含两个核心组件：1)"站在巨人肩膀上"：构建自动化的数据提取管道，从大量未标注的科学文献中提取和组织结构化研究知识；2)"猜想与反驳"：引入"下一个想法预测"训练范式，将研究想法的生成建模为持续预测、评估和精炼合理且新颖的下一个想法的迭代过程。

Result: 自动评估和专家评估均表明，DeepInnovator-14B显著优于未经训练的基线模型，胜率在80.53%-93.81%之间，并且达到了与当前领先大语言模型相当的性能水平。

Conclusion: 这项工作为构建具有真正原创性创新能力的研究智能体提供了一条可扩展的训练路径，并将开源数据集以促进社区发展。DeepInnovator框架能够有效激发大语言模型的创新能力，在科学研究中具有重要应用价值。

Abstract: The application of Large Language Models (LLMs) in accelerating scientific discovery has garnered increasing attention, with a key focus on constructing research agents endowed with innovative capability, i.e., the ability to autonomously generate novel and significant research ideas. Existing approaches predominantly rely on sophisticated prompt engineering and lack a systematic training paradigm. To address this, we propose DeepInnovator, a training framework designed to trigger the innovative capability of LLMs. Our approach comprises two core components. (1) ``Standing on the shoulders of giants''. We construct an automated data extraction pipeline to extract and organize structured research knowledge from a vast corpus of unlabeled scientific literature. (2) ``Conjectures and refutations''. We introduce a ``Next Idea Prediction'' training paradigm, which models the generation of research ideas as an iterative process of continuously predicting, evaluating, and refining plausible and novel next idea. Both automatic and expert evaluations demonstrate that our DeepInnovator-14B significantly outperforms untrained baselines, achieving win rates of 80.53\%-93.81\%, and attains performance comparable to that of current leading LLMs. This work provides a scalable training pathway toward building research agents with genuine, originative innovative capability, and will open-source the dataset to foster community advancement. Source code and data are available at: https://github.com/HKUDS/DeepInnovator.

</details>


### [17] [Yor-Sarc: A gold-standard dataset for sarcasm detection in a low-resource African language](https://arxiv.org/abs/2602.18964)
*Toheeb Aduramomi Jimoh,Tabea De Wille,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 首个约鲁巴语讽刺检测数据集Yor-Sarc，包含436个实例，由三位母语者标注，考虑文化因素，达到高标注一致性（Fleiss' κ=0.7660）。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测在计算语义学中具有挑战性，需要解决字面意义和意图意义之间的差异。在低资源语言中，由于标注数据集稀缺或不存在，这一挑战更加严峻。约鲁巴语作为尼日尔-刚果语系的有声调语言，拥有超过5000万使用者，但缺乏专门的讽刺检测数据集。

Method: 创建了首个约鲁巴语讽刺检测黄金标准数据集Yor-Sarc，包含436个实例。由三位来自不同方言背景的母语者使用专门为约鲁巴语讽刺设计的标注协议进行标注，该协议考虑了文化因素，包含上下文敏感解释和社区知情指南。进行了全面的标注者间一致性分析，支持在其他非洲语言中复制。

Result: 实现了实质性到几乎完美的一致性（Fleiss' κ=0.7660；成对Cohen's κ=0.6732-0.8743），83.3%的实例达成一致共识。一对标注者达到几乎完美的一致性（κ=0.8743；原始一致性93.8%），超过了许多英语讽刺研究的基准。16.7%的多数同意案例保留为软标签，用于不确定性感知建模。

Conclusion: Yor-Sarc是首个约鲁巴语讽刺检测数据集，填补了低资源非洲语言在语义解释和文化知情NLP研究方面的空白。该数据集预计将促进低资源非洲语言的语义解释和文化知情NLP研究，其标注协议和一致性分析方法可为其他非洲语言提供参考。

Abstract: Sarcasm detection poses a fundamental challenge in computational semantics, requiring models to resolve disparities between literal and intended meaning. The challenge is amplified in low-resource languages where annotated datasets are scarce or nonexistent. We present \textbf{Yor-Sarc}, the first gold-standard dataset for sarcasm detection in Yorùbá, a tonal Niger-Congo language spoken by over $50$ million people. The dataset comprises 436 instances annotated by three native speakers from diverse dialectal backgrounds using an annotation protocol specifically designed for Yorùbá sarcasm by taking culture into account. This protocol incorporates context-sensitive interpretation and community-informed guidelines and is accompanied by a comprehensive analysis of inter-annotator agreement to support replication in other African languages. Substantial to almost perfect agreement was achieved (Fleiss' $κ= 0.7660$; pairwise Cohen's $κ= 0.6732$--$0.8743$), with $83.3\%$ unanimous consensus. One annotator pair achieved almost perfect agreement ($κ= 0.8743$; $93.8\%$ raw agreement), exceeding a number of reported benchmarks for English sarcasm research works. The remaining $16.7\%$ majority-agreement cases are preserved as soft labels for uncertainty-aware modelling. Yor-Sarc\footnote{https://github.com/toheebadura/yor-sarc} is expected to facilitate research on semantic interpretation and culturally informed NLP for low-resource African languages.

</details>


### [18] [Whisper: Courtside Edition Enhancing ASR Performance Through LLM-Driven Context Generation](https://arxiv.org/abs/2602.18966)
*Yonathan Ron,Shiri Gilboa,Tammuz Dubnov*

Main category: cs.CL

TL;DR: 提出Whisper: Courtside Edition，一个无需重新训练的多智能体LLM流程，通过领域上下文识别、命名实体识别和术语检测来增强Whisper转录，在NBA篮球解说领域实现17%的WER相对降低。


<details>
  <summary>Details</summary>
Motivation: 领域特定语音（如体育解说）对ASR系统构成持续挑战，即使是最先进的Whisper系统也难以处理密集的专有名词和技术术语。现有方法如模型微调成本高昂，需要更实用的领域适应方案。

Method: 设计多智能体LLM流程：1) 拦截Whisper初始转录；2) 使用专门LLM智能体进行领域上下文识别、命名实体识别和术语检测；3) 生成紧凑提示来指导Whisper解码器；4) 在421个NBA篮球解说片段上评估。

Result: 最佳流程实现17.0%的相对WER降低（从0.217到0.180，p<0.001），40.1%的片段有改进，仅7.1%退化，显著优于直接转录后编辑。证明了提示增强在ASR领域适应中的有效性。

Conclusion: 基于提示的增强方法可为ASR提供可扩展的领域适应，是成本高昂模型微调的实用替代方案，展示了多智能体LLM流程在提升领域特定语音识别性能方面的潜力。

Abstract: Domain-specific speech remains a persistent challenge for automatic speech recognition (ASR), even for state-of-the-art systems like OpenAI's Whisper. We introduce Whisper: Courtside Edition, a novel multi-agent large language model (LLM) pipeline that enhances Whisper transcriptions without retraining. The pipeline intercepts Whisper's initial transcript, applies specialized LLM agents for domain context identification, named entity recognition, and jargon detection, and generates compact prompts that guide Whisper's decoder. Evaluated on 421 NBA basketball commentary segments (a domain characterized by dense proper nouns and technical terminology) our best pipeline achieves a statistically significant 17.0% relative reduction in word error rate (WER; from 0.217 to 0.180, p<0.001). Improvements are observed in 40.1% of segments with degradation in only 7.1%, substantially outperforming direct transcript post-editing. These results demonstrate that prompt-based augmentation can deliver scalable domain adaptation for ASR, offering a practical alternative to costly model fine-tuning.

</details>


### [19] [Capable but Unreliable: Canonical Path Deviation as a Causal Mechanism of Agent Failure in Long-Horizon Tasks](https://arxiv.org/abs/2602.19008)
*Wilson Y. Lee*

Main category: cs.CL

TL;DR: 该研究发现语言智能体失败主要是可靠性问题而非能力问题，通过分析工具使用任务中的轨迹发现，成功运行更接近规范解路径，且偏离路径具有自增强效应。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解释为什么语言智能体在能够解决的任务上仍然会失败，提出许多失败是可靠性问题而非能力问题，需要理解任务解的结构和智能体轨迹的关系。

Method: 使用Toolathlon基准进行自然实验，分析22个前沿模型在108个真实世界工具使用任务上的3次独立运行，共515个模型×任务单元。通过Jaccard相似度衡量轨迹与规范解路径的接近程度，并进行六项稳健性检验。

Result: 成功运行比失败运行显著更接近规范解路径（Jaccard +0.060，p<0.0001）。偏离路径具有自增强效应：每个偏离规范的调用使下一个调用也偏离的概率增加22.7个百分点。基于轨迹中期的规范接近度监控可将成功率提升8.8个百分点。

Conclusion: 智能体可靠性不能仅通过能力扩展来改善，但可通过监控轨迹与规范解路径的接近度进行有效干预。偏离规范路径具有累积效应，导致可靠性失败。

Abstract: Why do language agents fail on tasks they are capable of solving? We argue that many such failures are reliability failures caused by stochastic drift from a task's latent solution structure, not capability failures. Every well-defined tool-use task imposes a canonical solution path (i.e., a convergent set of tool invocations shared across successful runs) and agent success depends critically on whether a trajectory stays within this path's operating envelope. We establish this causally using a natural experiment that holds model capability and task difficulty fixed by construction. We analyze trajectories from the Toolathlon benchmark: 22 frontier models each attempt 108 real-world tool-use tasks across 3 independent runs, yielding 515 model$\times$task units where the same model succeeds on some runs and fails on others due to LLM sampling stochasticity alone. Within these units, successful runs adhere significantly more closely to the canonical solution path than failed runs ($+$0.060 Jaccard, $p<0.0001$, $n=488$ units, 95% CI [+0.043, +0.077]). This result survives six robustness checks including cross-model-family leave-one-out validation. Critically, the causal mechanism is gradual and self-reinforcing: the adherence gap is statistically indistinguishable from zero through the first 50% of the trajectory, ruling out early-branching selection bias, and each off-canonical tool call raises the probability that the next call is also off-canonical by 22.7 percentage points ($\hatβ=+0.227$, $p<0.0001$), more than doubling the baseline rate. These findings imply that agent reliability cannot be improved by capability scaling alone, but offer a highly actionable intervention: a simple monitor that restarts the bottom tercile of runs based on mid-trajectory canonical adherence lifts success rates by $+$8.8 percentage points among intervened runs.

</details>


### [20] [Uncovering Context Reliance in Unstructured Knowledge Editing](https://arxiv.org/abs/2602.19043)
*Zisheng Zhou,Mengqi Zhang,Shiguang Wu,Xiaotian Ye,Chi Zhang,Zhumin Chen,Pengjie Ren*

Main category: cs.CL

TL;DR: COIN框架通过减少上下文依赖，提升LLM非结构化知识编辑的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于下一词预测的LLM编辑方法存在"上下文依赖"问题，即编辑后的知识高度依赖特定上下文，导致推理时上下文缺失时无法有效回忆知识

Method: 提出COIN框架，通过鼓励模型关注局部范围内的知识而非记忆上下文模式，减少上下文依赖

Result: COIN将上下文依赖降低45.2%，编辑成功率比强基线提升23.6%

Conclusion: 缓解上下文依赖对于实现鲁棒的LLM知识编辑至关重要，COIN框架为此提供了有效解决方案

Abstract: Editing Large language models (LLMs) with real-world, unstructured knowledge is essential for correcting and updating their internal parametric knowledge. In this work, we revisit the fundamental next-token prediction (NTP) as a candidate paradigm for unstructured editing. We identify Context Reliance as a critical failure mode of NTP-based approaches, where knowledge acquired from edited text becomes highly dependent on its preceding context, leading to recall failures when that context is absent during inference. This hypothesis is supported by our empirical validation that prepending context during inference recovers knowledge recall. We further theoretically demonstrate that Context Reliance is an inherent consequence of gradient-based optimization, which tends to bind acquired knowledge to a specific aggregated contextual representation. To address this, we propose a simple yet effective COntext-INdependent editing framework (COIN), encouraging model to focus on knowledge within local scope rather than memorizing contextual patterns. Evaluations show that COIN reduces Context Reliance by 45.2% and outperforms strong baselines by 23.6% in editing success rate, highlighting the vital role of mitigating Context Reliance for robust editing.

</details>


### [21] [IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning](https://arxiv.org/abs/2602.19049)
*Yinhan He,Yaochen Zhu,Mingjia Shi,Wendy Zheng,Lin Su,Xiaoqing Wang,Qi Guo,Jundong Li*

Main category: cs.CL

TL;DR: IAPO是一种基于信息论的训练后优化框架，通过条件互信息为每个token分配优势值，在保持准确性的同时显著减少推理长度


<details>
  <summary>Details</summary>
Motivation: 大型语言模型依赖长链思维提升准确性，但带来高昂的推理时间成本。现有序列级奖励塑造方法对token级推理努力分配控制有限，需要更精细的token效率优化机制

Method: 提出IAPO信息论训练后框架，基于每个token与最终答案的条件互信息分配token级优势值，识别信息丰富的推理步骤并抑制低效用探索

Result: IAPO在保持正确性的同时将推理长度减少达36%，在各种推理数据集上优于现有token效率强化学习方法，持续提升推理准确性

Conclusion: 信息感知的优势塑造是token效率训练后优化的强大通用方向，IAPO提供了明确的理论机制来平衡推理准确性与效率

Abstract: Large language models increasingly rely on long chains of thought to improve accuracy, yet such gains come with substantial inference-time costs. We revisit token-efficient post-training and argue that existing sequence-level reward-shaping methods offer limited control over how reasoning effort is allocated across tokens. To bridge the gap, we propose IAPO, an information-theoretic post-training framework that assigns token-wise advantages based on each token's conditional mutual information (MI) with the final answer. This yields an explicit, principled mechanism for identifying informative reasoning steps and suppressing low-utility exploration. We provide a theoretical analysis showing that our IAPO can induce monotonic reductions in reasoning verbosity without harming correctness. Empirically, IAPO consistently improves reasoning accuracy while reducing reasoning length by up to 36%, outperforming existing token-efficient RL methods across various reasoning datasets. Extensive empirical evaluations demonstrate that information-aware advantage shaping is a powerful and general direction for token-efficient post-training. The code is available at https://github.com/YinhanHe123/IAPO.

</details>


### [22] [Do LLMs and VLMs Share Neurons for Inference? Evidence and Mechanisms of Cross-Modal Transfer](https://arxiv.org/abs/2602.19058)
*Chenhang Cui,An Zhang,Yuxin Chen,Gelei Deng,Jingnan Zheng,Zhenkai Liang,Xiang Wang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 论文发现大语言模型(LLMs)和大视觉语言模型(LVLM)在多步推理任务中存在大量共享神经元，并提出SNRF框架通过低秩融合将这些推理能力从LLMs迁移到LVLMs。


<details>
  <summary>Details</summary>
Motivation: 尽管大视觉语言模型(LVLM)在各个领域快速发展，但在需要多步推理和组合决策的任务上仍落后于纯文本大语言模型(LLMs)。研究动机是探索这两种模型家族是否依赖共同的内部计算机制进行推理。

Method: 1. 在神经元层面分析发现超过一半的激活单元在LLMs和LVLMs之间共享，揭示了模态不变的推理子空间。2. 通过激活放大的因果探测证明这些共享神经元编码一致且可解释的概念级效应。3. 提出共享神经元低秩融合(SNRF)框架：通过跨模型激活分析识别共享神经元，计算模型间权重差异的低秩近似，并选择性地在共享神经元子空间中注入这些更新。

Result: 1. 在多种数学和感知基准测试中，SNRF持续提升LVLM的推理性能，同时保持感知能力。2. SNRF仅需最小参数更改，无需大规模多模态微调即可增强多模态推理性能。3. 共享神经元在LLMs和LVLMs之间形成了可解释的桥梁。

Conclusion: 共享神经元构成了LLMs和LVLMs之间的可解释桥梁，使得推理能力能够以低成本迁移到多模态模型中。SNRF框架为提升LVLM的推理性能提供了一种参数高效的方法。

Abstract: Large vision-language models (LVLMs) have rapidly advanced across various domains, yet they still lag behind strong text-only large language models (LLMs) on tasks that require multi-step inference and compositional decision-making. Motivated by their shared transformer architectures, we investigate whether the two model families rely on common internal computation for such inference. At the neuron level, we uncover a surprisingly large overlap: more than half of the top-activated units during multi-step inference are shared between representative LLMs and LVLMs, revealing a modality-invariant inference subspace.
  Through causal probing via activation amplification, we further show that these shared neurons encode consistent and interpretable concept-level effects, demonstrating their functional contribution to inference. Building on this insight, we propose Shared Neuron Low-Rank Fusion (SNRF), a parameter-efficient framework that transfers mature inference circuitry from LLMs to LVLMs. SNRF profiles cross-model activations to identify shared neurons, computes a low-rank approximation of inter-model weight differences, and injects these updates selectively within the shared-neuron subspace. This mechanism strengthens multimodal inference performance with minimal parameter changes and requires no large-scale multimodal fine-tuning.
  Across diverse mathematics and perception benchmarks, SNRF consistently enhances LVLM inference performance while preserving perceptual capabilities. Our results demonstrate that shared neurons form an interpretable bridge between LLMs and LVLMs, enabling low-cost transfer of inference ability into multimodal models. Our code is available at [https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons](https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons).

</details>


### [23] [TriTopic: Tri-Modal Graph-Based Topic Modeling with Iterative Refinement and Archetypes](https://arxiv.org/abs/2602.19079)
*Roman Egger*

Main category: cs.CL

TL;DR: TriTopic是一个三模态图融合框架，通过语义嵌入、TF-IDF和元数据的融合解决了BERTopic等主题模型的随机不稳定性、嵌入模糊和单一数据视角问题，实现了更高的NMI分数和100%语料覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法（如BERTopic）存在三个关键限制：随机不稳定性导致结果不可复现；嵌入模糊导致词汇精度损失；以及过度依赖单一数据视角（仅语义嵌入）。这些限制影响了主题建模的可靠性和实用性。

Method: TriTopic采用三模态图融合框架：1）通过互kNN和共享最近邻构建混合图以消除噪声并应对维度诅咒；2）使用共识Leiden聚类实现可复现的稳定分区；3）通过动态中心点拉动的迭代细化来锐化嵌入；4）用基于边界案例的原型表示替代传统的"平均文档"概念。

Result: 在20 Newsgroups、BBC News、AG News和Arxiv数据集上的基准测试显示，TriTopic在所有数据集上都取得了最高的NMI分数（平均NMI 0.575），相比BERTopic（0.513）、NMF（0.416）和LDA（0.299）有显著提升，并保证了100%语料覆盖率和0%异常值。

Conclusion: TriTopic通过三模态图融合和创新的聚类方法，有效解决了现有主题建模方法的局限性，提供了更稳定、精确且全面的主题发现能力，已作为开源PyPI库提供。

Abstract: Topic modeling extracts latent themes from large text collections, but leading approaches like BERTopic face critical limitations: stochastic instability, loss of lexical precision ("Embedding Blur"), and reliance on a single data perspective.
  We present TriTopic, a framework that addresses these weaknesses through a tri-modal graph fusing semantic embeddings, TF-IDF, and metadata. Three core innovations drive its performance: hybrid graph construction via Mutual kNN and Shared Nearest Neighbors to eliminate noise and combat the curse of dimensionality; Consensus Leiden Clustering for reproducible, stable partitions; and Iterative Refinement that sharpens embeddings through dynamic centroid-pulling. TriTopic also replaces the "average document" concept with archetype-based topic representations defined by boundary cases rather than centers alone.
  In benchmarks across 20 Newsgroups, BBC News, AG News, and Arxiv, TriTopic achieves the highest NMI on every dataset (mean NMI 0.575 vs. 0.513 for BERTopic, 0.416 for NMF, 0.299 for LDA), guarantees 100% corpus coverage with 0% outliers, and is available as an open-source PyPI library.

</details>


### [24] [Value Entanglement: Conflation Between Different Kinds of Good In (Some) Large Language Models](https://arxiv.org/abs/2602.19101)
*Seong Hah Cho,Junyi Li,Anna Leshinskaya*

Main category: cs.CL

TL;DR: 研究发现大型语言模型存在价值纠缠问题：模型将道德、语法和经济三种不同价值类型混淆，特别是语法和经济价值受到道德价值的过度影响，通过选择性消融道德相关激活向量可以修复这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的价值对齐需要实证测量模型实际习得的价值表征。人类价值表征的特点之一是能够区分不同类型的价值，本研究旨在探究LLMs是否同样能够区分道德、语法和经济这三种不同类型的价值。

Method: 通过探测模型行为、嵌入表示和残差流激活，分析LLMs对道德、语法和经济三种价值类型的表征。使用选择性消融技术去除与道德相关的激活向量，以验证价值纠缠问题。

Result: 发现了普遍的价值纠缠现象：模型混淆了这三种不同的价值表征。具体而言，语法和经济价值评估都受到道德价值的过度影响，这与人类规范不符。通过选择性消融道德相关激活向量可以修复这种混淆。

Conclusion: 大型语言模型存在价值表征的纠缠问题，特别是道德价值对其他价值类型的过度影响。这种混淆可以通过有针对性的干预来修复，这对LLMs的价值对齐研究具有重要意义。

Abstract: Value alignment of Large Language Models (LLMs) requires us to empirically measure these models' actual, acquired representation of value. Among the characteristics of value representation in humans is that they distinguish among value of different kinds. We investigate whether LLMs likewise distinguish three different kinds of good: moral, grammatical, and economic. By probing model behavior, embeddings, and residual stream activations, we report pervasive cases of value entanglement: a conflation between these distinct representations of value. Specifically, both grammatical and economic valuation was found to be overly influenced by moral value, relative to human norms. This conflation was repaired by selective ablation of the activation vectors associated with morality.

</details>


### [25] [Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2602.19111)
*Kainan Liu,Yong Zhang,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Main category: cs.CL

TL;DR: Astra是一种新的参数高效微调方法，利用模型输出激活的尾部特征向量构建任务自适应低秩适配器，在减少参数预算的同时实现更快收敛和更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA及其变体方法未能充分利用激活空间中尾部特征向量的潜力，这可能导致微调性能不理想。尾部特征向量包含重要但未被充分利用的信息，可以改善模型适应下游任务的能力。

Method: Astra通过从任务特定校准集估计模型输出激活的尾部特征向量，构建任务自适应低秩适配器。该方法将更新约束在这些尾部特征向量张成的子空间中，从而在减少参数预算的同时实现更有效的微调。

Result: 在16个自然语言理解和生成任务上的广泛实验表明，Astra始终优于现有的PEFT基线方法，在某些场景下甚至超越了全参数微调（FFT）。

Conclusion: Astra通过利用激活空间的尾部特征向量，提供了一种更有效的参数高效微调方法，在减少参数的同时实现了更好的下游任务性能。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra (Activation-Space Tail-Eigenvector Low-Rank Adaptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations-estimated from a small task-specific calibration set-to construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios.

</details>


### [26] [How Do LLMs Encode Scientific Quality? An Empirical Study Using Monosemantic Features from Sparse Autoencoders](https://arxiv.org/abs/2602.19115)
*Michael McCoubrey,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 本文首次使用稀疏自编码器提取单义特征，研究大语言模型如何编码科学质量概念，发现LLMs通过四类特征表示研究质量：研究方法、出版类型、高影响力领域和科学术语。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明大语言模型（LLMs）能在一定程度上评估研究质量，但我们对其内部机制的理解仍然有限。本研究旨在探索LLMs如何通过单义特征编码科学质量概念，填补这一知识空白。

Method: 使用稀疏自编码器从LLMs中提取相关的单义特征，在不同实验设置下推导这些特征，并评估它们作为预测因子在三个研究质量相关任务中的表现：预测引用次数、期刊SJR和期刊h指数。

Result: 研究发现LLMs编码了与科学质量多个维度相关的特征，识别出四类重复出现的特征类型：1）反映研究方法的特征；2）与出版类型相关的特征（文献综述通常具有更高影响力）；3）与高影响力研究领域和技术相关的特征；4）对应特定科学术语的特征。

Conclusion: 这些发现代表了理解LLMs如何封装与研究质量相关概念的重要一步，揭示了LLMs内部对科学质量的多维度编码机制。

Abstract: In recent years, there has been a growing use of generative AI, and large language models (LLMs) in particular, to support both the assessment and generation of scientific work. Although some studies have shown that LLMs can, to a certain extent, evaluate research according to perceived quality, our understanding of the internal mechanisms that enable this capability remains limited. This paper presents the first study that investigates how LLMs encode the concept of scientific quality through relevant monosemantic features extracted using sparse autoencoders. We derive such features under different experimental settings and assess their ability to serve as predictors across three tasks related to research quality: predicting citation count, journal SJR, and journal h-index. The results indicate that LLMs encode features associated with multiple dimensions of scientific quality. In particular, we identify four recurring types of features that capture key aspects of how research quality is represented: 1) features reflecting research methodologies; 2) features related to publication type, with literature reviews typically exhibiting higher impact; 3) features associated with high-impact research fields and technologies; and 4) features corresponding to specific scientific jargons. These findings represent an important step toward understanding how LLMs encapsulate concepts related to research quality.

</details>


### [27] [AgenticRAGTracer: A Hop-Aware Benchmark for Diagnosing Multi-Step Retrieval Reasoning in Agentic RAG](https://arxiv.org/abs/2602.19127)
*Qijie You,Wenkai Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: AgenticRAGTracer：首个通过大语言模型自动构建的Agentic RAG基准，支持逐步验证，包含1305个多领域数据点，揭示当前LLM在多跳推理中的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有Agentic RAG基准存在两大问题：1）缺乏中间跳级问题，无法分析模型在哪个步骤失败；2）多为人工构建，耗时耗力且扩展性差。需要支持逐步验证的自动化基准来促进Agentic RAG研究。

Method: 提出AgenticRAGTracer基准，主要利用大语言模型自动构建，包含1305个多领域数据点，与现有主流基准无重叠。基准设计支持逐步验证，包含中间跳级问题，能够追踪推理链的每一步。

Result: 实验表明，即使最佳大语言模型（如GPT-5）在该数据集上表现也很差，在最难部分仅获得22.6%的EM准确率。跳级感知诊断揭示失败主要由扭曲的推理链驱动：过早崩溃或过度扩展，显示模型无法根据任务逻辑结构合理分配推理步骤。

Conclusion: AgenticRAGTracer填补了传统评估中缺失的诊断维度，揭示了当前LLM在Agentic RAG任务中的关键缺陷，将为Agentic RAG研究提供重要支持并推动该领域的实质性进展。

Abstract: With the rapid advancement of agent-based methods in recent years, Agentic RAG has undoubtedly become an important research direction. Multi-hop reasoning, which requires models to engage in deliberate thinking and multi-step interaction, serves as a critical testbed for assessing such capabilities. However, existing benchmarks typically provide only final questions and answers, while lacking the intermediate hop-level questions that gradually connect atomic questions to the final multi-hop query. This limitation prevents researchers from analyzing at which step an agent fails and restricts more fine-grained evaluation of model capabilities. Moreover, most current benchmarks are manually constructed, which is both time-consuming and labor-intensive, while also limiting scalability and generalization. To address these challenges, we introduce AgenticRAGTracer, the first Agentic RAG benchmark that is primarily constructed automatically by large language models and designed to support step-by-step validation. Our benchmark spans multiple domains, contains 1,305 data points, and has no overlap with existing mainstream benchmarks. Extensive experiments demonstrate that even the best large language models perform poorly on our dataset. For instance, GPT-5 attains merely 22.6\% EM accuracy on the hardest portion of our dataset. Hop-aware diagnosis reveals that failures are primarily driven by distorted reasoning chains -- either collapsing prematurely or wandering into over-extension. This highlights a critical inability to allocate steps consistent with the task's logical structure, providing a diagnostic dimension missing in traditional evaluations. We believe our work will facilitate research in Agentic RAG and inspire further meaningful progress in this area. Our code and data are available at https://github.com/YqjMartin/AgenticRAGTracer.

</details>


### [28] [A Dataset for Named Entity Recognition and Relation Extraction from Art-historical Image Descriptions](https://arxiv.org/abs/2602.19133)
*Stefanie Schneider,Miriam Göldl,Julian Stalter,Ricarda Vollmer*

Main category: cs.CL

TL;DR: FRAME是一个用于艺术史图像描述的手动标注数据集，支持命名实体识别和关系抽取，包含37种实体类型和类型化关系链接，与Wikidata对齐以支持实体链接和知识图谱构建。


<details>
  <summary>Details</summary>
Motivation: 艺术史领域缺乏专门针对图像描述的高质量标注数据集，现有数据集难以支持细粒度的命名实体识别和关系抽取任务，特别是在艺术史这种需要专业知识的领域。

Method: 从博物馆目录、拍卖清单、开放平台和学术数据库中收集艺术史图像描述，筛选出专注于单个艺术品且包含明确材料、构图或图像学陈述的文本。采用三层标注架构：元数据层（对象属性）、内容层（描绘主题和图案）、共指层（链接重复提及）。实体类型与Wikidata对齐。

Result: 创建了FRAME数据集，包含37种实体类型的标注和类型化关系链接，以UIMA XMI CAS文件格式发布，附带图像和书目元数据。该数据集可用于基准测试和微调NER/RE系统，包括大语言模型的零样本和少样本设置。

Conclusion: FRAME填补了艺术史领域高质量标注数据集的空白，为艺术史图像描述的细粒度信息提取提供了标准化资源，支持NER、RE和NEL任务，有助于艺术史知识图谱的构建和大语言模型在该领域的应用。

Abstract: This paper introduces FRAME (Fine-grained Recognition of Art-historical Metadata and Entities), a manually annotated dataset of art-historical image descriptions for Named Entity Recognition (NER) and Relation Extraction (RE). Descriptions were collected from museum catalogs, auction listings, open-access platforms, and scholarly databases, then filtered to ensure that each text focuses on a single artwork and contains explicit statements about its material, composition, or iconography. FRAME provides stand-off annotations in three layers: a metadata layer for object-level properties, a content layer for depicted subjects and motifs, and a co-reference layer linking repeated mentions. Across layers, entity spans are labeled with 37 types and connected by typed RE links between mentions. Entity types are aligned with Wikidata to support Named Entity Linking (NEL) and downstream knowledge-graph construction. The dataset is released as UIMA XMI Common Analysis Structure (CAS) files with accompanying images and bibliographic metadata, and can be used to benchmark and fine-tune NER and RE systems, including zero- and few-shot setups with Large Language Models (LLMs).

</details>


### [29] [TurkicNLP: An NLP Toolkit for Turkic Languages](https://arxiv.org/abs/2602.19174)
*Sherzod Hakimov*

Main category: cs.CL

TL;DR: TurkicNLP是一个开源Python库，为突厥语系提供统一的NLP处理管道，覆盖四种文字体系，包含多种NLP任务和跨语言功能。


<details>
  <summary>Details</summary>
Motivation: 突厥语系有超过2亿使用者，但NLP工具和资源分散且不统一，大多数语言缺乏整合的工具链，需要统一的解决方案。

Method: 采用模块化多后端架构，集成基于规则的有限状态转录器和神经模型，通过语言无关的API提供自动文字检测和变体路由，输出遵循CoNLL-U标准。

Result: 开发了TurkicNLP库，支持分词、形态分析、词性标注、依存句法分析、命名实体识别、双向文字转写、跨语言句子嵌入和机器翻译等功能。

Conclusion: TurkicNLP为突厥语系提供了首个统一的NLP工具库，解决了资源碎片化问题，支持四种文字体系，具有完全互操作性和可扩展性。

Abstract: Natural language processing for the Turkic language family, spoken by over 200 million people across Eurasia, remains fragmented, with most languages lacking unified tooling and resources. We present TurkicNLP, an open-source Python library providing a single, consistent NLP pipeline for Turkic languages across four script families: Latin, Cyrillic, Perso-Arabic, and Old Turkic Runic. The library covers tokenization, morphological analysis, part-of-speech tagging, dependency parsing, named entity recognition, bidirectional script transliteration, cross-lingual sentence embeddings, and machine translation through one language-agnostic API. A modular multi-backend architecture integrates rule-based finite-state transducers and neural models transparently, with automatic script detection and routing between script variants. Outputs follow the CoNLL-U standard for full interoperability and extension. Code and documentation are hosted at https://github.com/turkic-nlp/turkicnlp .

</details>


### [30] [Retrieval Augmented Enhanced Dual Co-Attention Framework for Target Aware Multimodal Bengali Hateful Meme Detection](https://arxiv.org/abs/2602.19212)
*Raihan Tanvir,Md. Golam Rabiul Alam*

Main category: cs.CL

TL;DR: 该研究针对孟加拉语仇恨表情包的检测挑战，通过数据集增强和提出xDORA框架，结合视觉-文本编码器、FAISS分类器和RAG增强，在低资源环境下实现了有效的多模态仇恨内容检测。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上多模态仇恨表情包日益增多，但在孟加拉语等低资源语言中，自动检测面临标注数据有限、类别不平衡和普遍存在的代码混合等挑战，需要开发有效的解决方案。

Method: 1) 通过整合MIMOSA数据集增强BHM数据集；2) 提出xDORA框架，结合CLIP/DINOv2视觉编码器和XGLM/XLM-R文本编码器，使用加权注意力池化学习跨模态表示；3) 开发基于FAISS的k近邻分类器进行非参数推理；4) 提出RAG-Fused DORA，引入检索增强的上下文推理；5) 评估LLaVA在零样本、少样本和检索增强提示下的表现。

Result: xDORA（CLIP + XLM-R）在仇恨表情包识别和目标实体检测上分别获得0.78和0.71的宏平均F1分数；RAG-Fused DORA提升至0.79和0.74，优于DORA基线。FAISS分类器表现竞争性，通过语义相似性建模对稀有类别具有鲁棒性。LLaVA在少样本设置下效果有限，检索增强仅带来适度改进，表明预训练视觉语言模型对代码混合孟加拉语内容需要微调。

Conclusion: 研究表明，监督学习、检索增强和非参数多模态框架能有效处理低资源仇恨言论检测中的语言和文化复杂性。xDORA和RAG-Fused DORA为孟加拉语仇恨表情包检测提供了有效解决方案，而预训练视觉语言模型在未微调时对代码混合内容效果有限。

Abstract: Hateful content on social media increasingly appears as multimodal memes that combine images and text to convey harmful narratives. In low-resource languages such as Bengali, automated detection remains challenging due to limited annotated data, class imbalance, and pervasive code-mixing. To address these issues, we augment the Bengali Hateful Memes (BHM) dataset with semantically aligned samples from the Multimodal Aggression Dataset in Bengali (MIMOSA), improving both class balance and semantic diversity. We propose the Enhanced Dual Co-attention Framework (xDORA), integrating vision encoders (CLIP, DINOv2) and multilingual text encoders (XGLM, XLM-R) via weighted attention pooling to learn robust cross-modal representations. Building on these embeddings, we develop a FAISS-based k-nearest neighbor classifier for non-parametric inference and introduce RAG-Fused DORA, which incorporates retrieval-driven contextual reasoning. We further evaluate LLaVA under zero-shot, few-shot, and retrieval-augmented prompting settings. Experiments on the extended dataset show that xDORA (CLIP + XLM-R) achieves macro-average F1-scores of 0.78 for hateful meme identification and 0.71 for target entity detection, while RAG-Fused DORA improves performance to 0.79 and 0.74, yielding gains over the DORA baseline. The FAISS-based classifier performs competitively and demonstrates robustness for rare classes through semantic similarity modeling. In contrast, LLaVA exhibits limited effectiveness in few-shot settings, with only modest improvements under retrieval augmentation, highlighting constraints of pretrained vision-language models for code-mixed Bengali content without fine-tuning. These findings demonstrate the effectiveness of supervised, retrieval-augmented, and non-parametric multimodal frameworks for addressing linguistic and cultural complexities in low-resource hate speech detection.

</details>


### [31] [Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering](https://arxiv.org/abs/2602.19317)
*Maryam Amirizaniani,Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: PR2是一个强化学习框架，通过集成推理和个性化上下文检索来改进问答系统的个性化效果，相比现有方法实现了8.8%-12%的相对提升。


<details>
  <summary>Details</summary>
Motivation: 现有问答个性化方法主要依赖检索增强生成(RAG)，直接使用用户查询检索个人文档，导致表面层次的个性化，无法深度整合用户背景、偏好和历史上下文。

Method: 提出PR2强化学习框架，学习自适应检索-推理策略：决定何时检索、从用户档案中检索什么证据、如何将其整合到中间推理步骤中。通过个性化奖励函数优化多轮推理轨迹，强化与用户特定偏好和上下文信号对齐的推理路径。

Result: 在LaMP-QA基准测试中使用三种LLM进行广泛实验，PR2始终优于强基线方法，在个性化问答中实现了平均8.8%-12%的相对改进。

Conclusion: PR2框架通过集成推理和个性化上下文检索，能够学习自适应策略，显著提升问答系统的个性化效果，超越了传统的检索增强生成方法。

Abstract: Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users' background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal context by retrieving relevant items from the user's profile. Existing methods use the user's query directly to retrieve personal documents, and such strategies often lead to surface-level personalization. We propose PR2 (Personalized Retrieval-Augmented Reasoning), a reinforcement learning framework that integrates reasoning and retrieval from personal context for personalization. PR2 learns adaptive retrieval-reasoning policies, determining when to retrieve, what evidence to retrieve from user profiles, and how to incorporate it into intermediate reasoning steps. By optimizing multi-turn reasoning trajectories under a personalized reward function, the framework reinforces reasoning paths that better align with user-specific preferences and contextual signals reflected by the reward model. Extensive experiments on the LaMP-QA benchmark using three LLMs show that PR2 consistently outperforms strong baselines, achieving an average relative improvement of 8.8%-12% in personalized QA.

</details>


### [32] [Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations](https://arxiv.org/abs/2602.19320)
*Dongming Jiang,Yi Li,Songtao Wei,Jinxin Yang,Ayushi Kishore,Alysa Zhao,Dingyi Kang,Xu Hu,Feng Chen,Qiannan Li,Bingzhe Li*

Main category: cs.CL

TL;DR: 该论文系统分析了LLM智能体记忆系统的现状，指出了现有基准测试不足、评估指标不匹配、性能依赖骨干模型、系统成本被忽视等核心问题，并提出了结构化分析框架和改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体记忆系统架构发展迅速，但其经验基础仍然脆弱。现有基准测试规模不足、评估指标与语义效用不匹配、性能在不同骨干模型间差异显著、系统级成本常被忽视，这些问题限制了记忆系统的实际应用效果。

Method: 论文从架构和系统两个角度对智能体记忆进行结构化分析：1）基于四种记忆结构提出MAG系统分类法；2）分析当前系统的关键痛点，包括基准饱和效应、指标有效性和评估敏感性、骨干模型依赖的准确性、以及记忆维护引入的延迟和吞吐量开销。

Result: 通过将记忆结构与经验限制联系起来，论文阐明了为什么当前智能体记忆系统常常未能实现其理论承诺，揭示了评估和系统设计中的系统性缺陷。

Conclusion: 该调查为更可靠的评估和可扩展的系统设计指明了方向，强调需要建立更健壮的基准测试、更有效的评估指标、减少对特定骨干模型的依赖，并系统考虑性能开销，以实现智能体记忆系统的实际应用价值。

Abstract: Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.

</details>


### [33] [Personalized Prediction of Perceived Message Effectiveness Using Large Language Model Based Digital Twins](https://arxiv.org/abs/2602.19403)
*Jasmin Han,Janardan Devkota,Joseph Waring,Amanda Luken,Felix Naughton,Roger Vilardaga,Jonathan Bricker,Carl Latkin,Meghan Moran,Yiqun Chen,Johannes Thrul*

Main category: cs.CL

TL;DR: LLM数字孪生模型在预测戒烟信息感知有效性方面优于监督学习和零/少样本方法，为移动健康干预个性化提供新途径


<details>
  <summary>Details</summary>
Motivation: 感知信息有效性对于选择和优化移动健康平台中的个性化戒烟干预信息至关重要，需要评估大语言模型是否能准确预测戒烟信息的感知有效性

Method: 评估了三种模型：基于标记数据训练的监督学习模型、无需任务特定微调的零/少样本LLM、以及结合个体特征和先前PME历史的LLM数字孪生模型。使用301名年轻成年吸烟者的3010条信息评分，在三个领域（内容质量、应对支持、戒烟支持）评估模型性能

Result: LLM数字孪生模型在准确性上平均优于零/少样本LLM 12个百分点，优于监督基线13个百分点，在三个领域的准确率分别为0.49、0.45、0.49，在简化的3点量表上方向性准确率达到0.75、0.66、0.70。数字孪生预测在不同评分类别间显示出更大的离散度，表明对个体差异的敏感性提高

Conclusion: 将个人档案与大语言模型结合能够捕捉PME中的人特异性差异，优于监督学习和零/少样本方法。改进的PME预测可能使移动健康干预内容更加个性化。LLM数字孪生模型在支持移动戒烟和其他健康行为改变干预的个性化方面显示出潜力

Abstract: Perceived message effectiveness (PME) by potential intervention end-users is important for selecting and optimizing personalized smoking cessation intervention messages for mobile health (mHealth) platform delivery. This study evaluates whether large language models (LLMs) can accurately predict PME for smoking cessation messages.
  We evaluated multiple models for predicting PME across three domains: content quality, coping support, and quitting support. The dataset comprised 3010 message ratings (5-point Likert scale) from 301 young adult smokers. We compared (1) supervised learning models trained on labeled data, (2) zero and few-shot LLMs prompted without task-specific fine-tuning, and (3) LLM-based digital twins that incorporate individual characteristics and prior PME histories to generate personalized predictions. Model performance was assessed on three held-out messages per participant using accuracy, Cohen's kappa, and F1.
  LLM-based digital twins outperformed zero and few-shot LLMs (12 percentage points on average) and supervised baselines (13 percentage points), achieving accuracies of 0.49 (content), 0.45 (coping), and 0.49 (quitting), with directional accuracies of 0.75, 0.66, and 0.70 on a simplified 3-point scale. Digital twin predictions showed greater dispersion across rating categories, indicating improved sensitivity to individual differences.
  Integrating personal profiles with LLMs captures person-specific differences in PME and outperforms supervised and zero and few-shot approaches. Improved PME prediction may enable more tailored intervention content in mHealth. LLM-based digital twins show potential for supporting personalization of mobile smoking cessation and other health behavior change interventions.

</details>


### [34] [Hyper-KGGen: A Skill-Driven Knowledge Extractor for High-Quality Knowledge Hypergraph Generation](https://arxiv.org/abs/2602.19543)
*Rizhuo Huang,Yifan Feng,Rundong Xue,Shihui Ying,Jun-Hai Yong,Chuan Shi,Shaoyi Du,Yue Gao*

Main category: cs.CL

TL;DR: Hyper-KGGen：一个技能驱动的框架，通过动态技能演化过程解决知识超图构建中的场景鸿沟问题，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 知识超图能表示复杂的n元原子事实，但构建高质量超图面临场景鸿沟挑战：通用提取器难以跨领域泛化，现有方法无法平衡结构骨架与细粒度细节。

Method: 提出Hyper-KGGen框架，采用粗到细机制系统分解文档，确保从二元链接到复杂超边的全维度覆盖；引入自适应技能获取模块，通过基于稳定性的反馈循环将领域专业知识提炼到全局技能库中。

Result: 实验表明Hyper-KGGen显著优于强基线方法，验证了演化技能在多场景设置中比静态少样本示例提供更丰富的指导；同时提出了HyperDocRED基准数据集。

Conclusion: Hyper-KGGen通过动态技能演化有效解决了知识超图构建中的场景鸿沟问题，为多领域知识提取提供了更强大的框架。

Abstract: Knowledge hypergraphs surpass traditional binary knowledge graphs by encapsulating complex $n$-ary atomic facts, providing a more comprehensive paradigm for semantic representation. However, constructing high-quality hypergraphs remains challenging due to the \textit{scenario gap}: generic extractors struggle to generalize across diverse domains with specific jargon, while existing methods often fail to balance structural skeletons with fine-grained details. To bridge this gap, we propose \textbf{Hyper-KGGen}, a skill-driven framework that reformulates extraction as a dynamic skill-evolving process. First, Hyper-KGGen employs a \textit{coarse-to-fine} mechanism to systematically decompose documents, ensuring full-dimensional coverage from binary links to complex hyperedges. Crucially, it incorporates an \textit{adaptive skill acquisition} module that actively distills domain expertise into a Global Skill Library. This is achieved via a stability-based feedback loop, where extraction stability serves as a relative reward signal to induce high-quality skills from unstable traces and missed predictions. Additionally, we present \textbf{HyperDocRED}, a rigorously annotated benchmark for document-level knowledge hypergraph extraction. Experiments demonstrate that Hyper-KGGen significantly outperforms strong baselines, validating that evolved skills provide substantially richer guidance than static few-shot examples in multi-scenario settings.

</details>


### [35] [Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining](https://arxiv.org/abs/2602.19548)
*Jeffrey Li,Josh Gardner,Doug Kang,Fangping Shi,Karanjeet Singh,Chun-Liang Li,Herumb Shandilya,David Hall,Oncel Tuzel,Percy Liang,Ludwig Schmidt,Hadi Pour Ansari,Fartash Faghri*

Main category: cs.CL

TL;DR: 通过使用多种提取器的并集而非单一固定提取器，可以显著提高网页文本提取的覆盖率和下游任务性能


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集对多样化的网页内容仅使用单一固定提取器，可能导致互联网数据覆盖不足和利用不充分

Method: 研究不同提取器对网页文本提取的影响，通过取不同提取器的并集来增加token产量，并评估提取器选择对结构化内容（表格和代码块）下游任务性能的影响

Result: 不同提取器虽然对标准语言理解任务性能相似，但通过并集操作可将DCLM-Baseline的token产量提高71%且保持基准性能；对于结构化内容，提取器选择显著影响下游任务性能，WikiTQ差异达10个百分点，HumanEval差异达3个百分点

Conclusion: 使用多种提取器的并集而非单一固定提取器可以更有效地利用互联网数据，提高文本提取覆盖率并改善结构化内容的下游任务性能

Abstract: One of the first pre-processing steps for constructing web-scale LLM pretraining datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor to all webpages. In this work, we investigate whether this practice leads to suboptimal coverage and utilization of Internet data. We first show that while different extractors may lead to similar model performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and code blocks, extractor choice can significantly impact downstream task performance, with differences of up to 10 percentage points (p.p.) on WikiTQ and 3 p.p. on HumanEval.

</details>


### [36] [Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework](https://arxiv.org/abs/2602.19549)
*Yibo Yan,Mingdong Ou,Yi Cao,Xin Zou,Jiahao Huo,Shuliang Liu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 提出Prune-then-Merge两阶段框架，通过自适应剪枝和分层合并解决视觉文档检索中多向量范式的高开销问题，在保持特征保真度的同时实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 当前视觉文档检索(VDR)中，最先进的多向量范式虽然性能优异，但存在过高的计算开销问题。现有的效率优化方法（如剪枝和合并）存在局限性，需要在压缩率和特征保真度之间做出困难权衡。

Method: 提出Prune-then-Merge两阶段框架：1) 自适应剪枝阶段：过滤掉低信息量的图像块，创建精炼的高信号嵌入集合；2) 分层合并阶段：对预过滤的集合进行压缩，有效总结语义内容，避免单阶段方法中噪声引起的特征稀释问题。

Result: 在29个VDR数据集上的广泛实验表明，该框架始终优于现有方法，显著扩展了近无损压缩范围，并在高压缩比下提供稳健性能。

Conclusion: Prune-then-Merge框架通过协同利用剪枝和合并这两种互补方法，有效解决了VDR中多向量范式的效率与性能权衡问题，为视觉文档检索提供了更优的解决方案。

Abstract: Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.

</details>


### [37] [DEEP: Docker-based Execution and Evaluation Platform](https://arxiv.org/abs/2602.19583)
*Sergio Gómez González,Miguel Domingo,Francisco Casacuberta*

Main category: cs.CL

TL;DR: DEEP是一个自动化评估软件，用于机器翻译和光学字符识别模型的执行与评分，支持Docker容器化系统，通过统计分析和聚类算法帮助研究者理解模型性能差异。


<details>
  <summary>Details</summary>
Motivation: 在研究中，系统比较评估是常见任务，无论是选择合适系统还是展示研究成果潜力都至关重要。公开竞赛评估也需要高效的系统比较工具。当前缺乏自动化、可扩展的评估框架。

Method: DEEP软件接收Docker容器化系统，自动执行模型并提取信息，将假设结果与参考标准对比评估。采用基于统计显著性分析的聚类算法对模型结果进行分组，并提供可视化Web应用界面。

Result: DEEP能够自动化执行和评估模型，通过聚类分析帮助评估者识别性能集群，理解模型间差异的统计显著性。可视化工具确保结果易于理解和解释。

Conclusion: DEEP提供了一个自动化、可扩展的评估框架，显著简化了系统比较过程，使研究者能够更深入地理解模型性能差异，并通过实际用例展示了其有效性。

Abstract: Comparative evaluation of several systems is a recurrent task in researching. It is a key step before deciding which system to use for our work, or, once our research has been conducted, to demonstrate the potential of the resulting model. Furthermore, it is the main task of competitive, public challenges evaluation. Our proposed software (DEEP) automates both the execution and scoring of machine translation and optical character recognition models. Furthermore, it is easily extensible to other tasks. DEEP is prepared to receive dockerized systems, run them (extracting information at that same time), and assess hypothesis against some references. With this approach, evaluators can achieve a better understanding of the performance of each model. Moreover, the software uses a clustering algorithm based on a statistical analysis of the significance of the results yielded by each model, according to the evaluation metrics. As a result, evaluators are able to identify clusters of performance among the swarm of proposals and have a better understanding of the significance of their differences. Additionally, we offer a visualization web-app to ensure that the results can be adequately understood and interpreted. Finally, we present an exemplary case of use of DEEP.

</details>


### [38] [Eye-Tracking-while-Reading: A Living Survey of Datasets with Open Library Support](https://arxiv.org/abs/2602.19598)
*Deborah N. Jakobi,David R. Reich,Paul Prasse,Jana M. Hofmann,Lena S. Bolliger,Lena A. Jäger*

Main category: cs.CL

TL;DR: 该论文创建了一个眼动阅读数据集的综合资源，包括在线数据库、Python包和标准化框架，以提高眼动阅读研究的FAIR原则和可重用性。


<details>
  <summary>Details</summary>
Motivation: 眼动阅读数据集在多个学科中有重要价值，但现有数据集分散在不同领域，缺乏数据共享标准，导致互操作性和重用性差。需要提高数据集的透明度和可访问性。

Method: 1) 对现有数据集进行广泛概述；2) 创建在线动态数据库（https://dili-lab.github.io/datasets.html），记录每个数据集的45个以上特征；3) 将所有公开数据集集成到Python包pymovements中，提供眼动数据集库。

Result: 建立了包含多个数据集的综合资源，提供了标准化的数据访问接口，促进了眼动阅读研究的FAIR原则实施，支持研究的复制和验证。

Conclusion: 通过创建统一的眼动阅读数据集资源，提高了数据集的互操作性和可重用性，促进了眼动阅读研究的开放科学实践和良好科研规范。

Abstract: Eye-tracking-while-reading corpora are a valuable resource for many different disciplines and use cases. Use cases range from studying the cognitive processes underlying reading to machine-learning-based applications, such as gaze-based assessments of reading comprehension. The past decades have seen an increase in the number and size of eye-tracking-while-reading datasets as well as increasing diversity with regard to the stimulus languages covered, the linguistic background of the participants, or accompanying psychometric or demographic data. The spread of data across different disciplines and the lack of data sharing standards across the communities lead to many existing datasets that cannot be easily reused due to a lack of interoperability. In this work, we aim at creating more transparency and clarity with regards to existing datasets and their features across different disciplines by i) presenting an extensive overview of existing datasets, ii) simplifying the sharing of newly created datasets by publishing a living overview online, https://dili-lab.github.io/datasets.html, presenting over 45 features for each dataset, and iii) integrating all publicly available datasets into the Python package pymovements which offers an eye-tracking datasets library. By doing so, we aim to strengthen the FAIR principles in eye-tracking-while-reading research and promote good scientific practices, such as reproducing and replicating studies.

</details>


### [39] [Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning](https://arxiv.org/abs/2602.19612)
*Borisiuk Anna,Andrey Savchenko,Alexander Panchecko,Elena Tutubalina*

Main category: cs.CL

TL;DR: 提出了DUAL基准来评估大语言模型在不同训练阶段（预训练vs监督微调）的知识遗忘效果，发现SFT阶段的知识更容易平稳遗忘且保留率更高


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘研究假设所有知识同等可遗忘，忽略了知识来源（预训练vs监督微调）对遗忘效果的影响，需要建立更细粒度的评估基准

Method: 构建DUAL基准，包含28.6k个基于Wikidata的三元组，使用Wikipedia链接计数和LLM显著性评分标注事实流行度；比较预训练模型和SFT模型在不同遗忘策略下的表现

Result: SFT阶段的知识遗忘更平稳、调优更稳定，保留率比预训练模型高10-50%；直接对预训练模型进行遗忘不稳定，容易发生重新学习或灾难性遗忘

Conclusion: 知识来源训练阶段对机器遗忘效果有显著影响，SFT阶段的知识更容易实现平稳遗忘，为LLM安全遗忘提供了重要设计指导

Abstract: Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.

</details>


### [40] [KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge](https://arxiv.org/abs/2602.19643)
*Alex Robertson,Huizhi Liang,Mahbub Gani,Rohit Kumar,Srijith Rajamohan*

Main category: cs.CL

TL;DR: KGHaluBench是一个基于知识图谱的幻觉基准测试，通过动态构建多层面问题来评估LLM的真实性，提供更公平全面的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在静态、狭窄问题，导致覆盖范围有限和误导性评估。LLM虽然能生成连贯语言，但连贯性不等于真实性，常包含微妙幻觉。

Method: 利用知识图谱动态构建具有挑战性的多层面问题，通过统计估计难度以解决流行度偏差。建立自动化验证流程，在概念和正确性层面检测弃权和验证响应，识别不同类型的幻觉。

Result: 评估了25个前沿模型，使用新颖的准确性和幻觉指标。结果提供了关于不同模型规模导致幻觉的知识因素的可解释性见解。

Conclusion: KGHaluBench为幻觉缓解的未来发展提供支持，公开可用，能更公平全面地评估LLM的真实性。

Abstract: Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.

</details>


### [41] [Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling](https://arxiv.org/abs/2602.19919)
*Xiang Li,Zikai Wei,Yiyan Qi,Wanyun Zhou,Xiang Liu,Penglei Sun,Yongqi Zhang,Xiaowen Chu*

Main category: cs.CL

TL;DR: Janus-Q是一个端到端的事件驱动交易框架，将金融新闻事件从辅助信号提升为主要决策单元，通过两阶段范式统一事件中心数据构建和模型优化，实现更一致、可解释且盈利的交易决策。


<details>
  <summary>Details</summary>
Motivation: 金融市场波动常由新闻传达的离散金融事件驱动，这些事件具有异质性、突发性，难以在纯数值预测目标下捕捉。现有方法面临两大挑战：缺乏大规模事件中心数据集来联合建模新闻语义和统计基础的市场反应；语言模型推理与动态市场条件下财务有效交易行为之间的错配。

Method: 采用两阶段范式：第一阶段聚焦事件中心数据构建，创建包含62,400篇文章的大规模金融新闻事件数据集，标注10种细粒度事件类型、关联股票、情感标签和事件驱动的累积异常收益(CAR)；第二阶段进行决策导向的微调，结合监督学习和强化学习，由分层门控奖励模型(HGRM)指导，明确捕捉多个交易目标之间的权衡。

Result: Janus-Q相比市场指数和LLM基线实现了更一致、可解释且盈利的交易决策，夏普比率提升高达102.0%，方向准确性相比最强竞争策略提高超过17.5%。

Conclusion: Janus-Q成功解决了事件驱动交易中的关键挑战，通过将金融新闻事件作为主要决策单元，结合大规模事件中心数据集和决策导向的优化，实现了显著优于现有方法的交易性能。

Abstract: Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.

</details>


### [42] [Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval](https://arxiv.org/abs/2602.19961)
*Yibo Yan,Jiahao Huo,Guanbo Feng,Mingdong Ou,Yi Cao,Xin Zou,Shuliang Liu,Yuanhuiyi Lyu,Yu Huang,Jungang Li,Kening Zheng,Xu Zheng,Philip S. Yu,James Kwok,Xuming Hu*

Main category: cs.CL

TL;DR: 本文首次全面综述了多模态大语言模型时代下的视觉文档检索领域，系统梳理了基准测试、方法演进（多模态嵌入模型、重排序模型、RAG与智能体系统集成），并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态信息的快速增长，视觉文档检索成为连接非结构化视觉丰富数据与精确信息获取的关键前沿。传统自然图像检索方法无法有效处理视觉文档特有的密集文本内容、复杂布局和细粒度语义依赖，因此需要系统梳理该领域在多模态大语言模型时代的发展现状。

Method: 本文采用系统性综述方法：首先分析基准测试现状，然后将方法演进分为三类：1）多模态嵌入模型；2）多模态重排序模型；3）检索增强生成与智能体系统在复杂文档智能中的集成。

Result: 提供了视觉文档检索领域的首个全面综述，系统梳理了多模态大语言模型时代下的技术发展脉络，建立了清晰的方法分类框架，并识别了该领域的关键技术演进路径。

Conclusion: 视觉文档检索是多模态文档智能的关键前沿，本文通过系统综述为该领域提供了清晰的发展路线图，指出了持续存在的挑战和未来有前景的研究方向，将推动多模态文档智能的进一步发展。

Abstract: With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.

</details>


### [43] [ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting](https://arxiv.org/abs/2602.19969)
*Yuxing Tian,Fengran Mo,Weixu Zhang,Yiyan Qi,Jian-Yun Nie*

Main category: cs.CL

TL;DR: ReAttn是一种后处理重加权策略，用于改进基于注意力的LLM重排序方法，通过跨文档IDF加权减少词汇偏置，并通过熵正则化缓解注意力过度集中问题。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的重排序方法虽然高效且可解释，但存在两个主要局限：1) 注意力信号过度集中在少数文档的少量token上，导致其他文档难以区分；2) 注意力过度强调与查询词汇相似的短语，造成词汇偏置，使仅具有词汇相似性的不相关文档被误判为相关。

Method: 提出ReAttn后处理重加权策略：1) 计算跨文档IDF加权，降低对在候选文档中频繁出现的查询重叠token的注意力权重，减少词汇偏置并突出独特术语；2) 采用基于熵的正则化，缓解注意力过度集中问题，鼓励在信息丰富的token之间更平衡的分布。两种调整都直接在现有注意力权重上操作，无需额外训练或监督。

Result: 大量实验证明了该方法的有效性，表明ReAttn能够显著改进基于注意力的重排序性能。

Conclusion: ReAttn通过简单的后处理重加权策略有效解决了基于注意力重排序方法的两个主要局限，在不增加训练成本的情况下提升了排序质量，为LLM重排序提供了更可靠和平衡的解决方案。

Abstract: The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.

</details>


### [44] [Cross-lingual Matryoshka Representation Learning across Speech and Text](https://arxiv.org/abs/2602.19991)
*Yaya Sy,Dioula Doucouré,Christophe Cerisara,Irina Illina*

Main category: cs.CL

TL;DR: 首个法语-沃洛夫语双语语音-文本Matryoshka嵌入模型，支持沃洛夫语语音查询直接检索法语文本，无需ASR-翻译流水线


<details>
  <summary>Details</summary>
Motivation: 解决少数语言使用者在获取在线知识时面临的双重障碍：语言障碍（知识主要用少数主导语言表达）和模态障碍（信息主要为文本形式，而许多语言主要是口语）

Method: 训练首个双语语音-文本Matryoshka嵌入模型；引入大规模数据整理流水线和新基准；比较建模策略；在冻结的文本Matryoshka模型内进行模态融合

Result: 模态融合在冻结文本Matryoshka模型中表现最佳；模型虽然仅针对检索任务训练，但在语音意图检测等其他任务上泛化良好；分析显示信息仅集中在少数组件中，存在效率提升潜力

Conclusion: 提出的方法有效解决了少数语言使用者的双重障碍，Matryoshka嵌入模型展示了跨模态和跨语言的泛化能力，且信息集中特性为效率优化提供了可能

Abstract: Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation pipelines and new benchmarks, compare modeling strategies, and show that modality fusion within a frozen text Matryoshka model performs best. Although trained only for retrieval, the model generalizes well to other tasks, such as speech intent detection, indicating the learning of general semantic representations. Finally, we analyze cost-accuracy trade-offs across Matryoshka dimensions and ranks, showing that information is concentrated only in a few components, suggesting potential for efficiency improvements.

</details>


### [45] [QUIETT: Query-Independent Table Transformation for Robust Reasoning](https://arxiv.org/abs/2602.20017)
*Gaurav Najpande,Tampu Ravi Kumar,Manan Roy Choudhury,Neha Valeti,Yanjie Fu,Vivek Gupta*

Main category: cs.CL

TL;DR: QuIeTT是一个查询无关的表格转换框架，将原始表格预处理为SQL就绪的规范表示，实现表格清理与推理的解耦


<details>
  <summary>Details</summary>
Motivation: 现实世界表格通常存在不规则模式、异构值格式和隐式关系结构，这会降低下游表格推理和问答的可靠性。现有方法大多以查询相关的方式处理这些问题，将表格清理与推理纠缠在一起，限制了泛化能力

Method: QuIeTT框架在观察到任何测试时查询之前，将原始表格预处理为单一SQL就绪的规范表示。它执行无损模式和值规范化，暴露隐式关系，并通过原始表格快照保留完整的溯源信息

Result: 在WikiTQ、HiTab、NQ-Table和SequentialQA四个基准测试上的实验显示，QuIeTT在不同模型和推理范式上均获得一致增益，特别是在结构多样、未见问题的挑战集上表现出特别强的改进

Conclusion: 通过将表格转换与推理解耦，QuIeTT实现了更清洁、更可靠且高效的查询，无需修改下游模型，为表格数据处理提供了通用的预处理解决方案

Abstract: Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation before any test-time queries are observed. QuIeTT performs lossless schema and value normalization, exposes implicit relations, and preserves full provenance via raw table snapshots. By decoupling table transformation from reasoning, QuIeTT enables cleaner, more reliable, and highly efficient querying without modifying downstream models. Experiments on four benchmarks, WikiTQ, HiTab, NQ-Table, and SequentialQA show consistent gains across models and reasoning paradigms, with particularly strong improvements on a challenge set of structurally diverse, unseen questions.

</details>


### [46] [gencat: Generative computerized adaptive testing](https://arxiv.org/abs/2602.20020)
*Wanyong Feng,Andrew Lan*

Main category: cs.CL

TL;DR: GENCAT是一个基于大语言模型的新型计算机自适应测试框架，通过生成式项目反应理论模型处理开放式问题，在编程数据集上比传统方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有CAT框架主要基于预测学生回答的正确性，未能充分利用问题和回答中的文本信息，特别是对于开放式问题。需要一种能够处理开放式回答并利用文本信息进行知识评估和题目选择的新方法。

Method: 提出GENCAT框架：1）开发生成式项目反应理论模型，通过监督微调和偏好优化训练，从开放式回答估计学生知识并预测未见问题的回答；2）基于生成能力设计三种题目选择算法（不确定性、语言多样性、信息量）；3）在两个真实世界编程数据集上进行实验验证。

Result: 在真实世界编程数据集上的实验表明，GENCAT优于现有CAT基线方法，在关键早期测试阶段实现了高达4.32%的AUC改进。

Conclusion: GENCAT通过利用大语言模型的生成能力处理开放式回答，显著提升了计算机自适应测试的性能，特别是在早期测试阶段，为利用文本信息进行知识评估提供了有效框架。

Abstract: Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\textbf{GEN}erative \textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\% in the key early testing stages.

</details>


### [47] [AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization](https://arxiv.org/abs/2602.20040)
*Fahmida Liza Piya,Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: AgenticSum是一个推理时智能体框架，通过分离上下文选择、生成、验证和针对性修正来减少临床文本摘要中的幻觉内容


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本摘要自动化方面具有巨大潜力，但由于临床文档的长度、噪声和异质性，保持事实一致性仍然具有挑战性。现有方法容易产生幻觉内容，需要更可靠的方法来确保摘要的准确性。

Method: AgenticSum是一个推理时智能体框架，将摘要任务分解为协调的阶段：压缩任务相关上下文、生成初始草稿、使用内部注意力基础信号识别弱支持跨度、在监督控制下有选择地修订标记内容。该框架分离了上下文选择、生成、验证和针对性修正。

Result: 在两个公共数据集上使用基于参考的指标、LLM-as-a-judge评估和人工评估进行测试。在各种测量指标上，AgenticSum相比普通LLM和其他强基线都表现出了一致的改进。

Conclusion: 结构化、智能体设计结合针对性修正为使用LLM改进临床笔记摘要提供了一个有效的推理时解决方案。这种方法能够显著减少幻觉内容，提高临床文本摘要的事实一致性。

Abstract: Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.

</details>


### [48] [Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously](https://arxiv.org/abs/2602.20042)
*Han Bao,Yue Huang,Xiaoda Wang,Zheyuan Zhang,Yujun Zhou,Carl Yang,Xiangliang Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 论文批判当前基于单一标量奖励的通用对齐范式，提出边缘对齐作为替代方案，强调在多维价值结构、多元利益相关者和认知不确定性下的动态规范治理。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型部署在复杂的社会技术系统中，暴露了现有对齐实践的局限性。通用对齐范式将多样化的人类价值压缩为单一标量奖励，在价值冲突、多元利益相关者和不可约不确定性场景下达到结构上限。

Method: 提出边缘对齐作为替代方法，包含七个相互依存的支柱，分为三个阶段：1) 保留多维价值结构；2) 支持多元民主表征；3) 整合用于交互和澄清的认知机制。涉及数据收集、训练目标和评估等关键技术挑战。

Result: 识别出标量化的数学和激励机制导致三个核心问题：结构性的价值扁平化、规范表征损失和认知不确定性盲视。边缘对齐框架将对齐重新定义为动态规范治理的生命周期问题。

Conclusion: 对齐不应是单一实例优化任务，而应作为需要持续动态规范治理的生命周期问题。边缘对齐通过保留价值多样性、支持民主参与和整合认知机制，为复杂社会技术系统中的AI对齐提供了更可持续的路径。

Abstract: Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \textbf{structural} value flattening, \textbf{normative} representation loss, and \textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.

</details>


### [49] [Entropy in Large Language Models](https://arxiv.org/abs/2602.20052)
*Marco Scharringhausen*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）的词熵低于自然语言（书面或口语），这为评估使用LLM生成数据进行训练的影响提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过信息论框架量化LLM输出的信息特性，特别是比较LLM与自然语言的信息熵差异。长期目标是形式化LLM训练中的信息和不确定性直觉，评估使用LLM生成数据进行训练（尤其是来自互联网的文本）的影响。

Method: 将LLM输出视为从有限字母表生成无限符号序列的信息源，假设LLM遵循恒定随机分布的统计模型，从而源是平稳的。将LLM的词熵与Open American National Corpus（OANC）代表的自然语言（书面和口语）词熵进行比较。

Result: 研究结果表明，此类LLM的词熵低于自然语言（无论是书面还是口语形式）的词熵。这揭示了LLM输出相比自然语言具有更低的信息不确定性。

Conclusion: LLM输出的信息熵低于自然语言，这为理解LLM生成文本的信息特性提供了量化依据。该研究为评估使用LLM生成数据进行训练（如从互联网获取文本）的影响建立了理论基础，有助于形式化LLM训练中的信息不确定性概念。

Abstract: In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.

</details>


### [50] [Multilingual Large Language Models do not comprehend all natural languages to equal degrees](https://arxiv.org/abs/2602.20065)
*Natalia Moskvina,Raquel Montero,Masaya Yoshida,Ferdy Hubers,Paolo Morosi,Walid Irhaymi,Jin Yan,Tamara Serrano,Elena Pagliarini,Fritz Günther,Evelina Leivada*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在多语言理解任务中表现存在差异，英语并非最佳表现语言，多个罗曼语种（包括低资源语言）表现优于英语，但所有语言都落后于人类基线水平。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基准测试主要评估LLMs在高资源语言（主要是WEIRD社区使用的语言）上的表现，默认假设英语是LLMs表现最佳的语言，而低资源语言则与不可靠输出相关。需要了解LLMs在不同语言家族中的真实理解能力差异。

Method: 研究使用3个流行的大型语言模型，在12种代表不同语言家族（印欧语系、亚非语系、突厥语系、汉藏语系、日本语系）的语言上进行语言理解任务测试，分析模型表现差异。

Result: 模型在类型学多样的语言中表现出显著的语言准确性，但在所有语言中都落后于人类基线水平，程度不同。出乎意料的是，英语并非最佳表现语言，多个罗曼语种（包括低资源语言）系统性地优于英语表现。

Conclusion: LLMs的多语言表现受多种因素影响，包括分词策略、与西班牙语/英语的语言距离、训练数据规模以及数据来源（高/低资源语言、WEIRD/非WEIRD社区）。研究挑战了英语是LLMs最佳表现语言的默认假设，强调需要更全面的多语言评估。

Abstract: Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.

</details>


### [51] [How Retrieved Context Shapes Internal Representations in RAG](https://arxiv.org/abs/2602.20091)
*Samuel Yeh,Sharon Li*

Main category: cs.CL

TL;DR: 该论文通过分析大语言模型内部表征来研究检索增强生成中检索文档对模型表示的影响，揭示了上下文相关性和层级处理如何塑造内部表征并影响输出行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过输出行为分析检索增强生成中检索文档的影响，但对检索上下文如何塑造大语言模型内部表征以介导信息整合的过程了解甚少。在现实检索场景中，检索到的文档集通常包含相关性和有用性各异的混合文档，这些文档对模型内部表示的影响机制尚不明确。

Method: 采用潜在表征视角系统分析不同类型检索文档对大语言模型隐藏状态的影响，研究内部表征变化与下游生成行为的关系。在四个问答数据集和三个大语言模型上，通过控制单文档和多文档设置分析内部表征。

Result: 研究揭示了上下文相关性和层级处理如何影响内部表征，为理解大语言模型输出行为提供了机制解释。具体发现包括：不同相关性的文档对模型隐藏状态产生差异化影响，层级处理过程在信息整合中起关键作用，内部表征变化与最终生成质量存在相关性。

Conclusion: 通过内部表征分析为检索增强生成系统设计提供了新见解，表明理解检索上下文如何塑造模型内部表示对于优化RAG系统性能至关重要。该方法为诊断和改进检索增强生成提供了理论基础和实践指导。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.

</details>


### [52] [BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop](https://arxiv.org/abs/2602.20092)
*Leshem Choshen,Ryan Cotterell,Mustafa Omer Gul,Jaap Jumelet,Tal Linzen,Aaron Mueller,Suchir Salhan,Raj Sanjay Shah,Alex Warstadt,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: BabyLM研讨会旨在弥合认知建模与语言建模之间的界限，举办第四届BabyLM竞赛，包含通用赛道（数据高效预训练）和新增的多语言赛道，同时征集竞赛外的相关研究论文。


<details>
  <summary>Details</summary>
Motivation: 该研讨会的动机是消除认知建模与语言建模之间的界限，推动这两个领域的交叉融合。通过举办竞赛和征集论文，旨在促进数据高效预训练、认知合理性研究、弱模型评估等相关领域的发展。

Method: 通过组织第四届BabyLM竞赛，包含两个赛道：1) 通用赛道：数据高效预训练挑战；2) 新增的多语言赛道。同时征集竞赛外的相关研究论文，涵盖训练效率、认知合理性研究、弱模型评估等多个领域。

Result: 该摘要主要是一个研讨会和竞赛的征稿通知，尚未提供具体的研究结果。但可以预期通过竞赛和论文征集，将推动数据高效预训练、多语言建模、认知合理性等方面的研究进展。

Conclusion: BabyLM研讨会通过竞赛和论文征集的方式，旨在促进认知建模与语言建模的交叉研究，特别关注数据效率、多语言能力和认知合理性等关键方向，为相关领域的研究者提供交流平台。

Abstract: BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual.
  We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.

</details>


### [53] [NanoKnow: How to Know What Your Language Model Knows](https://arxiv.org/abs/2602.20122)
*Lingwei Gu,Nour Jedidi,Jimmy Lin*

Main category: cs.CL

TL;DR: NanoKnow是一个基准数据集，用于分析LLM知识来源，通过将问题划分为预训练数据中是否包含答案来区分参数化知识和外部知识。


<details>
  <summary>Details</summary>
Motivation: 理解LLM如何获取和编码知识是困难的，因为预训练数据通常是"黑箱"。nanochat模型的发布提供了完全开放的预训练数据，为解决这一问题创造了条件。

Method: 创建NanoKnow基准数据集，将Natural Questions和SQuAD中的问题根据答案是否出现在nanochat预训练语料库中进行划分。使用八个nanochat检查点进行实验，分析封闭式回答准确性和外部证据的影响。

Result: 1) 封闭式回答准确率受预训练数据中答案频率的强烈影响；2) 提供外部证据可以减轻这种频率依赖性；3) 即使有外部证据，当答案在预训练中见过时模型更准确，表明参数化知识和外部知识是互补的；4) 不相关信息是有害的，准确率随不相关上下文的位置和数量而下降。

Conclusion: NanoKnow基准能够有效区分LLM知识来源，揭示了参数化知识和外部知识的互补关系，为理解LLM如何编码和利用知识提供了新工具。

Abstract: How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a "black box" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [54] [DCInject: Persistent Backdoor Attacks via Frequency Manipulation in Personal Federated Learning](https://arxiv.org/abs/2602.18489)
*Nahom Birhan,Daniel Wesego,Dereje Shenkut,Frank Liu,Daniel Takabi*

Main category: cs.CR

TL;DR: 提出DCInject攻击方法，通过频域操作在个性化联邦学习中植入后门，在保持模型正常精度的同时实现高攻击成功率


<details>
  <summary>Details</summary>
Motivation: 虽然个性化联邦学习(PFL)被认为对后门攻击传播具有天然抵抗力，但本文揭示PFL仍然存在安全漏洞，需要研究更隐蔽的攻击方法

Method: 提出DCInject攻击：在频域中移除部分零频率(DC)分量，并用高斯分布样本替换，通过自适应频域操作实现后门植入

Result: 在四个数据集(CIFAR-10/100, GTSRB, SVHN)上实现高攻击成功率(最高100%)，同时保持清洁精度；在I-BAU防御下仍保持90.30%攻击成功率，显著优于传统空间域攻击

Conclusion: 个性化联邦学习对后门攻击并非免疫，DCInject攻击暴露了PFL安全假设中的关键漏洞，频域攻击比传统空间域攻击更具隐蔽性和持久性

Abstract: Personalized federated learning (PFL) creates client-specific models to handle data heterogeneity. Previously, PFL has been shown to be naturally resistant to backdoor attack propagation across clients. In this work, we reveal that PFL remains vulnerable to backdoor attacks through a novel frequency-domain approach. We propose DCInject, an adaptive frequency-domain backdoor attack for PFL, which removes portions of the zero-frequency (DC) component and replaces them with Gaussian-distributed samples in the frequency domain. Our attack achieves superior attack success rates while maintaining clean accuracy across four datasets (CIFAR-10/100, GTSRB, SVHN) compared to existing spatial-domain attacks, evaluated under parameter decoupling based personalization. DCInject achieves superior performance with ASRs of 96.83% (CIFAR-10), 99.38% (SVHN), and 100% (GTSRB) while maintaining clean accuracy. Under I-BAU defense, DCInject demonstrates strong persistence, retaining 90.30% ASR vs BadNet's 58.56% on VGG-16, exposing critical vulnerabilities in PFL security assumptions. Our code is available at https://github.com/NahomMA/DCINject-PFL

</details>


### [55] [Trojan Horses in Recruiting: A Red-Teaming Case Study on Indirect Prompt Injection in Standard vs. Reasoning Models](https://arxiv.org/abs/2602.18514)
*Manuel Wirth*

Main category: cs.CR

TL;DR: 该研究挑战了"推理模型更安全"的假设，通过红队测试发现推理模型在间接提示注入攻击中表现出危险的双重性：既能战略性地重构简单攻击使其更具说服力，又会在处理复杂逻辑指令时发生"元认知泄漏"。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地集成到自动化决策流程（特别是人力资源领域），间接提示注入的安全问题变得至关重要。当前普遍假设认为"推理"或"思维链"模型因其自我纠正能力而具有安全优势，但新兴研究表明这些能力可能导致更复杂的对齐失败。

Method: 采用定性红队测试方法，使用Qwen 3 30B架构，对比标准指令调优模型和推理增强模型在面对"特洛伊木马"简历时的表现。通过设计简单和复杂的攻击场景，观察两种模型的失败模式差异。

Result: 标准模型采用脆弱的幻觉来合理化简单攻击，并在复杂场景中过滤掉不合逻辑的约束。推理模型则表现出危险的双重性：1) 对简单攻击使用高级战略重构使其极具说服力；2) 面对逻辑混乱的指令时出现"元认知泄漏"，即处理复杂对抗指令的认知负荷导致注入逻辑无意中出现在最终输出中，使攻击比标准模型更容易被人类检测到。

Conclusion: 研究揭示了推理模型在安全方面的复杂权衡：虽然具备更强的推理能力，但这种能力可能被用于更隐蔽和有效的攻击，同时在某些情况下会产生新的可检测性漏洞。这挑战了"推理能力提升安全性"的简单假设，强调了需要更细致的安全评估方法。

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated decision-making pipelines, specifically within Human Resources (HR), the security implications of Indirect Prompt Injection (IPI) become critical. While a prevailing hypothesis posits that "Reasoning" or "Chain-of-Thought" Models possess safety advantages due to their ability to self-correct, emerging research suggests these capabilities may enable more sophisticated alignment failures. This qualitative Red-Teaming case study challenges the safety-through-reasoning premise using the Qwen 3 30B architecture. By subjecting both a standard instruction-tuned model and a reasoning-enhanced model to a "Trojan Horse" curriculum vitae, distinct failure modes are observed. The results suggest a complex trade-off: while the Standard Model resorted to brittle hallucinations to justify simple attacks and filtered out illogical constraints in complex scenarios, the Reasoning Model displayed a dangerous duality. It employed advanced strategic reframing to make simple attacks highly persuasive, yet exhibited "Meta-Cognitive Leakage" when faced with logically convoluted commands. This study highlights a failure mode where the cognitive load of processing complex adversarial instructions causes the injection logic to be unintentionally printed in the final output, rendering the attack more detectable by humans than in Standard Models.

</details>


### [56] [Poster: Privacy-Preserving Compliance Checks on Ethereum via Selective Disclosure](https://arxiv.org/abs/2602.18539)
*Supriya Khadka,Dhiman Goswami,Sanchari Das*

Main category: cs.CR

TL;DR: 基于以太坊的选择性披露框架，使用zk-SNARKs技术让用户在不暴露身份文档的情况下证明特定资格条件


<details>
  <summary>Details</summary>
Motivation: 数字身份验证通常迫使用户在隐私方面做出妥协，必须披露敏感个人数据来证明简单的资格标准。随着区块链应用与监管环境的整合，这种过度披露带来了数据泄露和监控的重大风险

Method: 提出基于以太坊的通用选择性披露框架，利用客户端zk-SNARKs技术，将属性验证与身份揭示解耦。通过案例研究ZK-Compliance实现年龄验证的授权、验证、撤销生命周期功能

Result: 初步结果表明，严格的合规要求可以在客户端延迟极低（<200毫秒）的情况下得到满足，同时保持公共区块链的匿名特性

Conclusion: 该框架成功解决了区块链应用中隐私与合规的平衡问题，通过选择性披露技术实现了在满足监管要求的同时保护用户隐私

Abstract: Digital identity verification often forces a privacy trade-off, where users must disclose sensitive personal data to prove simple eligibility criteria. As blockchain applications integrate with regulated environments, this over-disclosure creates significant risks of data breaches and surveillance. This work proposes a general Selective Disclosure Framework built on Ethereum, designed to decouple attribute verification from identity revelation. By utilizing client-side zk-SNARKs, the framework enables users to prove specific eligibility predicates without revealing underlying identity documents. We present a case study, ZK-Compliance, which implements a functional Grant, Verify, Revoke lifecycle for age verification. Preliminary results indicate that strict compliance requirements can be satisfied with negligible client-side latency (< 200 ms) while preserving the pseudonymous nature of public blockchains.

</details>


### [57] [Media Integrity and Authentication: Status, Directions, and Futures](https://arxiv.org/abs/2602.18681)
*Jessica Young,Sam Vaughan,Andrew Jenks,Henrique Malvar,Christian Paquin,Paul England,Thomas Roca,Juan LaVista Ferres,Forough Poursabzi,Neil Coles,Ken Archer,Eric Horvitz*

Main category: cs.CR

TL;DR: 该论文综述了区分AI生成媒体与真实相机/麦克风捕获内容的完整性认证方法，分析了来源验证、水印和指纹三种技术，评估了跨模态操作、威胁模型和现实工作流程，并探讨了对抗反转攻击的鲁棒性验证系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成媒体技术的快速发展，区分AI生成内容与真实捕获内容成为媒体完整性和认证领域的新挑战。需要评估现有方法在应对技术和社会心理操纵方面的有效性。

Method: 论文首先定义了来源验证、水印和指纹三种认证方法，然后重点分析了三种代表性技术：加密安全来源验证、不可感知水印和软哈希指纹。通过评估跨模态操作、威胁模型（包括技术攻击和社会心理反转攻击）以及从捕获、编辑、分发到验证的完整工作流程来系统分析这些工具。

Result: 分析表明，需要能够抵抗技术和社会心理反转攻击的鲁棒验证系统。社会心理反转攻击能够颠倒完整性信号，使真实内容看起来像合成的，反之亦然。论文还提出了通过安全飞地等技术加强边缘设备安全，实现高置信度来源认证的方法。

Conclusion: 媒体完整性认证需要综合考虑技术和社会心理因素。未来方向包括开发能够抵抗反转攻击的鲁棒验证系统，以及通过安全飞地等技术加强边缘设备安全，实现可靠的高置信度来源认证。

Abstract: We provide background on emerging challenges and future directions with media integrity and authentication methods, focusing on distinguishing AI-generated media from authentic content captured by cameras and microphones. We evaluate several approaches, including provenance, watermarking, and fingerprinting. After defining each method, we analyze three representative technologies: cryptographically secured provenance, imperceptible watermarking, and soft-hash fingerprinting. We analyze how these tools operate across modalities and evaluate relevant threat models, attack categories, and real-world workflows spanning capture, editing, distribution, and verification. We consider sociotechnical reversal attacks that can invert integrity signals, making authentic content appear synthetic and vice versa, highlighting the value of verification systems that are resilient to both technical and psychosocial manipulation. Finally, we outline techniques for delivering high-confidence provenance authentication, including directions for strengthening edge-device security using secure enclaves.

</details>


### [58] [Watermarking LLM Agent Trajectories](https://arxiv.org/abs/2602.18700)
*Wenlong Meng,Chen Gong,Terry Yue Zhuo,Fan Zhang,Kecen Li,Zheng Liu,Zhou Yang,Chengkun Wei,Wenzhi Chen*

Main category: cs.CR

TL;DR: ActHook：首个针对LLM智能体轨迹数据集的水印方法，通过植入钩子动作实现版权保护，不影响原始任务结果，在数学推理、网页搜索和软件工程智能体上平均检测AUC达94.3%


<details>
  <summary>Details</summary>
Motivation: LLM智能体依赖高质量轨迹数据指导问题解决行为，但生成这些数据成本高昂（任务设计、大模型生成、人工筛选）。现有文献忽视了LLM智能体轨迹的版权保护，导致创作者面临数据盗窃风险，难以追踪滥用或维护所有权。

Method: 受软件工程中钩子机制启发，ActHook嵌入钩子动作，这些动作由秘密输入密钥激活且不改变原始任务结果。LLM智能体按顺序操作，允许在决策点插入钩子动作而不中断任务流程。当激活密钥存在时，训练在水印轨迹上的LLM智能体会以显著更高的频率产生这些钩子动作，实现可靠的黑盒检测。

Result: 在数学推理、网页搜索和软件工程智能体上的实验表明，ActHook在Qwen-2.5-Coder-7B上平均检测AUC达到94.3%，同时性能下降可忽略不计。

Conclusion: ActHook是首个专门为智能体轨迹数据集设计的水印方法，有效解决了LLM智能体轨迹数据的版权保护问题，在保持任务性能的同时实现了可靠的版权检测。

Abstract: LLM agents rely heavily on high-quality trajectory data to guide their problem-solving behaviors, yet producing such data requires substantial task design, high-capacity model generation, and manual filtering. Despite the high cost of creating these datasets, existing literature has overlooked copyright protection for LLM agent trajectories. This gap leaves creators vulnerable to data theft and makes it difficult to trace misuse or enforce ownership rights. This paper introduces ActHook, the first watermarking method tailored for agent trajectory datasets. Inspired by hook mechanisms in software engineering, ActHook embeds hook actions that are activated by a secret input key and do not alter the original task outcome. Like software execution, LLM agents operate sequentially, allowing hook actions to be inserted at decision points without disrupting task flow. When the activation key is present, an LLM agent trained on watermarked trajectories can produce these hook actions at a significantly higher rate, enabling reliable black-box detection. Experiments on mathematical reasoning, web searching, and software engineering agents show that ActHook achieves an average detection AUC of 94.3 on Qwen-2.5-Coder-7B while incurring negligible performance degradation.

</details>


### [59] [UFO: Unlocking Ultra-Efficient Quantized Private Inference with Protocol and Algorithm Co-Optimization](https://arxiv.org/abs/2602.18758)
*Wenxuan Zeng,Chao Yang,Tianshi Xu,Bo Zhang,Changrui Ren,Jin Dong,Meng Li*

Main category: cs.CR

TL;DR: UFO是一个量化两方计算推理框架，通过联合优化2PC协议和量化算法，结合Winograd卷积和量化来提升私有CNN推理效率。


<details>
  <summary>Details</summary>
Motivation: 基于安全两方计算的私有CNN推理存在高通信和延迟开销，特别是卷积层。现有方法中，简单结合量化和Winograd卷积面临通信开销大和模型精度下降的挑战。

Method: 1) 协议层面：提出图级优化最小化通信开销；2) 算法层面：开发基于层敏感度的混合精度量化感知训练算法，并引入2PC友好的比特重加权算法处理异常值。

Result: 相比最先进框架SiRNN、COINN和CoPriv，UFO分别实现11.7倍、3.6倍和6.3倍的通信减少，同时精度分别提高1.29%、1.16%和1.29%。

Conclusion: UFO通过联合优化2PC协议和量化算法，有效解决了私有CNN推理中的通信开销和精度平衡问题，显著提升了推理效率和模型准确性。

Abstract: Private convolutional neural network (CNN) inference based on secure two-party computation (2PC) suffers from high communication and latency overhead, especially from convolution layers. In this paper, we propose UFO, a quantized 2PC inference framework that jointly optimizes the 2PC protocols and quantization algorithm. UFO features a novel 2PC protocol that systematically combines the efficient Winograd convolution algorithm with quantization to improve inference efficiency. However, we observe that naively combining quantization and Winograd convolution faces the following challenges: 1) From the inference perspective, Winograd transformations introduce extensive additions and require frequent bit width conversions to avoid inference overflow, leading to non-negligible communication overhead; 2) From the training perspective, Winograd transformations introduce weight outliers that make quantization-aware training (QAT) difficult, resulting in inferior model accuracy. To address these challenges, we co-optimize both protocol and algorithm. 1) At the protocol level, we propose a series of graph-level optimizations for 2PC inference to minimize the communication. 2) At the algorithm level, we develop a mixed-precision QAT algorithm based on layer sensitivity to optimize model accuracy given communication constraints. To accommodate the outliers, we further introduce a 2PC-friendly bit re-weighting algorithm to increase the representation range without explicitly increasing bit widths. With extensive experiments, UFO demonstrates 11.7x, 3.6x, and 6.3x communication reduction with 1.29%, 1.16%, and 1.29% higher accuracy compared to state-of-the-art frameworks SiRNN, COINN, and CoPriv, respectively.

</details>


### [60] [MANATEE: Inference-Time Lightweight Diffusion Based Safety Defense for LLMs](https://arxiv.org/abs/2602.18782)
*Chun Yan Ryan Kan,Tommy Tran,Vedant Yadav,Ava Cai,Kevin Zhu,Ruizhe Li,Maheep Chaudhary*

Main category: cs.CR

TL;DR: MANATEE是一种基于密度估计的推理时防御方法，通过扩散过程将异常隐藏状态投影到安全区域，有效防御LLM对抗性越狱攻击，无需有害训练数据或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对抗性越狱攻击防御方法存在局限性：基于二元分类器的方法在对抗性输入超出学习决策边界时失效；重复微调计算成本高且可能降低模型能力。需要一种更有效、轻量级的防御方案。

Method: MANATEE是一种推理时防御方法，基于良性表示流形的密度估计。它学习良性隐藏状态的得分函数，使用扩散过程将异常表示投影到安全区域。该方法无需有害训练数据，无需架构修改。

Result: 在Mistral-7B-Instruct、Llama-3.1-8B-Instruct和Gemma-2-9B-it上的实验表明，MANATEE在某些数据集上可将攻击成功率降低高达100%，同时保持良性输入上的模型效用。

Conclusion: MANATEE提供了一种有效、轻量级的LLM对抗性越狱攻击防御方案，通过密度估计和扩散投影实现推理时保护，在保持模型效用的同时显著降低攻击成功率。

Abstract: Defending LLMs against adversarial jailbreak attacks remains an open challenge. Existing defenses rely on binary classifiers that fail when adversarial input falls outside the learned decision boundary, and repeated fine-tuning is computationally expensive while potentially degrading model capabilities. We propose MANATEE, an inference-time defense that uses density estimation over a benign representation manifold. MANATEE learns the score function of benign hidden states and uses diffusion to project anomalous representations toward safe regions--requiring no harmful training data and no architectural modifications. Experiments across Mistral-7B-Instruct, Llama-3.1-8B-Instruct, and Gemma-2-9B-it demonstrate that MANATEE reduce Attack Success Rate by up to 100\% on certain datasets, while preserving model utility on benign inputs.

</details>


### [61] [Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection](https://arxiv.org/abs/2602.19025)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali. A Ghorbani*

Main category: cs.CR

TL;DR: 该论文提出了一种用于恶意软件检测的混合专家图模型，通过节点级多统计量编码和专家级多样性构建，结合路由感知解释机制，在控制流图上实现透明决策。


<details>
  <summary>Details</summary>
Motivation: 研究混合专家图模型在恶意软件检测中的路由感知解释问题，旨在提高模型决策的透明度，使解释能够反映每个专家关注的内容及其被选择的重要性。

Method: 提出双层多样性架构：1) 节点级：计算多个邻域统计量，通过MLP融合，使用度重加权因子ρ和池化选择λ生成互补节点表示；2) 读出级：六个专家各自对应特定(ρ, λ)视图，输出图级logits，路由器加权得到最终预测。后验解释通过专家级边归因聚合路由器门控生成。

Result: 在相同CFG数据集上评估，相比GCN、GIN、GAT等单专家GNN基线，提出的MoE模型在保持强检测准确率的同时，在稀疏性扰动下产生稳定、忠实的归因。路由器显式化和多统计量节点编码与专家级多样性结合能提高恶意软件分析的透明度。

Conclusion: 通过构建节点级多统计量编码和专家级多样性，结合路由感知解释机制，混合专家图模型能够在恶意软件检测中实现准确预测并提供透明决策，为恶意软件分析中的MoE决策透明度提供了改进方案。

Abstract: Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis.

</details>


### [62] [SiGRRW: A Single-Watermark Robust Reversible Watermarking Framework with Guiding Strategy](https://arxiv.org/abs/2602.19097)
*Zikai Xu,Bin Liu,Weihai Li,Lijunxian Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: SiGRRW是一种单水印鲁棒可逆水印框架，通过引导策略在单次水印中同时实现鲁棒性和可逆性，适用于生成模型和自然图像。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒可逆水印方案通常采用两阶段框架，无法在单次水印中同时实现鲁棒性和可逆性，且两个水印之间的功能干扰导致容量和不可感知性等多方面性能下降。

Method: 提出单水印RRW框架SiGRRW，引入新颖的引导策略生成引导图像作为嵌入和恢复的指导。水印通过引导残差可逆嵌入，该残差可从原始图像和水印图像计算得出。框架可作为生成模型输出阶段的即插即用水印层，或直接应用于自然图像。

Result: 大量实验表明，SiGRRW相比现有RRW方案有效提升了不可感知性和鲁棒性，同时保持原始图像的无损恢复，容量显著高于传统方案。

Conclusion: SiGRRW通过单水印框架解决了传统两阶段方案的功能干扰问题，在保持可逆性的同时显著提升了水印性能，为生成模型和自然图像提供了统一的鲁棒可逆水印解决方案。

Abstract: Robust reversible watermarking (RRW) enables copyright protection for images while overcoming the limitation of distortion introduced by watermark itself. Current RRW schemes typically employ a two-stage framework, which fails to achieve simultaneous robustness and reversibility within a single watermarking, and functional interference between the two watermarks results in performance degradation in multiple terms such as capacity and imperceptibility. We propose SiGRRW, a single-watermark RRW framework, which is applicable to both generative models and natural images. We introduce a novel guiding strategy to generate guiding images, serving as the guidance for embedding and recovery. The watermark is reversibly embedded with the guiding residual, which can be calculated from both cover images and watermark images. The proposed framework can be deployed either as a plug-and-play watermarking layer at the output stage of generative models, or directly applied to natural images. Extensive experiments demonstrate that SiGRRW effectively enhances imperceptibility and robustness compared to existing RRW schemes while maintaining lossless recovery of cover images, with significantly higher capacity than conventional schemes.

</details>


### [63] [ReVision : A Post-Hoc, Vision-Based Technique for Replacing Unacceptable Concepts in Image Generation Pipeline](https://arxiv.org/abs/2602.19149)
*Gurjot Singh,Prabhjot Singh,Aashima Sharma,Maninder Singh,Ryan Ko*

Main category: cs.CR

TL;DR: ReVision：一种无需训练、基于提示的后处理安全框架，通过VLM辅助空间门控机制实现精确的局部语义编辑，作为图像生成管道的最后一道防线。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成安全缓解策略（如提示过滤、安全感知训练/微调）存在可被绕过且会降低生成质量的问题，特别是在多概念场景中，后处理编辑方法通常依赖不精确的空间定位，影响可用性和部署性。

Method: ReVision使用Gemini-2.5-Flash作为通用策略违规概念检测器，避免依赖多个类别特定检测器，并引入VLM辅助空间门控机制实现实例一致定位，进行局部语义编辑来替换不安全内容。

Result: 在245张图像的基准测试中，ReVision：(i) 将安全提示的CLIP对齐度平均提高+0.121；(ii) 显著改善多概念背景保真度（LPIPS从0.166降至0.058）；(iii) 在类别特定检测器上实现近乎完全抑制（如NudeNet从70.51降至0）；(iv) 在人工审核研究中将策略违规内容可识别性从95.99%降至10.16%。

Conclusion: ReVision作为训练免费、基于提示的后处理安全框架，通过精确的局部语义编辑有效抑制策略违规内容，同时保持场景完整性，为图像生成管道提供了可靠的最后一道防线。

Abstract: Image-generative models are widely deployed across industries. Recent studies show that they can be exploited to produce policy-violating content. Existing mitigation strategies primarily operate at the pre- or mid-generation stages through techniques such as prompt filtering and safety-aware training/fine-tuning. Prior work shows that these approaches can be bypassed and often degrade generative quality. In this work, we propose ReVision, a training-free, prompt-based, post-hoc safety framework for image-generation pipeline. ReVision acts as a last-line defense by analyzing generated images and selectively editing unsafe concepts without altering the underlying generator. It uses the Gemini-2.5-Flash model as a generic policy-violating concept detector, avoiding reliance on multiple category-specific detectors, and performs localized semantic editing to replace unsafe content. Prior post-hoc editing methods often rely on imprecise spatial localization, that undermines usability and limits deployability, particularly in multi-concept scenes. To address this limitation, ReVision introduces a VLM-assisted spatial gating mechanism that enforces instance-consistent localization, enabling precise edits while preserving scene integrity. We evaluate ReVision on a 245-image benchmark covering both single- and multi-concept scenarios. Results show that ReVision (i) improves CLIP-based alignment toward safe prompts by +$0.121$ on average; (ii) significantly improves multi-concept background fidelity (LPIPS $0.166 \rightarrow 0.058$); (iii) achieves near-complete suppression on category-specific detectors (e.g., NudeNet $70.51 \rightarrow 0$); and (iv) reduces policy-violating content recognizability in a human moderation study from $95.99\%$ to $10.16\%$.

</details>


### [64] [KUDA: Knowledge Unlearning by Deviating Representation for Large Language Models](https://arxiv.org/abs/2602.19275)
*Ce Fang,Zhikun Zhang,Min Chen,Qing Liu,Lu Zhou,Zhe Liu,Yunjun Gao*

Main category: cs.CR

TL;DR: KUDA是一种新的LLM知识遗忘方法，通过因果追踪定位知识存储层，设计表示偏离目标实现知识移除，采用松弛零空间投影缓解优化冲突，在WMDP和MUSE基准上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过大规模预训练获得丰富知识，但也放大了训练数据中敏感、版权或有害内容的风险。现有LLM遗忘方法往往导致模型生成随机或不连贯回答，因为它们无法精确改变编码的知识。需要一种在知识层面实现有效遗忘的方法。

Method: 提出KUDA方法：1)使用因果追踪定位目标知识存储的特定层；2)设计新的遗忘目标，诱导模型表示偏离原始位置，破坏与目标知识的关联能力；3)采用松弛零空间投影机制缓解遗忘与保留之间的优化冲突，减少对保留知识表示空间的干扰。

Result: 在WMDP和MUSE等代表性基准上的广泛实验表明，KUDA在有效平衡知识移除和模型效用保留方面优于大多数现有基线方法。

Conclusion: KUDA通过精确定位知识存储层、设计表示偏离目标和采用松弛零空间投影，实现了在知识层面的有效遗忘，为解决LLM中的敏感、版权和有害内容风险提供了有前景的技术方案。

Abstract: Large language models (LLMs) acquire a large amount of knowledge through pre-training on vast and diverse corpora. While this endows LLMs with strong capabilities in generation and reasoning, it amplifies risks associated with sensitive, copyrighted, or harmful content in training data.LLM unlearning, which aims to remove specific knowledge encoded within models, is a promising technique to reduce these risks. However, existing LLM unlearning methods often force LLMs to generate random or incoherent answers due to their inability to alter the encoded knowledge precisely. To achieve effective unlearning at the knowledge level of LLMs, we propose Knowledge Unlearning by Deviating representAtion (KUDA). We first utilize causal tracing to locate specific layers for target knowledge storage. We then design a new unlearning objective that induces the model's representations to deviate from its original position in the phase of knowledge removal, thus disrupting the ability to associate with the target knowledge. To resolve the optimization conflicts between forgetting and retention, we employ a relaxation null-space projection mechanism to mitigate the disruption to the representation space of retaining knowledge. Extensive experiments on representative benchmarks, WMDP and MUSE, demonstrate that KUDA outperforms most existing baselines by effectively balancing knowledge removal and model utility retention.

</details>


### [65] [Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments](https://arxiv.org/abs/2602.19450)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: 论文提出TEE-RedBench评估框架，研究LLM作为TEE安全顾问时的风险，发现提示诱导的失败在LLM间可转移达12.02%，并提出"LLM-in-the-loop"管道将失败率降低80.62%


<details>
  <summary>Details</summary>
Motivation: TEEs（如Intel SGX和Arm TrustZone）旨在保护敏感计算免受受损操作系统影响，但实际部署仍面临微架构泄漏、侧信道攻击和故障注入等风险。同时，安全团队越来越多依赖LLM助手作为TEE架构审查、缓解计划制定和漏洞分诊的安全顾问，这产生了社会技术风险表面：助手可能产生TEE机制幻觉、过度保证（如认证功能）或在对抗性提示下表现不安全

Method: 提出TEE-RedBench评估方法学，包括：(1) LLM介导安全工作的TEE特定威胁模型；(2) 结构化提示套件，涵盖SGX和TrustZone架构、认证和密钥管理、威胁建模、非操作缓解指导以及政策约束的滥用探测；(3) 联合测量技术正确性、基础性、不确定性校准、拒绝质量和安全帮助性的注释标准。研究聚焦ChatGPT-5.2和Claude Opus-4.6两个主流LLM助手

Result: 研究发现某些失败并非纯粹特异性的，在LLM助手间的转移率高达12.02%。通过"LLM-in-the-loop"评估管道（政策门控、检索基础、结构化模板和轻量验证检查的组合）可将失败减少80.62%

Conclusion: LLM作为TEE安全顾问存在显著风险，包括幻觉、过度保证和对抗性提示下的不安全行为。提出的TEE-RedBench框架和"LLM-in-the-loop"管道能有效评估和缓解这些风险，为安全团队提供实用的安全增强方法

Abstract: Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting.
  We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs. We introduce TEE-RedBench, a TEE-grounded evaluation methodology comprising (i) a TEE-specific threat model for LLM-mediated security work, (ii) a structured prompt suite spanning SGX and TrustZone architecture, attestation and key management, threat modeling, and non-operational mitigation guidance, along with policy-bound misuse probes, and (iii) an annotation rubric that jointly measures technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that some failures are not purely idiosyncratic, transferring up to 12.02% across LLM assistants, and we connect these outcomes to secure architecture by outlining an "LLM-in-the-loop" evaluation pipeline: policy gating, retrieval grounding, structured templates, and lightweight verification checks that, when combined, reduce failures by 80.62%.

</details>


### [66] [Hardware-Friendly Randomization: Enabling Random-Access and Minimal Wiring in FHE Accelerators with Low Total Cost](https://arxiv.org/abs/2602.19550)
*Ilan Rosenfeld,Noam Kleinburd,Hillel Chapman,Dror Reuven*

Main category: cs.CR

TL;DR: 提出一种基于种子的RLWE参数a生成方案，减少通信开销和硬件实现复杂度，同时保持安全性和低客户端开销


<details>
  <summary>Details</summary>
Motivation: RLWE问题中的随机多项式a在FHE方案中通信开销大，特别是在客户端到服务器的传输和硬件加速器输入过程中。现有基于种子的生成方法在硬件实现中面临布线密度、功耗、计算面积和调度灵活性等挑战。

Method: 提出具体的基于种子的方案和参数，在加速器内部动态生成a。方案支持并行生成均匀分布样本，放宽布线要求，允许无限制的RNS limb随机访问，并采用极低开销的客户端密钥生成过程。

Result: 方案在保持通信延迟和内存占用减少的同时，消除了对厚金属层进行随机性分布的需求，防止PRNG子系统功耗随加速因子呈指数增长，在高速配置中可节省数十瓦功耗，客户端开销低于3%。

Conclusion: 提出的方案有效解决了RLWE中随机多项式a的通信和硬件实现挑战，为高效FHE加速器设计提供了实用解决方案，显著降低了功耗和实现复杂度。

Abstract: The Ring-Learning With Errors (RLWE) problem forms the backbone of highly efficient Fully Homomorphic Encryption (FHE) schemes. A significant component of the RLWE public key and ciphertext of the form $(b,a)$ is the uniformly random polynomial $a \in R_q$ . While essential for security, the communication overhead of transmitting $a$ from client to server, and inputting it into a hardware accelerator, can be substantial, especially for FHE accelerators aiming at high acceleration factors. A known technique in reducing this overhead generates $a$ from a small seed on the client side via a deterministic process, transmits only the seed, and generates $a$ on-the-fly within the accelerator. Challenges in the hardware implementation of an accelerator include wiring (density and power), compute area, compute power as well as flexibility in scheduling of on-the-fly generation instructions. This extended abstract proposes a concrete scheme and parameters wherein these practical challenges are addressed. We detail the benefits of our approach, which maintains the reduction in communication latency and memory footprint, while allowing parallel generation of uniformly distributed samples, relaxed wiring requirements, unrestricted randomaccess to RNS limbs, and results in an extremely low overhead on the client side (i.e. less than 3%) during the key generation process. The proposed scheme eliminates the need for thick metal layers for randomness distribution and prevents the power consumption of the PRNG subsystem from scaling prohibitively with the acceleration factor, potentially saving tens of Watts per accelerator chip in high-throughput configurations.

</details>


### [67] [Efficient Multi-Party Secure Comparison over Different Domains with Preprocessing Assistance](https://arxiv.org/abs/2602.19604)
*Kaiwen Wang,Xiaolin Chang,Yuehan Dong,Ruichen Zhang*

Main category: cs.CR

TL;DR: 该论文提出了首个经销商辅助的n方LTBits和MSB提取协议，在𝔽ₚ和ℤ₂ᵏ上实现协议级完美安全，通过充分利用经销商生成丰富相关随机性的能力，显著提升了安全比较协议的在线阶段性能。


<details>
  <summary>Details</summary>
Motivation: 安全比较是多方计算中的基础原语，但现有协议的预处理阶段性能瓶颈严重。虽然近期引入了被动非共谋经销商来加速预处理，但仍存在两个关键问题：1) 现有经销商辅助方法仅将经销商作为传统预处理的替代，未重新设计比较协议以优化在线阶段；2) 大多数协议针对特定代数域、敌手模型或参与方配置，缺乏广泛通用性。

Method: 提出了首个经销商辅助的n方LTBits（小于位）和MSB（最高有效位）提取协议，支持𝔽ₚ和ℤ₂ᵏ两种代数域。通过充分利用经销商生成丰富相关随机性的能力，𝔽ₚ构造实现了常数轮在线复杂度，ℤ₂ᵏ构造实现了O(logₙ k)轮次且具有可调分支因子。所有协议通过扩展的ABB模型作为黑盒构造实现，确保跨MPC后端和敌手模型的可移植性。

Result: 实验结果表明，与最先进的MPC框架相比，实现了1.79倍到19.4倍的加速，证明了协议在比较密集型MPC应用中的实用性。协议在协议级别实现了完美安全性。

Conclusion: 该工作通过充分利用经销商生成相关随机性的能力，设计了高效的安全比较协议，解决了现有方法的局限性。提出的协议具有广泛通用性、高性能和可移植性，为隐私保护应用如机器学习和数据分析提供了实用的解决方案。

Abstract: Secure comparison is a fundamental primitive in multi-party computation, supporting privacy-preserving applications such as machine learning and data analytics. A critical performance bottleneck in comparison protocols is their preprocessing phase, primarily due to the high cost of generating the necessary correlated randomness. Recent frameworks introduce a passive, non-colluding dealer to accelerate preprocessing. However, two key issues still remain. First, existing dealer-assisted approaches treat the dealer as a drop-in replacement for conventional preprocessing without redesigning the comparison protocol to optimize the online phase. Second, most protocols are specialized for particular algebraic domains, adversary models, or party configurations, lacking broad generality. In this work, we present the first dealer-assisted $n$-party LTBits (Less-Than-Bits) and MSB (Most Significant Bit) extraction protocols over both $\mathbb{F}_p$ and $\mathbb{Z}_{2^k}$, achieving perfect security at the protocol level. By fully exploiting the dealer's capability to generate rich correlated randomness, our $\mathbb{F}_p$ construction achieves constant-round online complexity and our $\mathbb{Z}_{2^k}$ construction achieves $O(\log_n k)$ rounds with tunable branching factor. All protocols are formulated as black-box constructions via an extended ABB model, ensuring portability across MPC backends and adversary models. Experimental results demonstrate $1.79\times$ to $19.4\times$ speedups over state-of-the-art MPC frameworks, highlighting the practicality of our protocols for comparison-intensive MPC applications.

</details>


### [68] [AegisSat: Securing AI-Enabled SoC FPGA Satellite Platforms](https://arxiv.org/abs/2602.19777)
*Huimin Li,Vusal Novruzov,Nikhilesh Singh,Lichao Wu,Mohamadreza Rostami,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: AegisSat：面向SoC FPGA卫星系统的安全框架，通过可信启动、运行时隔离、认证更新和回滚机制，保障AI卫星系统的安全性和弹性。


<details>
  <summary>Details</summary>
Motivation: 随着SoC FPGA在AI卫星系统中的广泛应用，其可重配置性和在轨更新能力带来了严重的安全风险，包括性能降级、服务中断和任务结果被恶意操控等威胁。

Method: 提出AegisSat安全框架，包含四个核心机制：(1)基于密码学的安全启动建立可信计算基；(2)严格的运行时资源隔离；(3)在轨重配置和AI模型更新的认证程序；(4)强大的回滚能力以恢复启动和更新失败。

Result: 在当代SoC FPGA设备上进行了实验验证，成功集成了所提出的安全机制，证明了框架的可行性和有效性。

Conclusion: AegisSat为空间应用提供了深度防御框架，在物理访问不可行且系统需长期可靠运行的场景下，增强了SoC FPGA卫星系统的可信度，实现了安全弹性的在轨AI操作。

Abstract: The increasing adoption of System-on-Chip Field-Programmable Gate Arrays (SoC FPGAs) in AI-enabled satellite systems, valued for their reconfigurability and in-orbit update capabilities, introduces significant security challenges. Compromised updates can lead to performance degradation, service disruptions, or adversarial manipulation of mission outcomes. To address these risks, this paper proposes a comprehensive security framework, AegisSat. It ensures the integrity and resilience of satellite platforms by (i) integrating cryptographically-based secure boot mechanisms to establish a trusted computing base; (ii) enforcing strict runtime resource isolation; (iii) employing authenticated procedures for in-orbit reconfiguration and AI model updates to prevent unauthorized modifications; and (iv) providing robust rollback capabilities to recover from boot and update failures and maintain system stability. To further support our claims, we conducted experiments demonstrating the integration of these mechanisms on contemporary SoC FPGA devices. This defense-in-depth framework is crucial for space applications, where physical access is impossible and systems must operate reliably over extended periods, thereby enhancing the trustworthiness of SoC FPGA-based satellite systems and enabling secure and resilient AI operations in orbit.

</details>


### [69] [SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models](https://arxiv.org/abs/2602.19818)
*Hillel Ohayon,Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: 提出基于机器学习的轻量级扫描器，用于检测恶意Pickle文件，无需策略生成或代码插桩，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Hugging Face等模型仓库使用Python pickle格式分发机器学习模型，存在远程代码执行风险。现有防御方法如PickleBall需要复杂的系统设置和验证良性模型，可扩展性和泛化性有限。

Method: 静态提取Pickle字节码的结构和语义特征，应用监督和无监督模型对文件进行良性/恶意分类。构建并发布了包含727个Pickle文件的标注数据集。

Result: 在自有数据集上达到90.01% F1分数，优于现有扫描器（7.23%-62.75%）；在PickleBall数据（OOD）上达到81.22% F1分数，优于PickleBall方法的76.09%；在Hide-and-Seek数据集上正确分类9/9个恶意模型。

Conclusion: 数据驱动的检测方法能够有效且通用地缓解基于Pickle的模型文件攻击，无需依赖特定库策略，具有更好的可扩展性和泛化能力。

Abstract: Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks.

</details>


### [70] [Quantum approaches to learning parity with noise](https://arxiv.org/abs/2602.19819)
*Daniel Shiu*

Main category: cs.CR

TL;DR: 论文探讨了用量子方法（特别是Simon算法）解决LPN问题的可能性，通过构造接近Simon承诺的函数来生成新的LPN样本，从而迭代简化问题。


<details>
  <summary>Details</summary>
Motivation: LPN问题是后量子密码学（如HQC和Classic McEliece）安全性的关键基础。经典攻击方法（信息集解码）对感兴趣的参数具有指数复杂度，因此需要探索量子方法是否能提供替代方案。

Method: 受Regev将格问题与隐藏二面子群问题关联的启发，使用二进制域的邻域构造接近满足Simon承诺的函数，其差值等于秘密奇偶向量。运行Simon算法本质上生成新的LPN样本，通过忽略变量迭代简化问题。

Result: 该方法理论上能够生成新的LPN样本，为迭代简化问题提供可能，但未声称在效率上能与现有方法竞争，仅表明值得深入研究。

Conclusion: 量子方法（特别是Simon算法）为解决LPN问题提供了新的研究方向，虽然不一定比现有方法更具竞争力，但值得进一步深入探索。

Abstract: The learning parity with noise (LPN) problem is a well-established computational challenge whose difficulty is critical to the security of several post-quantum cryptographic primitives such as HQC and Classic McEliece. Classically, the best-known attacks involve information set decoding methods which are exponential in complexity for parameterisations of interest. In this paper we investigate whether quantum methods might offer alternative approaches. The line of inquiry is inspired by Regev's relating of certain lattice problems to the hidden dihedral subgroup problem. We use neighbourhoods of binary fields to produce a function close to fulfilling Simon's promise with difference equal to the secret parity vector. Although unlikely to recover the secret parity vector directly, running Simon's algorithm essentially produces new LPN samples. This gives the hope that we might be able to produce enough new samples to ignore one or more variables and iteratively reduce the problem.
  We make no claim that these methods will necessarily be competitive with existing approaches, merely that they warrant deeper investigation.

</details>


### [71] [An Explainable Memory Forensics Approach for Malware Analysis](https://arxiv.org/abs/2602.19831)
*Silvia Lucia Sanna,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 提出基于大语言模型的可解释AI辅助内存取证方法，用于分析恶意软件内存转储，自动提取IoC指标并生成人类可读的解释


<details>
  <summary>Details</summary>
Motivation: 现有内存取证自动化模型缺乏可解释性，依赖专家手动分析复杂工具输出，需要更智能、可解释的辅助分析工具

Method: 利用通用大语言模型解释内存分析输出，自动提取有意义的IoC指标，支持Windows和Android恶意软件分析，比较完整RAM获取与目标进程内存转储

Result: 在某些情况下检测到比当前最先进工具更多的IoC指标，LLM能够支持专家和非专家分析师解释结果、关联证据并证明恶意软件分类

Conclusion: LLM辅助的人类参与工作流程提高了可重复性并降低了操作复杂性，增强了AI驱动内存取证在现代恶意软件调查中的实际适用性

Abstract: Memory forensics is an effective methodology for analyzing living-off-the-land malware, including threats that employ evasion, obfuscation, anti-analysis, and steganographic techniques. By capturing volatile system state, memory analysis enables the recovery of transient artifacts such as decrypted payloads, executed commands, credentials, and cryptographic keys that are often inaccessible through static or traditional dynamic analysis. While several automated models have been proposed for malware detection from memory, their outputs typically lack interpretability, and memory analysis still relies heavily on expert-driven inspection of complex tool outputs, such as those produced by Volatility. In this paper, we propose an explainable, AI-assisted memory forensics approach that leverages general-purpose large language models (LLMs) to interpret memory analysis outputs in a human-readable form and to automatically extract meaningful Indicators of Compromise (IoCs), in some circumstances detecting more IoCs than current state-of-the-art tools. We apply the proposed methodology to both Windows and Android malware, comparing full RAM acquisition with target-process memory dumping and highlighting their complementary forensic value. Furthermore, we demonstrate how LLMs can support both expert and non-expert analysts by explaining analysis results, correlating artifacts, and justifying malware classifications. Finally, we show that a human-in-the-loop workflow, assisted by LLMs during kernel-assisted setup and analysis, improves reproducibility and reduces operational complexity, thereby reinforcing the practical applicability of AI-driven memory forensics for modern malware investigations.

</details>


### [72] [LLM-enabled Applications Require System-Level Threat Monitoring](https://arxiv.org/abs/2602.19844)
*Yedi Zhang,Haoyu Wang,Xianglin Yang,Jin Song Dong,Jun Sun*

Main category: cs.CR

TL;DR: 该立场论文主张对LLM应用进行系统性安全威胁监控，将其作为可靠部署的先决条件和事件响应框架的基础


<details>
  <summary>Details</summary>
Motivation: LLM作为核心推理组件重塑软件生态系统，但其非确定性、学习驱动和难以验证的特性引入了新的可靠性挑战和安全攻击面，这些风险应被视为预期操作条件而非异常事件

Method: 提出从事件响应视角出发，建立系统级威胁监控机制，能够检测和情境化部署后的安全相关异常，超越传统的测试或护栏防御

Result: 识别了当前LLM应用安全监控的空白，指出可信部署的主要障碍不是进一步提升模型能力，而是建立有效的系统级威胁监控

Conclusion: LLM应用的安全威胁监控应作为可靠操作的前提条件，并为专门的事件响应框架奠定基础，这是当前研究不足但至关重要的方向

Abstract: LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.

</details>


### [73] [Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios](https://arxiv.org/abs/2602.20061)
*Zoha Hayat Bhatti,Bakhtawar Ahtisham,Seemal Tausif,Niklas George,Nida ul Habib Bajwa,Mobin Javed*

Main category: cs.CR

TL;DR: 参与者无法可靠区分AI生成与人类录音的诈骗语音，准确率仅37.5%，低于随机水平，表明传统声音线索在当代语音诈骗中已失效


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和商业语音合成系统的发展，AI生成的语音诈骗（vishing）变得高度逼真，引发对大规模欺骗的担忧。需要研究人们是否能在真实诈骗场景中可靠区分AI生成与人类录音的语音，以及其判断背后的感知策略。

Method: 进行在线对照研究，22名参与者评估16个诈骗风格音频片段（8个AI生成，8个人类录音），对每个片段分类为人类或AI并报告置信度。使用信号检测理论分析区分能力，并对315个编码摘录进行定性分析。

Result: 参与者表现极差：平均准确率仅37.5%，低于二元分类的随机水平。双向误分类：75%的AI生成片段被多数标记为人类，62.5%的人类录音片段被多数标记为AI。信号检测理论显示区分能力接近零（d'≈0）。定性分析显示参与者依赖副语言和情感启发式（停顿、填充词、声音变化、节奏、情感表达），但这些传统的人类真实性线索常被AI样本复制。误分类常伴随中高置信度，表明感知校准错误而非不确定性。

Conclusion: 基于声音启发式的真实性判断在当代语音诈骗场景中不可靠。研究结果对安全干预、用户教育和AI介导欺骗缓解具有重要意义，表明需要开发新的检测方法和教育策略来应对高度逼真的AI生成语音威胁。

Abstract: Large Language Models and commercial speech synthesis systems now enable highly realistic AI-generated voice scams (vishing), raising urgent concerns about deception at scale. Yet it remains unclear whether individuals can reliably distinguish AI-generated speech from human-recorded voices in realistic scam contexts and what perceptual strategies underlie their judgments. We conducted a controlled online study in which 22 participants evaluated 16 vishing-style audio clips (8 AI-generated, 8 human-recorded) and classified each as human or AI while reporting confidence. Participants performed poorly: mean accuracy was 37.5%, below chance in a binary classification task. At the stimulus level, misclassification was bidirectional: 75% of AI-generated clips were majority-labeled as human, while 62.5% of human-recorded clips were majority-labeled as AI. Signal Detection Theory analysis revealed near-zero discriminability (d' approx 0), indicating inability to reliably distinguish synthetic from human voices rather than simple response bias. Qualitative analysis of 315 coded excerpts revealed reliance on paralinguistic and emotional heuristics, including pauses, filler words, vocal variability, cadence, and emotional expressiveness. However, these surface-level cues traditionally associated with human authenticity were frequently replicated by AI-generated samples. Misclassifications were often accompanied by moderate to high confidence, suggesting perceptual miscalibration rather than uncertainty. Together, our findings demonstrate that authenticity judgments based on vocal heuristics are unreliable in contemporary vishing scenarios. We discuss implications for security interventions, user education, and AI-mediated deception mitigation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [74] [Feedback-based Automated Verification in Vibe Coding of CAS Adaptation Built on Constraint Logic](https://arxiv.org/abs/2602.18607)
*Michal Töpfer,František Plášil,Tomáš Bureš,Petr Hnětynka*

Main category: cs.AI

TL;DR: 论文提出使用vibe coding反馈循环结合新型时序逻辑FCL来生成自适应系统的适配管理器(AM)，通过精确的功能需求约束和迭代测试实现代码生成验证。


<details>
  <summary>Details</summary>
Motivation: 在复杂自适应系统(CAS)适配中，定义系统动态架构和行为变化具有挑战性。传统上通过适配管理器(AM)实现，但生成正确AM代码困难。随着生成式LLMs的发展，基于系统规范和自然语言描述生成AM代码成为可能，但需要解决生成代码的正确性问题。

Method: 采用vibe coding反馈循环方法，结合新型时序逻辑FCL表达功能需求约束。FCL相比经典LTL能更细粒度地描述轨迹行为。通过迭代测试和反馈循环，将FCL约束评估结果作为LLM输入，生成满足约束的AM代码。

Result: 在CAS领域的两个示例系统中，结合适配和vibe coding反馈循环取得了良好效果。通常只需几次反馈循环迭代，每次向LLM提供详细约束违反报告。通过不同初始设置实现了高运行路径覆盖率。

Conclusion: 当基于精确功能需求约束验证时，通过vibe coding反馈循环生成AM是可行方案。FCL逻辑提供了比LTL更细粒度的行为描述能力，结合迭代测试和反馈机制能有效生成正确的适配管理器代码。

Abstract: In CAS adaptation, a challenge is to define the dynamic architecture of the system and changes in its behavior. Implementation-wise, this is projected into an adaptation mechanism, typically realized as an Adaptation Manager (AM). With the advances of generative LLMs, generating AM code based on system specification and desired AM behavior (partially in natural language) is a tempting opportunity. The recent introduction of vibe coding suggests a way to target the problem of the correctness of generated code by iterative testing and vibe coding feedback loops instead of direct code inspection.
  In this paper, we show that generating an AM via vibe coding feedback loops is a viable option when the verification of the generated AM is based on a very precise formulation of the functional requirements. We specify these as constraints in a novel temporal logic FCL that allows us to express the behavior of traces with much finer granularity than classical LTL enables.
  Furthermore, we show that by combining the adaptation and vibe coding feedback loops where the FCL constraints are evaluated for the current system state, we achieved good results in the experiments with generating AMs for two example systems from the CAS domain. Typically, just a few feedback loop iterations were necessary, each feeding the LLM with reports describing detailed violations of the constraints. This AM testing was combined with high run path coverage achieved by different initial settings.

</details>


### [75] [Decoding ML Decision: An Agentic Reasoning Framework for Large-Scale Ranking System](https://arxiv.org/abs/2602.18640)
*Longfei Yun,Yihan Wu,Haoran Liu,Xiaoxuan Liu,Ziyun Xu,Yi Wang,Yang Xia,Pengfei Wang,Mingze Gao,Yunxiang Wang,Changfan Chen,Junfeng Pan*

Main category: cs.AI

TL;DR: GEARS框架将排序优化重构为可编程实验环境中的自主发现过程，通过专业化智能体技能封装专家知识，使操作者能够通过高层意图指导系统，同时确保生产可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代大规模排序系统面临工程上下文约束瓶颈：将模糊的产品意图转化为合理、可执行、可验证假设的艰巨过程，而非仅受建模技术限制。需要解决意图到可执行策略的转化难题。

Method: 提出GEARS框架，将排序优化重构为可编程实验环境中的自主发现过程。使用专业化智能体技能封装排序专家知识为可重用推理能力，通过验证钩子确保统计鲁棒性，过滤过度拟合短期信号的脆弱策略。

Result: 在多样化产品界面上的实验验证表明，GEARS能够持续识别出接近帕累托最优的优越策略，通过将算法信号与深度排序上下文协同结合，同时保持严格的部署稳定性。

Conclusion: GEARS框架通过将排序优化重构为自主发现过程，解决了工程上下文约束瓶颈，使操作者能够通过高层意图指导系统，同时确保生产可靠性，为大规模排序系统提供了有效的优化范式。

Abstract: Modern large-scale ranking systems operate within a sophisticated landscape of competing objectives, operational constraints, and evolving product requirements. Progress in this domain is increasingly bottlenecked by the engineering context constraint: the arduous process of translating ambiguous product intent into reasonable, executable, verifiable hypotheses, rather than by modeling techniques alone. We present GEARS (Generative Engine for Agentic Ranking Systems), a framework that reframes ranking optimization as an autonomous discovery process within a programmable experimentation environment. Rather than treating optimization as static model selection, GEARS leverages Specialized Agent Skills to encapsulate ranking expert knowledge into reusable reasoning capabilities, enabling operators to steer systems via high-level intent vibe personalization. Furthermore, to ensure production reliability, the framework incorporates validation hooks to enforce statistical robustness and filter out brittle policies that overfit short-term signals. Experimental validation across diverse product surfaces demonstrates that GEARS consistently identifies superior, near-Pareto-efficient policies by synergizing algorithmic signals with deep ranking context while maintaining rigorous deployment stability.

</details>


### [76] [Spilled Energy in Large Language Models](https://arxiv.org/abs/2602.18671)
*Adrian Robert Minut,Hazem Dewidar,Iacopo Masi*

Main category: cs.AI

TL;DR: 将LLM的softmax分类器重新解释为能量基模型，通过分析推理过程中的能量溢出检测幻觉，无需训练额外分类器或进行激活消融。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法通常需要训练探针分类器或进行激活消融，计算成本高且缺乏理论依据。需要一种无需训练、基于理论原则的方法来检测LLM生成中的事实错误、偏见和失败。

Method: 将序列到序列的概率链分解为多个相互作用的能量基模型，引入两个完全无需训练的指标：溢出能量（捕捉连续生成步骤中理论上应匹配的能量值差异）和边缘化能量（可在单一步骤测量）。

Result: 在九个基准测试和合成代数操作上评估，涵盖LLaMA、Mistral、Gemma和Qwen3等先进LLM，方法展示了鲁棒的幻觉检测和跨任务泛化能力，结果对预训练和指令调优变体均有效。

Conclusion: 通过将LLM softmax重新解释为能量基模型，开发了无需训练、基于理论原则的幻觉检测方法，能够有效识别事实错误和偏见，为LLM可靠性评估提供了新视角。

Abstract: We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track "energy spills" during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.

</details>


### [77] [Many AI Analysts, One Dataset: Navigating the Agentic Data Science Multiverse](https://arxiv.org/abs/2602.18710)
*Martin Bertran,Riccardo Fogliato,Zhiwei Steven Wu*

Main category: cs.AI

TL;DR: AI分析师基于大语言模型能廉价、大规模地复现结构化分析多样性，在不同模型和提示框架下对相同数据和假设产生冲突结论


<details>
  <summary>Details</summary>
Motivation: 传统"多分析师"研究需要大量协调工作且成本高昂，难以系统研究分析决策对研究结论的影响。本文旨在开发自主AI分析师来廉价、大规模地探索分析多样性问题

Method: 构建基于大语言模型的自主AI分析师系统，让它们在固定数据集上测试预设假设，通过变化底层模型和提示框架进行重复运行。每个AI分析师独立构建并执行完整分析流程，然后由AI审计员筛选方法学上有效的运行

Result: AI分析师产生的分析在效应大小、p值和二元决策（是否支持假设）上显示出广泛分散，经常逆转假设是否被判断为支持。这种分散是结构化的：预处理、模型规范和推理中的可识别分析选择在不同LLM和角色条件下系统性不同。效果是可操控的：重新分配分析师角色或LLM即使在排除方法学缺陷运行后仍会改变结果分布

Conclusion: 自主AI分析师能够廉价、大规模地复现传统多分析师研究中观察到的分析多样性，揭示了分析决策对研究结论的系统性影响，并展示了这种影响可通过模型和角色选择进行操控

Abstract: The conclusions of empirical research depend not only on data but on a sequence of analytic decisions that published results seldom make explicit. Past ``many-analyst" studies have demonstrated this: independent teams testing the same hypothesis on the same dataset regularly reach conflicting conclusions. But such studies require months of coordination among dozens of research groups and are therefore rarely conducted. In this work, we show that fully autonomous AI analysts built on large language models (LLMs) can reproduce a similar structured analytic diversity cheaply and at scale. We task these AI analysts with testing a pre-specified hypothesis on a fixed dataset, varying the underlying model and prompt framing across replicate runs. Each AI analyst independently constructs and executes a full analysis pipeline; an AI auditor then screens each run for methodological validity. Across three datasets spanning experimental and observational designs, AI analyst-produced analyses display wide dispersion in effect sizes, $p$-values, and binary decisions on supporting the hypothesis or not, frequently reversing whether a hypothesis is judged supported. This dispersion is structured: recognizable analytic choices in preprocessing, model specification, and inference differ systematically across LLM and persona conditions. Critically, the effects are \emph{steerable}: reassigning the analyst persona or LLM shifts the distribution of outcomes even after excluding methodologically deficient runs.

</details>


### [78] [Task-Aware Exploration via a Predictive Bisimulation Metric](https://arxiv.org/abs/2602.18724)
*Dayang Liang,Ruihan Liu,Lipeng Wan,Yunlong Liu,Bo An*

Main category: cs.AI

TL;DR: TEB提出了一种任务感知探索方法，通过预测双模拟度量将任务相关表示与探索紧密耦合，解决了视觉强化学习中稀疏奖励下的探索难题。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习在稀疏奖励下的探索面临挑战，主要由于任务无关的视觉变化干扰。现有内在探索方法要么假设能访问低维状态，要么缺乏任务感知的探索策略，在视觉领域表现脆弱。

Method: TEB通过预测双模拟度量学习行为基础的任务表示，并在学习的潜在空间中测量行为内在新颖性。首先理论上缓解了稀疏奖励下退化双模拟度量的表示崩溃问题，通过引入预测奖励差异。基于此鲁棒度量，设计了基于潜在空间的相邻观测相对新颖性的势能探索奖励。

Result: 在MetaWorld和Maze2D上的大量实验表明，TEB实现了优越的探索能力，并超越了最近的基线方法。

Conclusion: TEB通过将任务相关表示与探索紧密耦合，提供了一种有效的任务感知探索方法，显著提升了视觉强化学习在稀疏奖励环境下的性能。

Abstract: Accelerating exploration in visual reinforcement learning under sparse rewards remains challenging due to the substantial task-irrelevant variations. Despite advances in intrinsic exploration, many methods either assume access to low-dimensional states or lack task-aware exploration strategies, thereby rendering them fragile in visual domains. To bridge this gap, we present TEB, a Task-aware Exploration approach that tightly couples task-relevant representations with exploration through a predictive Bisimulation metric. Specifically, TEB leverages the metric not only to learn behaviorally grounded task representations but also to measure behaviorally intrinsic novelty over the learned latent space. To realize this, we first theoretically mitigate the representation collapse of degenerate bisimulation metrics under sparse rewards by internally introducing a simple but effective predicted reward differential. Building on this robust metric, we design potential-based exploration bonuses, which measure the relative novelty of adjacent observations over the latent space. Extensive experiments on MetaWorld and Maze2D show that TEB achieves superior exploration ability and outperforms recent baselines.

</details>


### [79] [Beyond Description: A Multimodal Agent Framework for Insightful Chart Summarization](https://arxiv.org/abs/2602.18731)
*Yuhang Bai,Yujuan Ding,Shanru Lin,Wenqi Fan*

Main category: cs.AI

TL;DR: 提出Chart Insight Agent Flow多智能体框架和ChartSummInsights数据集，显著提升MLLMs在图表总结任务中的性能，生成具有深度洞察的总结


<details>
  <summary>Details</summary>
Motivation: 现有图表总结方法（包括MLLMs）主要关注低层数据描述，未能捕捉数据可视化的根本目的——深层洞察，且缺乏合适的基准数据集

Method: 提出Chart Insight Agent Flow，一个计划-执行多智能体框架，有效利用MLLMs的感知和推理能力从图表图像中挖掘深层洞察；同时构建ChartSummInsights数据集，包含真实世界图表和专家撰写的高质量洞察性总结

Result: 实验结果表明，该方法显著提升了MLLMs在图表总结任务上的性能，生成的总结具有深度和多样化的洞察

Conclusion: Chart Insight Agent Flow框架和ChartSummInsights数据集有效解决了现有图表总结方法缺乏深度洞察的问题，为数据可访问性和信息高效消费提供了更好的解决方案

Abstract: Chart summarization is crucial for enhancing data accessibility and the efficient consumption of information. However, existing methods, including those with Multimodal Large Language Models (MLLMs), primarily focus on low-level data descriptions and often fail to capture the deeper insights which are the fundamental purpose of data visualization. To address this challenge, we propose Chart Insight Agent Flow, a plan-and-execute multi-agent framework effectively leveraging the perceptual and reasoning capabilities of MLLMs to uncover profound insights directly from chart images. Furthermore, to overcome the lack of suitable benchmarks, we introduce ChartSummInsights, a new dataset featuring a diverse collection of real-world charts paired with high-quality, insightful summaries authored by human data analysis experts. Experimental results demonstrate that our method significantly improves the performance of MLLMs on the chart summarization task, producing summaries with deep and diverse insights.

</details>


### [80] [Federated Reasoning Distillation Framework with Model Learnability-Aware Data Allocation](https://arxiv.org/abs/2602.18749)
*Wei Guo,Siyuan Lu,Xiangdong Ran,Yiqi Tong,Yikun Ban,Zelong Xu,Jing Fan,Zixuan Huang,Xiao Zhang,Zhaojun Hu,Fuzhen Zhuang*

Main category: cs.AI

TL;DR: LaDa是一个联邦推理蒸馏框架，通过模型可学习性感知的数据分配解决联邦大语言模型与小语言模型协作中的双向可学习性差距和领域无关推理转移问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦LLM和SLM协作中的数据分配方法未能解决两个关键挑战：1）双向模型可学习性差距，即客户端SLM无法识别符合其可学习性约束的高奖励样本以从LLM有效获取知识，而LLM难以选择能提供超出其现有数据的新知识样本；2）领域无关推理转移，现有推理转移方法无法灵活适应本地领域数据，阻碍SLM从通用LLM有效获取逐步推理能力

Method: 提出LaDa框架，包含两个核心组件：1）模型可学习性感知数据过滤器，基于每个SLM-LLM对的可学习性差距自适应分配高奖励样本，促进双向知识转移；2）领域自适应推理蒸馏方法，通过对齐SLM和LLM在过滤高奖励样本上的推理路径联合概率，通过对比蒸馏学习使SLM在本地数据分布下捕获底层推理模式。LaDa可作为现有协作框架的插件模块运行

Result: 未在摘要中明确说明具体实验结果，但框架设计旨在有效解决双向可学习性差距和领域无关推理转移问题，提升联邦LLM-SLM协作中的知识转移效率

Conclusion: LaDa通过模型可学习性感知的数据分配和领域自适应推理蒸馏，解决了联邦LLM-SLM协作中的关键挑战，能够根据模型可学习性差距自适应调整知识转移，提升协作效果

Abstract: Data allocation plays a critical role in federated large language model (LLM) and small language models (SLMs) reasoning collaboration. Nevertheless, existing data allocation methods fail to address an under-explored challenge in collaboration: bidirectional model learnability gap, where client-side SLMs cannot identify high-reward samples matching their learnability constraints for effective knowledge transfer from LLMs, while LLMs struggle to select samples contributing novel knowledge beyond their existing data. Furthermore, these collaboration frameworks face another key challenge: domain-agnostic reasoning transfer, where existing reasoning transfer methods fail to flexibly adapt to the local domain data, preventing SLMs from effectively acquiring step-by-step reasoning abilities within from general LLM. To address these challenges, we propose LaDa, a federated reasoning distillation framework with model learnability-aware data allocation. It introduces a model learnability-aware data filter that adaptively allocates high-reward samples based on the learnability gap between each SLM and LLM pair, effectively facilitating bidirectional knowledge transfer. We further design a domain adaptive reasoning distillation method that aligns joint probabilities of reasoning paths on filtered high-reward samples through contrastive distillation learning between SLM and LLM, enabling SLM to capture underlying reasoning patterns under local data distribution. LaDa operates as a plug-in module for existing collaboration frameworks, adapting knowledge transfer based on model learnability gaps.

</details>


### [81] [The Convergence of Schema-Guided Dialogue Systems and the Model Context Protocol](https://arxiv.org/abs/2602.18764)
*Andreas Schlapbach*

Main category: cs.AI

TL;DR: SGD与MCP代表LLM-agent交互的统一范式，通过模式设计实现确定性、可审计的交互。论文提取了五个模式设计原则，揭示了三个新见解，为AI系统监督提供了可扩展机制。


<details>
  <summary>Details</summary>
Motivation: 论文旨在揭示Schema-Guided Dialogue (SGD)和Model Context Protocol (MCP)之间的根本性趋同，两者都代表了LLM-agent交互的统一范式。通过分析这一趋同，提取模式设计的基本原则，以解决现有框架在失败模式、工具间关系和渐进披露方面的不足，为AI系统监督提供可扩展的机制。

Method: 通过对比分析SGD（2019年设计用于对话式API发现）和MCP（当前LLM工具集成的实际标准）的架构和设计理念，识别两者共享的核心洞察：模式不仅能编码工具签名，还能编码操作约束和推理指导。基于此分析提取五个基础设计原则，并提供具体的设计模式实现。

Result: 提取了五个模式设计基本原则：(1)语义完整性优于句法精度，(2)明确动作边界，(3)失败模式文档化，(4)渐进披露兼容性，(5)工具间关系声明。揭示了三个新见解：SGD原始设计本质上是合理的应被MCP继承；两个框架都未充分利用失败模式和工具间关系；渐进披露在真实世界令牌约束下成为关键的生产扩展洞察。

Conclusion: 模式驱动的治理是AI系统监督的可扩展机制，无需专有系统检查，这对Software 3.0至关重要。论文提供的设计原则和模式为构建确定性、可审计的LLM-agent交互系统提供了理论基础和实践指导。

Abstract: This paper establishes a fundamental convergence: Schema-Guided Dialogue (SGD) and the Model Context Protocol (MCP) represent two manifestations of a unified paradigm for deterministic, auditable LLM-agent interaction. SGD, designed for dialogue-based API discovery (2019), and MCP, now the de facto standard for LLM-tool integration, share the same core insight -- that schemas can encode not just tool signatures but operational constraints and reasoning guidance. By analyzing this convergence, we extract five foundational principles for schema design: (1) Semantic Completeness over Syntactic Precision, (2) Explicit Action Boundaries, (3) Failure Mode Documentation, (4) Progressive Disclosure Compatibility, and (5) Inter-Tool Relationship Declaration. These principles reveal three novel insights: first, SGD's original design was fundamentally sound and should be inherited by MCP; second, both frameworks leave failure modes and inter-tool relationships unexploited -- gaps we identify and resolve; third, progressive disclosure emerges as a critical production-scaling insight under real-world token constraints. We provide concrete design patterns for each principle. These principles position schema-driven governance as a scalable mechanism for AI system oversight without requiring proprietary system inspection -- central to Software 3.0.

</details>


### [82] [GenPlanner: From Noise to Plans -- Emergent Reasoning in Flow Matching and Diffusion Models](https://arxiv.org/abs/2602.18812)
*Agnieszka Polowczyk,Alicja Polowczyk,Michał Wieczorek*

Main category: cs.AI

TL;DR: 提出GenPlanner方法，基于扩散模型和流匹配，用于迷宫路径规划，通过多通道条件输入迭代生成正确路径，性能显著优于基线CNN模型。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中的路径规划是人工智能的关键问题，需要同时理解空间几何和全局结构。探索生成模型作为规划和推理机制的潜力。

Method: 提出GenPlanner方法，基于扩散模型和流匹配，包含DiffPlanner和FlowPlanner两个变体。使用多通道条件输入（障碍物地图、起点和终点信息）来条件化轨迹生成。模型从随机噪声开始，迭代地将其转化为正确路径。

Result: 实验表明，所提方法显著优于基线CNN模型。特别是FlowPlanner在有限生成步数下仍表现出高性能。

Conclusion: 生成模型（特别是扩散模型和流匹配）可作为有效的路径规划和推理机制，在复杂环境路径规划任务中展现出优越性能。

Abstract: Path planning in complex environments is one of the key problems of artificial intelligence because it requires simultaneous understanding of the geometry of space and the global structure of the problem. In this paper, we explore the potential of using generative models as planning and reasoning mechanisms. We propose GenPlanner, an approach based on diffusion models and flow matching, along with two variants: DiffPlanner and FlowPlanner. We demonstrate the application of generative models to find and generate correct paths in mazes. A multi-channel condition describing the structure of the environment, including an obstacle map and information about the starting and destination points, is used to condition trajectory generation. Unlike standard methods, our models generate trajectories iteratively, starting with random noise and gradually transforming it into a correct solution. Experiments conducted show that the proposed approach significantly outperforms the baseline CNN model. In particular, FlowPlanner demonstrates high performance even with a limited number of generation steps.

</details>


### [83] [TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.18884)
*Zhenkun Gao,Xuhong Wang,Xin Tan,Yuan Xie*

Main category: cs.AI

TL;DR: TPRU是一个用于增强小型多模态大语言模型时序推理能力的大规模数据集，通过强化学习微调显著提升模型在时序任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前可部署的小型多模态大语言模型在理解和处理时序性、过程性视觉数据方面存在严重不足，这限制了它们在具身AI等现实世界应用中的表现。这一缺陷主要源于训练范式缺乏大规模、过程连贯的数据。

Method: 提出了TPRU数据集，该数据集从机器人操作和GUI导航等多样具身场景中收集，包含三个互补任务：时序重排序、下一帧预测和前一帧回顾。关键特点是包含具有挑战性的负样本，促使模型从被动观察转向主动的跨模态验证。采用强化学习微调方法，专门针对资源高效模型进行优化。

Result: 在手动构建的TPRU-Test测试集上，TPRU-7B模型的准确率从50.33%大幅提升至75.70%，达到了最先进水平，显著优于包括GPT-4o在内的更大规模基线模型。这些能力能够有效泛化，在已有基准测试上也表现出显著改进。

Conclusion: TPRU数据集和强化学习微调方法有效解决了小型多模态大语言模型在时序推理方面的关键瓶颈，为具身AI等实际应用提供了重要支持，代码已开源。

Abstract: Multimodal Large Language Models (MLLMs), particularly smaller, deployable variants, exhibit a critical deficiency in understanding temporal and procedural visual data, a bottleneck hindering their application in real-world embodied AI. This gap is largely caused by a systemic failure in training paradigms, which lack large-scale, procedurally coherent data. To address this problem, we introduce TPRU, a large-scale dataset sourced from diverse embodied scenarios such as robotic manipulation and GUI navigation. TPRU is systematically designed to cultivate temporal reasoning through three complementary tasks: Temporal Reordering, Next-Frame Prediction, and Previous-Frame Review. A key feature is the inclusion of challenging negative samples, compelling models to transition from passive observation to active, cross-modal validation. We leverage TPRU with a reinforcement learning (RL) fine-tuning methodology, specifically targeting the enhancement of resource-efficient models. Experiments show our approach yields dramatic gains: on our manually curated TPRU-Test, the accuracy of TPRU-7B soars from 50.33\% to 75.70\%, a state-of-the-art result that significantly outperforms vastly larger baselines, including GPT-4o. Crucially, these capabilities generalize effectively, demonstrating substantial improvements on established benchmarks. The codebase is available at https://github.com/Stephen-gzk/TPRU/ .

</details>


### [84] [Early Evidence of Vibe-Proving with Consumer LLMs: A Case Study on Spectral Region Characterization with ChatGPT-5.2 (Thinking)](https://arxiv.org/abs/2602.18918)
*Brecht Verbeken,Brando Vagenende,Marie-Anne Guerry,Andres Algaba,Vincent Ginis*

Main category: cs.AI

TL;DR: 论文展示了使用消费级订阅LLM（ChatGPT-5.2）作为数学研究协作者的案例研究，通过迭代生成-评审-修复流程解决了关于4-循环行随机非负矩阵谱区域的猜想，证明了LLM在高层证明搜索中有用，但人类专家在正确性验证中仍不可或缺。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型越来越多地用作科学协作者，但关于它们在研究级数学中作用的证据仍然有限，特别是对于个体研究者可访问的工作流程。本研究旨在通过可审计的案例研究，提供LLM在数学研究中实际应用的早期证据。

Method: 采用"氛围证明"方法，使用ChatGPT-5.2（Thinking版本）作为研究工具，通过分析7个可共享的对话线程和4个版本化的证明草稿，记录了一个迭代的生成-评审-修复管道。该方法强调LLM在高层证明搜索中的作用，同时保持人类专家在正确性关键验证中的核心地位。

Result: 成功解决了Ran和Teng（2024）猜想20，给出了4-循环行随机非负矩阵族精确非实谱区域的充要条件以及明确的边界实现构造。最终定理提供了完整的数学结果。

Conclusion: LLM在高层证明搜索中最有用，而人类专家对于正确性关键的闭合仍然必不可少。研究不仅贡献了数学结果，还提供了过程层面的特征描述，指出了LLM辅助在哪些方面有实质性帮助以及验证瓶颈在哪里持续存在，这对评估AI辅助研究工作流程和设计人在回路定理证明系统具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly used as scientific copilots, but evidence on their role in research-level mathematics remains limited, especially for workflows accessible to individual researchers. We present early evidence for vibe-proving with a consumer subscription LLM through an auditable case study that resolves Conjecture 20 of Ran and Teng (2024) on the exact nonreal spectral region of a 4-cycle row-stochastic nonnegative matrix family. We analyze seven shareable ChatGPT-5.2 (Thinking) threads and four versioned proof drafts, documenting an iterative pipeline of generate, referee, and repair. The model is most useful for high-level proof search, while human experts remain essential for correctness-critical closure. The final theorem provides necessary and sufficient region conditions and explicit boundary attainment constructions. Beyond the mathematical result, we contribute a process-level characterization of where LLM assistance materially helps and where verification bottlenecks persist, with implications for evaluation of AI-assisted research workflows and for designing human-in-the-loop theorem proving systems.

</details>


### [85] [High Dimensional Procedural Content Generation](https://arxiv.org/abs/2602.18943)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出了高维程序化内容生成（HDPCG）框架，将非几何游戏玩法维度提升为联合状态空间的一等坐标，通过方向-空间和方向-时间两个具体方向实现，并提供了通用算法和验证。


<details>
  <summary>Details</summary>
Motivation: 现有PCG方法主要关注静态2D/3D几何形状生成，将游戏机制作为辅助元素处理，这限制了可控性和表达能力。需要将非几何游戏玩法维度提升为一级坐标，实现更全面的内容生成。

Method: 提出HDPCG框架，包含两个具体方向：1) 方向-空间：通过离散层维度增强几何，在4D空间(x,y,z,l)中验证可达性，处理2.5D/3.5D机制；2) 方向-时间：通过时间扩展图增强几何，捕捉动作语义和冲突规则。为每个方向提供三个通用算法，共享抽象骨架生成、受控接地、高维验证和多度量评估的流水线。

Result: 大规模实验验证了问题表述的完整性和方法的有效性，在可玩性、结构、风格、鲁棒性和效率方面表现良好。Unity案例研究创建了符合度量的可玩场景，展示了方法的实用性。

Conclusion: HDPCG框架鼓励PCG向通用表示转变，支持生成超越几何的游戏玩法相关维度，为实现可控、可验证和可扩展的关卡生成铺平道路。

Abstract: Procedural content generation (PCG) has made substantial progress in shaping static 2D/3D geometry, while most methods treat gameplay mechanics as auxiliary and optimize only over space. We argue that this limits controllability and expressivity, and formally introduce High-Dimensional PCG (HDPCG): a framework that elevates non-geometric gameplay dimensions to first-class coordinates of a joint state space. We instantiate HDPCG along two concrete directions. Direction-Space augments geometry with a discrete layer dimension and validates reachability in 4D (x,y,z,l), enabling unified treatment of 2.5D/3.5D mechanics such as gravity inversion and parallel-world switching. Direction-Time augments geometry with temporal dynamics via time-expanded graphs, capturing action semantics and conflict rules. For each direction, we present three general, practicable algorithms with a shared pipeline of abstract skeleton generation, controlled grounding, high-dimensional validation, and multi-metric evaluation. Large-scale experiments across diverse settings validate the integrity of our problem formulation and the effectiveness of our methods on playability, structure, style, robustness, and efficiency. Beyond quantitative results, Unity-based case studies recreate playable scenarios that accord with our metrics. We hope HDPCG encourages a shift in PCG toward general representations and the generation of gameplay-relevant dimensions beyond geometry, paving the way for controllable, verifiable, and extensible level generation.

</details>


### [86] [INDUCTION: Finite-Structure Concept Synthesis in First-Order Logic](https://arxiv.org/abs/2602.18956)
*Serafim Batzoglou*

Main category: cs.AI

TL;DR: INDUCTION是一个用于一阶逻辑中有限结构概念合成的基准测试，要求模型基于有限关系世界输出解释目标谓词的逻辑公式，并通过精确模型检查验证正确性。


<details>
  <summary>Details</summary>
Motivation: 开发一个系统性的基准来评估模型在一阶逻辑中进行概念合成的能力，特别是在有限关系世界中从扩展标记的目标谓词推导出统一解释公式的能力。

Method: 创建包含三种机制（FullObs、CI对比性、EC存在性补全）的基准，要求模型输出单一的一阶逻辑公式，通过精确模型检查验证正确性，并对公式膨胀进行惩罚。

Result: 发现明显的难度梯度、持续存在的困难结构家族，观察到低膨胀公式在保留世界上的泛化能力显著更好，不同精英模型在任务和性能指标上表现出质的不同行为。

Conclusion: INDUCTION基准揭示了概念合成的难度模式，表明公式简洁性与泛化能力相关，不同模型采用不同的概念泛化策略，为评估逻辑推理能力提供了有价值的工具。

Abstract: We introduce INDUCTION, a benchmark for finite structure concept synthesis in first order logic. Given small finite relational worlds with extensionally labeled target predicates, models must output a single first order logical formula that explains the target uniformly across worlds, with correctness verified via exact model checking. The benchmark includes three regimes, FullObs, CI (contrastive), and EC (existential completion), nd penalizes formula bloat. We find sharp difficulty gradients, persistent hard structural families, and observe that low bloat formulas generalize far better on held out worlds. Elite recent models show qualitatively different behaviors across tasks and performance metrics, hinting to their different strategies of concept generalization.

</details>


### [87] [Modularity is the Bedrock of Natural and Artificial Intelligence](https://arxiv.org/abs/2602.18960)
*Alessandro Salatiello*

Main category: cs.AI

TL;DR: 本文综述了模块化在人工智能和神经科学中的核心作用，指出模块化是实现高效学习和强泛化能力的关键原则，并探讨了如何通过模块化架构弥合自然智能与人工智能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统依赖远超人类智能所需的数据、计算和能源资源，这种差距表明需要新的指导原则。大脑计算的基本组织原则——特别是模块化——为人类高效学习和强泛化能力提供了关键支持，但在主流AI研究中仍未得到足够重视。

Method: 通过概念框架回顾人工智能和神经科学中的多个研究线索，分析模块化提供的计算优势，探讨模块化在不同AI研究领域如何作为解决方案出现，研究大脑利用的模块化原则，以及模块化如何帮助弥合自然与人工智能之间的差距。

Result: 模块化在支持人工和自然智能中发挥核心作用：1）提供计算优势；2）作为多个AI研究领域的解决方案；3）大脑利用特定的模块化原则；4）有助于弥合自然与人工智能之间的差距。

Conclusion: 模块化是实现高效智能系统的关键组织原则，应成为AI研究的重要方向。通过借鉴大脑的模块化架构，可以开发出更高效、更具泛化能力的人工智能系统，减少对大规模资源的依赖。

Abstract: The remarkable performance of modern AI systems has been driven by unprecedented scales of data, computation, and energy -- far exceeding the resources required by human intelligence. This disparity highlights the need for new guiding principles and motivates drawing inspiration from the fundamental organizational principles of brain computation. Among these principles, modularity has been shown to be critical for supporting the efficient learning and strong generalization abilities consistently exhibited by humans. Furthermore, modularity aligns well with the No Free Lunch Theorem, which highlights the need for problem-specific inductive biases and motivates architectures composed of specialized components that solve subproblems. However, despite its fundamental role in natural intelligence and its demonstrated benefits across a range of seemingly disparate AI subfields, modularity remains relatively underappreciated in mainstream AI research. In this work, we review several research threads in artificial intelligence and neuroscience through a conceptual framework that highlights the central role of modularity in supporting both artificial and natural intelligence. In particular, we examine what computational advantages modularity provides, how it has emerged as a solution across several AI research areas, which modularity principles the brain exploits, and how modularity can help bridge the gap between natural and artificial intelligence.

</details>


### [88] [Robust and Efficient Tool Orchestration via Layered Execution Structures with Reflective Correction](https://arxiv.org/abs/2602.18968)
*Tao Zhe,Haoyu Wang,Bo Luo,Min Wu,Wei Fan,Xiao Luo,Zijun Yao,Haifeng Chen,Dongjie Wang*

Main category: cs.AI

TL;DR: 提出了一种基于分层执行结构的工具编排方法，通过粗粒度层结构提供全局指导，结合模式感知的局部错误修复机制，实现轻量级、可重用的工具编排组件。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用方法将工具执行与逐步语言推理或显式规划紧密耦合，导致脆弱行为和较高的执行开销。需要从工具编排的角度重新审视工具调用，以克服这些限制。

Method: 将工具编排建模为学习分层执行结构，捕获高层工具依赖关系，通过上下文约束诱导分层执行。引入模式感知的反射修正机制，在本地检测和修复执行时错误，避免重新规划整个执行轨迹。

Result: 实验结果表明，该方法实现了鲁棒的工具执行，同时降低了执行复杂性和开销。代码将公开提供。

Conclusion: 结构化执行范式为智能体系统提供了轻量级、可重用的编排组件，有效解决了工具调用中的编排问题，通过粗粒度层结构和局部错误修复实现了鲁棒且高效的工具执行。

Abstract: Tool invocation is a core capability of agentic systems, yet failures often arise not from individual tool calls but from how multiple tools are organized and executed together. Existing approaches tightly couple tool execution with stepwise language reasoning or explicit planning, leading to brittle behavior and high execution overhead. To overcome these limitations, we revisit tool invocation from the perspective of tool orchestration. Our key insight is that effective orchestration does not require precise dependency graphs or fine-grained planning. Instead, a coarse-grained layer structure suffices to provide global guidance, while execution-time errors can be corrected locally. Specifically, we model tool orchestration as learning a layered execution structure that captures high-level tool dependencies, inducing layer-wise execution through context constraints. To handle execution-time failures, we introduce a schema-aware reflective correction mechanism that detects and repairs errors locally. This design confines errors to individual tool calls and avoids re-planning entire execution trajectories. This structured execution paradigm enables a lightweight and reusable orchestration component for agentic systems. Experimental results show that our approach achieves robust tool execution while reducing execution complexity and overhead. Code will be made publicly available.

</details>


### [89] [InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing](https://arxiv.org/abs/2602.18985)
*Kun Ding,Jian Xu,Ying Wang,Peipei Yang,Shiming Xiang*

Main category: cs.AI

TL;DR: InfEngine是一个自主智能计算引擎，通过自验证和自优化机制实现红外辐射计算从人工编排到协作自动化的范式转变，在200个红外任务上达到92.7%通过率，工作流速度比人工专家快21倍。


<details>
  <summary>Details</summary>
Motivation: 红外辐射计算在气候科学、遥感和光谱学中至关重要，但当前受限于人工工作流程，需要从人工编排转向协作自动化以提高效率和可靠性。

Method: 集成四个专用智能体，通过两大核心创新：1）自验证机制，通过联合求解器-评估器调试提高功能正确性和科学合理性；2）自优化机制，通过具有自发现适应度函数的进化算法实现自主性能优化。系统基于InfTools的270个精选工具构建。

Result: 在InfBench基准测试的200个红外特定任务中，InfEngine达到92.7%的通过率，生成的工作流程比人工专家工作快21倍，并能生成可重用、已验证和优化的代码。

Conclusion: InfEngine展示了研究人员如何从手动编码转变为与自验证、自优化的计算伙伴协作，将计算工作流程转化为持久的科学资产，加速科学发现周期。

Abstract: Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine

</details>


### [90] [MagicAgent: Towards Generalized Agent Planning](https://arxiv.org/abs/2602.19000)
*Xuhui Ren,Shaokang Dong,Chen Yang,Qing Gao,Yunbin Zhao,Yongsheng Liu,Xinwei Geng,Xiang Li,Demei Yan,Yanqing Li,Chenhao Huang,Dingwei Zhu,Junjie Ye,Boxuan Yue,Yingnan Fu,Mengzhe Lv,Zezeng Feng,Boshen Zhou,Bocheng Wang,Xuanjing Huang,Yu-Gang Jiang,Tao Gui,Qi Zhang,Yunke Zhang*

Main category: cs.AI

TL;DR: MagicAgent是一个专门为通用智能体规划设计的系列基础模型，通过合成数据框架和两阶段训练范式，在多个规划基准测试中超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型从被动文本处理器向自主智能体演进，规划成为现代智能的核心组成部分。然而，实现通用规划仍然困难，主要面临两个挑战：1）高质量交互数据的稀缺；2）异构规划任务之间的内在冲突。这些挑战导致模型在孤立任务上表现出色但泛化能力差，而现有的多任务训练尝试又受到梯度干扰的影响。

Method: 提出了MagicAgent系列基础模型，采用轻量级可扩展的合成数据框架生成多样化规划任务的高质量轨迹，包括分层任务分解、工具增强规划、多约束调度、程序逻辑编排和长时程工具执行。为缓解训练冲突，提出了两阶段训练范式：先进行监督微调，然后在静态数据集和动态环境上进行多目标强化学习。

Result: MagicAgent-32B和MagicAgent-30B-A3B在多个基准测试中表现出色：Worfbench准确率75.1%、NaturalPlan 55.9%、τ²-Bench 57.5%、BFCL-v3 86.9%、ACEBench 81.2%，在内部MagicEval基准测试中也取得强劲结果。这些结果显著优于现有的100B以下参数模型，甚至超越了领先的闭源模型。

Conclusion: MagicAgent通过创新的合成数据生成和两阶段训练方法，成功解决了通用规划中的数据和任务冲突问题，为构建更强大的自主智能体提供了有效的解决方案，在多个规划基准测试中取得了最先进的性能。

Abstract: The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\%$ on Worfbench, $55.9\%$ on NaturalPlan, $57.5\%$ on $τ^2$-Bench, $86.9\%$ on BFCL-v3, and $81.2\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.

</details>


### [91] [Evaluating Large Language Models on Quantum Mechanics: A Comparative Study Across Diverse Models and Tasks](https://arxiv.org/abs/2602.19006)
*S. K. Rithvik*

Main category: cs.AI

TL;DR: 该研究系统评估了15个大语言模型在量子力学问题解决上的表现，揭示了模型能力分层、任务难度差异、工具增强效果和可重复性特征。


<details>
  <summary>Details</summary>
Motivation: 量子力学作为复杂科学领域，需要评估大语言模型在此类专业问题上的解决能力，为模型选择和实际应用提供依据。

Method: 评估15个来自5个提供商的大语言模型，覆盖三个能力层级，在20个量子力学任务上进行测试，包括900个基线评估和75个工具增强评估，使用自动验证系统。

Result: 结果显示明显的层级分化：旗舰模型平均准确率81%，优于中端模型（77%）和快速模型（67%）。推导任务表现最佳（92%平均，旗舰模型100%），数值计算最困难（42%）。工具增强效果差异大，整体提升4.4%但代价是3倍token成本。可重复性分析显示平均方差6.3%，旗舰模型稳定性高。

Conclusion: 该研究建立了量子力学基准测试，量化了模型层级性能差异，分析了工具增强的权衡，并表征了可重复性，为量子力学领域的大语言模型应用提供了系统评估框架。

Abstract: We present a systematic evaluation of large language models on quantum mechanics problem-solving. Our study evaluates 15 models from five providers (OpenAI, Anthropic, Google, Alibaba, DeepSeek) spanning three capability tiers on 20 tasks covering derivations, creative problems, non-standard concepts, and numerical computation, comprising 900 baseline and 75 tool-augmented assessments. Results reveal clear tier stratification: flagship models achieve 81\% average accuracy, outperforming mid-tier (77\%) and fast models (67\%) by 4pp and 14pp respectively. Task difficulty patterns emerge distinctly: derivations show highest performance (92\% average, 100\% for flagship models), while numerical computation remains most challenging (42\%). Tool augmentation on numerical tasks yields task-dependent effects: modest overall improvement (+4.4pp) at 3x token cost masks dramatic heterogeneity ranging from +29pp gains to -16pp degradation. Reproducibility analysis across three runs quantifies 6.3pp average variance, with flagship models demonstrating exceptional stability (GPT-5 achieves zero variance) while specialized models require multi-run evaluation. This work contributes: (i) a benchmark for quantum mechanics with automatic verification, (ii) systematic evaluation quantifying tier-based performance hierarchies, (iii) empirical analysis of tool augmentation trade-offs, and (iv) reproducibility characterization. All tasks, verifiers, and results are publicly released.

</details>


### [92] [Asking the Right Questions: Improving Reasoning with Generated Stepping Stones](https://arxiv.org/abs/2602.19069)
*Hengyuan Hu,Tingchen Fu,Minqi Jiang,Alexander H Miller,Yoram Bachrach,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: ARQ框架通过添加问题生成器来创建中间步骤（垫脚石），帮助LLMs解决复杂推理任务，这些步骤可迁移且能通过微调优化生成


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被应用于更复杂的任务，需要关注它们构建中间步骤（垫脚石）的能力，这些步骤能帮助模型更好地解决单次无法完成的任务

Method: 提出ARQ框架，在标准推理流程中引入问题生成器来创建垫脚石（简化、重构、子问题等），通过SFT和RL在合成数据上微调LLMs以生成更有用的垫脚石

Result: 好的垫脚石问题确实存在且具有可迁移性，能显著帮助不同能力的LLMs解决目标任务；通过微调可以生成更有用的垫脚石

Conclusion: 垫脚石在LLM推理中具有重要价值，ARQ框架通过生成可迁移的中间问题有效提升LLMs解决复杂任务的能力，且可通过训练进一步优化

Abstract: Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\textbf{A}king the \textbf{R}ight \textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data.

</details>


### [93] [Defining Explainable AI for Requirements Analysis](https://arxiv.org/abs/2602.19071)
*Raymond Sheh,Isaac Monteath*

Main category: cs.AI

TL;DR: 本文提出了一个三维框架（来源、深度、范围）来分类不同应用对可解释人工智能的解释需求，并探讨如何将这些需求与机器学习技术的解释能力相匹配。


<details>
  <summary>Details</summary>
Motivation: 随着XAI的兴起，AI/ML社区认识到，要使AI系统值得信任，不仅需要良好的决策性能，还需要能够解释其决策过程。然而，不同应用对解释信息的需求各不相同，目前缺乏系统化的方法来定义这些需求并将其与现有ML技术的解释能力相匹配。

Method: 提出了一个三维分类框架：1) 来源（Source）：解释信息的来源（如模型内部、外部知识等）；2) 深度（Depth）：解释的详细程度（从简单到复杂）；3) 范围（Scope）：解释覆盖的决策过程范围（局部或全局）。该框架用于分析不同应用场景的解释需求，并将其与ML技术的解释能力进行匹配。

Result: 建立了一个系统化的三维分类框架，能够帮助研究人员和实践者更清晰地理解和定义不同应用对XAI的解释需求，并为选择合适的ML技术提供指导。该框架避免了现有文献中已充分讨论的解释方面，专注于ML领域，但其原则可推广到更广泛的AI领域。

Conclusion: 通过提出的三维框架（来源、深度、范围），为XAI领域提供了一个实用的工具，帮助系统化地分析不同应用场景的解释需求，并促进解释需求与ML技术能力之间的更好匹配，从而推动可信AI系统的发展。

Abstract: Explainable Artificial Intelligence (XAI) has become popular in the last few years. The Artificial Intelligence (AI) community in general, and the Machine Learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements?
  In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.

</details>


### [94] [Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions](https://arxiv.org/abs/2602.19109)
*Yao Yan*

Main category: cs.AI

TL;DR: 研究Meta-Llama-3-8B在单token读出下的三位数加法机制，发现跨token路由因果失效后存在层17附近的边界，后期注意力可被移除，数字方向字典在共享低秩子空间内通过近似正交映射关联。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在算术推理中的内部工作机制，特别是当跨token路由变得因果无关后，模型如何最终确定算术答案。旨在理解模型在后期处理阶段的信息流和表示结构。

Method: 使用因果残差修补和累积注意力消融技术，定位模型内部处理边界。分析数字方向字典在不同上下文中的变化，应用低秩Procrustes对齐方法研究表示关系。通过因果数字编辑实验验证几何结构。

Result: 在层17附近发现清晰边界：超过该层后，解码的和几乎完全由最后一个输入token控制，后期自注意力基本可被移除。数字方向字典随更高位数上下文变化，但在共享低秩子空间内通过近似正交映射良好关联。因果编辑实验验证了这种几何结构。

Conclusion: 三位数加法在Meta-Llama-3-8B中呈现两阶段处理：前期跨token路由阶段和后期单token控制阶段。后期表示在低秩子空间内具有结构化几何关系，可通过学习到的映射在不同上下文间转换数字方向，这为理解模型算术推理机制提供了新视角。

Abstract: We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how
  arithmetic answers are finalized after cross-token routing becomes causally irrelevant.
  Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:
  beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention
  is largely dispensable.
  In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are
  well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).
  Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the
  learned map restores strict counterfactual edits; negative controls do not recover.

</details>


### [95] [K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model](https://arxiv.org/abs/2602.19128)
*Shiyi Cao,Ziming Mao,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: K-Search：基于协同进化世界模型的GPU内核优化框架，通过解耦高层算法规划与底层程序实例化，显著超越现有进化搜索方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的GPU内核自动化化方法通常将大语言模型仅视为启发式引导进化循环中的随机代码生成器，缺乏显式规划能力，难以处理需要协调多步结构变换的复杂内核，且常因低效或不正确的中间实现而丢弃有前景的策略

Method: 提出Search via Co-Evolving World Model方法，构建K-Search框架：用协同进化世界模型替代静态搜索启发式，利用LLM的领域先验知识引导搜索；显式解耦高层算法规划与底层程序实例化，使系统能够导航非单调优化路径并对临时实现缺陷保持鲁棒性

Result: 在FlashInfer的GQA、MLA和MoE等复杂内核上，K-Search显著优于最先进的进化搜索方法，平均提升2.10倍，在复杂MoE内核上最高提升14.3倍；在GPUMode TriMul任务上，K-Search在H100上达到1030us，超越了先前进化和人工设计的解决方案

Conclusion: K-Search通过协同进化世界模型框架，有效解决了复杂GPU内核优化中规划与实现的解耦问题，实现了对现有方法的显著性能超越，为现代机器学习系统的高效优化提供了新范式

Abstract: Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.

</details>


### [96] [Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians](https://arxiv.org/abs/2602.19141)
*Kartik Chandra,Max Kleiman-Weiner,Jonathan Ragan-Kelley,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 该研究通过贝叶斯建模揭示了AI谄媚性（sycophancy）与AI诱发精神病（delusional spiraling）之间的因果关系，证明即使理想化的贝叶斯理性用户也容易陷入妄想螺旋，且现有缓解措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究AI谄媚性（即AI倾向于验证用户主张的偏见）与"AI精神病"或"妄想螺旋"现象之间的因果关系。这种现象表现为用户在长时间与AI聊天机器人对话后，对荒谬信念产生危险自信。

Method: 提出一个简单的贝叶斯模型来模拟用户与聊天机器人的对话过程，在该模型中形式化定义了谄媚性和妄想螺旋的概念。通过建模和仿真来验证因果关系，并测试两种缓解措施的效果：防止聊天机器人产生虚假主张（幻觉），以及告知用户模型可能存在谄媚性。

Result: 研究结果显示：1）即使理想化的贝叶斯理性用户也容易陷入妄想螺旋；2）谄媚性在其中起因果作用；3）两种候选缓解措施（防止AI幻觉和告知用户谄媚性可能性）都无法完全消除这种效应。

Conclusion: AI谄媚性与妄想螺旋之间存在因果关系，且现有缓解措施效果有限。这一发现对模型开发者和政策制定者具有重要意义，需要更有效的策略来减轻妄想螺旋问题。

Abstract: "AI psychosis" or "delusional spiraling" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called "sycophancy." In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.

</details>


### [97] [DoAtlas-1: A Causal Compilation Paradigm for Clinical AI](https://arxiv.org/abs/2602.19158)
*Yulong Li,Jianxu Chen,Xiwei Liu,Chuanyue Suo,Rong Xia,Zhixiang Lu,Yichen Li,Xinlin Zhuang,Niranjana Arun Menon,Yutong Xie,Eran Segal,Imran Razzak*

Main category: cs.AI

TL;DR: 将医学证据从叙述性文本转化为可执行代码的因果编译范式，通过DoAtlas-1系统实现了1445个效应核的标准化和可执行查询


<details>
  <summary>Details</summary>
Motivation: 现有医学基础模型只能生成叙述性解释，无法量化干预效果、检测证据冲突或验证文献主张，限制了临床可审计性

Method: 提出因果编译范式，将医学证据标准化为结构化估计对象，支持六种可执行因果查询；通过DoAtlas-1系统实现，包括效应标准化、冲突感知图构建和真实世界验证

Result: DoAtlas-1从754项研究中编译了1445个效应核，在人类表型项目（10000名参与者）中验证，达到98.5%的规范化准确率和80.5%的查询可执行性

Conclusion: 因果编译范式将医学AI从文本生成转向可执行、可审计和可验证的因果推理，提升了医学证据的标准化和可操作性

Abstract: Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning.

</details>


### [98] [Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM](https://arxiv.org/abs/2602.19159)
*Francesca Bianco,Derek Shiller*

Main category: cs.AI

TL;DR: 该研究通过机制可解释性方法探究了LLM中效价信息（痛苦vs快乐）的内部表征和因果作用，发现效价符号在早期层即可线性分离，强度信息在中后期层解码最佳，晚期注意力输出对决策有最大因果影响。


<details>
  <summary>Details</summary>
Motivation: 先前行为研究表明LLMs在选项被框定为痛苦或快乐时会改变选择，且这种偏差会随陈述强度而变化。本研究旨在将行为证据（模型做什么）与机制可解释性（支持这些行为的计算）联系起来，探究效价相关信息在Transformer内部的表征位置和因果作用。

Method: 使用Gemma-2-9B-it模型和基于先前工作的简约决策任务，采用三种方法：(1) 通过跨流层的线性探测映射表征可用性；(2) 通过激活干预（引导；修补/消融）测试因果贡献；(3) 在epsilon网格上量化剂量-响应效应，读取2-3对数边际和数字对归一化选择概率。

Result: 发现：(a) 效价符号（痛苦vs快乐）从非常早期层（L0-L1）即可在流族间完美线性分离，而词汇基线保留显著信号；(b) 分级强度在中后期层解码性最强，尤其在注意力/MLP输出中，决策对齐在最终token前略早处最高；(c) 沿数据导出的效价方向进行加性引导可在晚期位点因果调节2-3边际，最大效应出现在晚期层注意力输出（attn_out L14）；(d) 头级修补/消融表明这些效应分布在多个头中而非集中在单个单元。

Conclusion: 这些结果将行为敏感性与可识别的内部表征和干预敏感位点联系起来，为更严格的反事实测试和更广泛的复制提供了具体的机制目标。该研究支持了关于AI感知和福利的更有证据基础的辩论，以及在制定政策、审计标准和安全保障时的治理。

Abstract: Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.

</details>


### [99] [Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing](https://arxiv.org/abs/2602.19160)
*Maciej Świechowski,Adam Żychowski,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 该论文从新颖视角评估大语言模型在形式化规则环境中的推理能力，通过通用游戏对弈任务测试四个LLM，分析游戏结构特征与性能相关性，并研究语言语义和训练数据暴露的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在形式化、规则驱动的环境中的推理能力，填补现有研究在逻辑基础问题解决方面的空白，探索LLM处理结构化规则系统的能力。

Method: 使用通用游戏对弈实例作为测试平台，评估四个LLM（Gemini 2.5 Pro/Flash、Llama 3.3 70B、GPT-OSS 120B）在前向模拟任务上的表现，包括下一步/多步状态制定和合法行动生成。基于40个结构特征对游戏进行表征，分析特征与性能相关性，并通过游戏混淆技术研究语言语义作用和训练数据暴露影响。

Result: 三个评估模型在大多数实验设置中表现良好，但随着评估步数增加（游戏步骤增多）性能下降。详细案例分析揭示了LLM在逻辑基础问题中的常见推理错误，包括规则幻觉、冗余状态事实和语法错误。游戏结构特征与模型性能存在相关性，语言语义在游戏定义中起重要作用。

Conclusion: 当代大语言模型在形式化推理能力方面取得明显进展，但仍存在局限性，特别是在多步推理和复杂规则处理方面。研究为理解LLM在结构化环境中的推理机制提供了新见解，并指出了未来改进方向。

Abstract: This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.

</details>


### [100] [Proximity-Based Multi-Turn Optimization: Practical Credit Assignment for LLM Agent Training](https://arxiv.org/abs/2602.19225)
*Yangyi Fang,Jiaye Lin,Xiaoliang Fu,Cong Qin,Haolin Shi,Chang Liu,Peilin Zhao*

Main category: cs.AI

TL;DR: ProxMO：一种基于邻近度的多轮优化框架，通过成功率感知调制和邻近度软聚合机制，在任务难度波动时更准确地分配信用，提升多轮LLM智能体训练效率


<details>
  <summary>Details</summary>
Motivation: 现有基于分组的策略优化方法在处理任务难度波动时存在缺陷，它们依赖离散批次内的统计偏差，经常错误分配信用。在现实场景中，简单任务的失败可能反映随机不稳定性，而高难度任务的成功则代表真正的能力突破，需要更精细的信用分配机制。

Method: 提出Proximity-based Multi-turn Optimization (ProxMO)框架，包含两个轻量级机制：1) 成功率感知调制：基于情节级难度动态调整梯度强度；2) 邻近度软聚合：在步骤级通过连续语义加权推导基线。该框架与标准GRPO框架兼容，可即插即用。

Result: 在ALFWorld和WebShop基准测试上的广泛评估表明，ProxMO相比现有基线带来了显著的性能提升，且计算成本可忽略不计。消融研究进一步验证了两个机制各自独立和协同的有效性。

Conclusion: ProxMO是一个实用且鲁棒的框架，专为现实世界部署约束设计，能够准确区分高价值信息信号和随机噪声，在多轮LLM智能体训练中实现样本高效学习，并易于在现有工业训练管道中采用。

Abstract: Multi-turn LLM agents are becoming pivotal to production systems, spanning customer service automation, e-commerce assistance, and interactive task management, where accurately distinguishing high-value informative signals from stochastic noise is critical for sample-efficient training. In real-world scenarios, a failure in a trivial task may reflect random instability, whereas success in a high-difficulty task signifies a genuine capability breakthrough. Yet, existing group-based policy optimization methods rigidly rely on statistical deviation within discrete batches, frequently misallocating credit when task difficulty fluctuates. To address this issue, we propose Proximity-based Multi-turn Optimization (ProxMO), a practical and robust framework engineered specifically for the constraints of real-world deployment. ProxMO integrates global context via two lightweight mechanisms: success-rate-aware modulation dynamically adapts gradient intensity based on episode-level difficulty, while proximity-based soft aggregation derives baselines through continuous semantic weighting at the step level. Extensive evaluations on ALFWorld and WebShop benchmarks demonstrate that ProxMO yields substantial performance gains over existing baselines with negligible computational cost. Ablation studies further validate the independent and synergistic efficacy of both mechanisms. Crucially, ProxMO offers plug-and-play compatibility with standard GRPO frameworks, facilitating immediate, low-friction adoption in existing industrial training pipelines. Our implementation is available at: \href{https://anonymous.4open.science/r/proxmo-B7E7/README.md}{https://anonymous.4open.science/r/proxmo}.

</details>


### [101] [Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering](https://arxiv.org/abs/2602.19240)
*Sen Zhao,Lincheng Zhou,Yue Chen,Ding Zou*

Main category: cs.AI

TL;DR: TopoRAG是一个用于文本图问答的拓扑增强检索增强生成框架，通过将文本图提升为胞腔复形来捕捉高维拓扑结构，解决现有RAG方法忽略循环结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本图的RAG方法主要关注低维结构（节点作为0维实体，边或路径作为1维关系），但忽略了循环结构，而循环在关系循环推理中至关重要。这种限制导致上下文基础不完整和推理能力受限。

Method: 1. 将文本图提升为胞腔复形以建模多维拓扑结构；2. 提出拓扑感知的子复形检索机制，提取与输入查询相关的胞腔复形；3. 设计多维拓扑推理机制，在这些复形上传播关系信息，指导LLM进行结构化、逻辑感知的推理。

Result: 实证评估表明，该方法在多种文本图任务上持续超越现有基线。

Conclusion: TopoRAG通过捕捉高维拓扑和关系依赖，有效增强了文本图问答中的推理能力，解决了现有RAG方法忽略循环结构的问题。

Abstract: Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.

</details>


### [102] [Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts](https://arxiv.org/abs/2602.19244)
*Toshihide Ubukata,Zhiyao Wang,Enhong Mu,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: 提出Soft Mixture-of-Experts框架，通过先验置信度门控机制结合多个RL专家，解决控制器合成中强化学习策略的各向异性泛化问题


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的OTF-DCS方法存在各向异性泛化问题：RL策略仅在特定参数空间区域表现良好，在其他区域脆弱，这源于训练随机性和轨迹依赖偏差

Method: 提出Soft Mixture-of-Experts框架，结合多个RL专家作为互补专业化组件，通过先验置信度门控机制动态选择专家

Result: 在航空交通基准测试中，Soft-MoE显著扩展了可解参数空间范围，相比任何单一专家都提高了鲁棒性

Conclusion: Soft-MoE框架有效解决了控制器合成中RL策略的各向异性泛化问题，通过专家组合实现了更广泛、更鲁棒的参数空间覆盖

Abstract: On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert.

</details>


### [103] [Limited Reasoning Space: The cage of long-horizon reasoning in LLMs](https://arxiv.org/abs/2602.19281)
*Zhenyu Li,Guanlin Wu,Cheems Wang,Yongqiang Zhao*

Main category: cs.AI

TL;DR: Halo框架通过模型预测控制动态调节LLM推理规划，解决传统静态规划方法在增加计算预算时可能导致的推理性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算策略（如思维链）在增加计算预算时可能出现性能崩溃，这源于静态规划方法无法感知LLM推理的内在边界，导致过度规划损害推理能力。

Method: 提出Halo框架：基于模型预测控制的LLM规划方法，采用熵驱动的双控制器和"测量-规划"策略，在推理边界处动态调节规划过程。

Result: Halo在复杂长时域任务上优于静态基线方法，能够有效利用计算扩展优势同时抑制过度规划问题。

Conclusion: 计算预算存在最优范围，过度规划会损害推理能力；Halo框架通过动态调节规划在推理边界处实现可控推理，解决了静态规划方法的局限性。

Abstract: The test-time compute strategy, such as Chain-of-Thought (CoT), has significantly enhanced the ability of large language models to solve complex tasks like logical reasoning. However, empirical studies indicate that simply increasing the compute budget can sometimes lead to a collapse in test-time performance when employing typical task decomposition strategies such as CoT. This work hypothesizes that reasoning failures with larger compute budgets stem from static planning methods, which hardly perceive the intrinsic boundaries of LLM reasoning. We term it as the Limited Reasoning Space hypothesis and perform theoretical analysis through the lens of a non-autonomous stochastic dynamical system. This insight suggests that there is an optimal range for compute budgets; over-planning can lead to redundant feedback and may even impair reasoning capabilities. To exploit the compute-scaling benefits and suppress over-planning, this work proposes Halo, a model predictive control framework for LLM planning. Halo is designed for long-horizon tasks with reason-based planning and crafts an entropy-driven dual controller, which adopts a Measure-then-Plan strategy to achieve controllable reasoning. Experimental results demonstrate that Halo outperforms static baselines on complex long-horizon tasks by dynamically regulating planning at the reasoning boundary.

</details>


### [104] [Automated Generation of Microfluidic Netlists using Large Language Models](https://arxiv.org/abs/2602.19297)
*Jasper Davidson,Skylar Stockham,Allen Boston,Ashton Snelgrove. Valerio Tenace,Pierre-Emmanuel Gaillardon*

Main category: cs.AI

TL;DR: 首次将大语言模型应用于微流控设计自动化，通过自然语言描述生成系统级结构Verilog网表，验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 微流控设备设计复杂限制了广泛应用，现有微流控设计自动化技术缺乏实用直观的解决方案，需要连接微流控实践者与自动化技术。

Method: 基于大语言模型在硬件描述语言代码生成方面的先前研究，提出将自然语言微流控设备规范转换为系统级结构Verilog网表的初步方法。

Result: 为典型微流控设计生成结构网表，功能流正确，平均语法准确率达到88%，验证了方法的可行性。

Conclusion: 这是大语言模型在微流控设计自动化领域的首次实际应用，为连接微流控实践者与自动化技术提供了初步解决方案。

Abstract: Microfluidic devices have emerged as powerful tools in various laboratory applications, but the complexity of their design limits accessibility for many practitioners. While progress has been made in microfluidic design automation (MFDA), a practical and intuitive solution is still needed to connect microfluidic practitioners with MFDA techniques. This work introduces the first practical application of large language models (LLMs) in this context, providing a preliminary demonstration. Building on prior research in hardware description language (HDL) code generation with LLMs, we propose an initial methodology to convert natural language microfluidic device specifications into system-level structural Verilog netlists. We demonstrate the feasibility of our approach by generating structural netlists for practical benchmarks representative of typical microfluidic designs with correct functional flow and an average syntactical accuracy of 88%.

</details>


### [105] [ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease](https://arxiv.org/abs/2602.19298)
*Nolan Brady,Tom Yeh*

Main category: cs.AI

TL;DR: ALPACA是一个用于阿尔茨海默病个性化序贯治疗策略研究的开源强化学习环境，基于ADNI数据训练CAST模型模拟疾病进展，RL策略在记忆相关指标上优于无治疗和临床医生基线。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的临床试验面临疾病周期长和患者异质性大的挑战，难以评估个性化序贯治疗策略。需要开发计算平台来系统探索现有疗法的个性化治疗方案。

Method: 开发ALPACA开源强化学习环境，基于ADNI纵向数据训练CAST模型模拟药物条件下的疾病进展，使用强化学习训练个性化治疗策略，并与无治疗和临床医生行为克隆基线比较。

Result: CAST模型能自回归生成真实的药物条件轨迹；在ALPACA中训练的强化学习策略在记忆相关指标上优于无治疗和临床医生基线；可解释性分析显示学习策略依赖临床相关患者特征进行决策。

Conclusion: ALPACA为研究阿尔茨海默病个体化序贯治疗决策提供了一个可重复使用的计算测试平台，能够系统探索个性化治疗策略并产生临床有意义的决策。

Abstract: Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD.

</details>


### [106] [Time Series, Vision, and Language: Exploring the Limits of Alignment in Contrastive Representation Spaces](https://arxiv.org/abs/2602.19367)
*Pratham Yashwante,Rose Yu*

Main category: cs.AI

TL;DR: 论文研究了时间序列是否参与多模态表示收敛，发现时间序列与视觉表示的对齐强于与文本的对齐，图像可作为时间序列与语言间的有效中介。


<details>
  <summary>Details</summary>
Motivation: 研究柏拉图表示假说在多模态学习中的适用性，特别是时间序列是否与视觉和语言共享潜在世界结构。现有研究主要关注视觉和语言，时间序列的参与情况尚不明确。

Method: 首先在三模态设置下检查独立预训练的时间序列、视觉和语言编码器的几何关系；然后通过对比学习训练投影头对冻结编码器进行后验对齐；分析对齐表示在几何、缩放行为、信息密度和输入模态特性方面的表现。

Result: 独立预训练编码器在没有显式耦合时呈现近正交几何；对比对齐后，整体对齐随模型规模提升，但存在不对称性：时间序列与视觉表示的对齐强于与文本的对齐；图像可作为时间序列与语言间的有效中介；更丰富的文本描述仅在一定阈值内改善对齐，超过阈值后无进一步改善；视觉表示也有类似效应。

Conclusion: 时间序列确实参与多模态表示收敛，但对齐模式存在不对称性。这些发现为构建超越视觉和语言的非传统数据模态多模态系统提供了重要考虑因素。

Abstract: The Platonic Representation Hypothesis posits that learned representations from models trained on different modalities converge to a shared latent structure of the world. However, this hypothesis has largely been examined in vision and language, and it remains unclear whether time series participate in such convergence. We first examine this in a trimodal setting and find that independently pretrained time series, vision, and language encoders exhibit near-orthogonal geometry in the absence of explicit coupling. We then apply post-hoc alignment by training projection heads over frozen encoders using contrastive learning, and analyze the resulting representations with respect to geometry, scaling behavior, and dependence on information density and input modality characteristics. Our investigation reveals that overall alignment in contrastive representation spaces improves with model size, but this alignment is asymmetric: time series align more strongly with visual representations than with text, and images can act as effective intermediaries between time series and language. We further see that richer textual descriptions improve alignment only up to a threshold; training on denser captions does not lead to further improvement. Analogous effects are observed for visual representations. Our findings shed light on considerations for building multimodal systems involving non-conventional data modalities beyond vision and language.

</details>


### [107] [Artificial Intelligence for Modeling & Simulation in Digital Twins](https://arxiv.org/abs/2602.19390)
*Philipp Zech,Istvan David*

Main category: cs.AI

TL;DR: 本章探讨建模与仿真（M&S）、人工智能（AI）和数字孪生（DTs）三者之间的互补关系，分析M&S在DTs中的核心作用以及DTs如何促进AI与M&S的融合。


<details>
  <summary>Details</summary>
Motivation: 随着建模与仿真（M&S）和人工智能（AI）的融合对先进数字技术产生深远影响，数字孪生（DTs）作为物理资产的高保真实时表示，成为企业数字化转型的关键推动者。理解M&S在DTs中的作用，以及DTs如何促进AI与M&S的融合至关重要。

Method: 首先建立对数字孪生的基础理解，详细阐述其关键组件、架构层次以及在业务、开发和运营中的各种角色。然后探讨M&S在DTs中的核心作用，概述从物理基础仿真、离散事件仿真到混合方法的关键建模技术。接着研究AI的双向作用：一方面分析AI如何通过高级分析、预测能力和自主决策增强DTs，另一方面探讨DTs如何作为训练、验证和部署AI模型的有价值平台。

Result: 本章提供了对M&S、AI和DTs三者互补关系的全面探索，明确了M&S在DTs实现中的核心地位，揭示了DTs作为AI与M&S融合平台的价值，并展示了AI如何通过增强分析、预测和决策能力提升DTs的智能水平。

Conclusion: 本章通过系统分析M&S、AI和DTs的相互关系，识别了创建更集成和智能系统的关键挑战和未来研究方向，为这一融合领域的发展提供了理论框架和实践指导。

Abstract: The convergence of modeling & simulation (M&S) and artificial intelligence (AI) is leaving its marks on advanced digital technology. Pertinent examples are digital twins (DTs) - high-fidelity, live representations of physical assets, and frequent enablers of corporate digital maturation and transformation. Often seen as technological platforms that integrate an array of services, DTs have the potential to bring AI-enabled M&S closer to end-users. It is, therefore, paramount to understand the role of M&S in DTs, and the role of digital twins in enabling the convergence of AI and M&S. To this end, this chapter provides a comprehensive exploration of the complementary relationship between these three. We begin by establishing a foundational understanding of DTs by detailing their key components, architectural layers, and their various roles across business, development, and operations. We then examine the central role of M&S in DTs and provide an overview of key modeling techniques from physics-based and discrete-event simulation to hybrid approaches. Subsequently, we investigate the bidirectional role of AI: first, how AI enhances DTs through advanced analytics, predictive capabilities, and autonomous decision-making, and second, how DTs serve as valuable platforms for training, validating, and deploying AI models. The chapter concludes by identifying key challenges and future research directions for creating more integrated and intelligent systems.

</details>


### [108] [Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement](https://arxiv.org/abs/2602.19396)
*Amirhossein Farzam,Majid Behabahani,Mani Malek,Yuriy Nevmyvaka,Guillermo Sapiro*

Main category: cs.AI

TL;DR: 提出ReDAct框架分离LLM激活中的语义因子对（目标与框架），构建FrameShield异常检测器提升越狱攻击检测，并用于可解释性分析


<details>
  <summary>Details</summary>
Motivation: LLM对流畅且语义连贯的越狱提示仍然脆弱，特别是攻击者通过操纵请求框架来隐藏恶意目标时，传统基于结构特征或目标特定签名的防御方法容易失效

Method: 1) 提出自监督框架分离LLM激活中的语义因子对（目标与框架）；2) 构建GoalFrameBench语料库控制目标与框架变化；3) 训练ReDAct模块在冻结LLM中提取解耦表示；4) 提出FrameShield基于框架表示的异常检测器

Result: 1) ReDAct提供理论保证；2) FrameShield在多个LLM家族上提升模型无关的检测性能，计算开销最小；3) 解耦表示作为可解释性探针，揭示目标与框架信号的独特特征

Conclusion: 语义解耦是LLM安全性和机制可解释性的基础构建块，ReDAct框架有效分离目标与框架表示，FrameShield提供高效越狱攻击检测，解耦表示增强模型可解释性

Abstract: Large language models (LLMs) remain vulnerable to jailbreak prompts that are fluent and semantically coherent, and therefore difficult to detect with standard heuristics. A particularly challenging failure mode occurs when an attacker tries to hide the malicious goal of their request by manipulating its framing to induce compliance. Because these attacks maintain malicious intent through a flexible presentation, defenses that rely on structural artifacts or goal-specific signatures can fail. Motivated by this, we introduce a self-supervised framework for disentangling semantic factor pairs in LLM activations at inference. We instantiate the framework for goal and framing and construct GoalFrameBench, a corpus of prompts with controlled goal and framing variations, which we use to train Representation Disentanglement on Activations (ReDAct) module to extract disentangled representations in a frozen LLM. We then propose FrameShield, an anomaly detector operating on the framing representations, which improves model-agnostic detection across multiple LLM families with minimal computational overhead. Theoretical guarantees for ReDAct and extensive empirical validations show that its disentanglement effectively powers FrameShield. Finally, we use disentanglement as an interpretability probe, revealing distinct profiles for goal and framing signals and positioning semantic disentanglement as a building block for both LLM safety and mechanistic interpretability.

</details>


### [109] [ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making](https://arxiv.org/abs/2602.19458)
*Ziyang Guo,Yifan Wu,Jason Hartline,Kenneth Holstein,Jessica Hullman*

Main category: cs.AI

TL;DR: ComplLLM是一个基于决策理论的后训练框架，通过将互补信息作为奖励来微调决策助手LLM，使其输出能够补充现有智能体决策的信号


<details>
  <summary>Details</summary>
Motivation: 当互补性成立时（即不同智能体带来独特信息以支持最终决策），多智能体决策流程可以超越单智能体工作流。需要开发能够利用这种互补性的方法

Method: 提出ComplLLM后训练框架，基于决策理论，使用互补信息作为奖励来微调决策助手LLM，使其输出能够补充现有智能体决策的信号

Result: 在涉及领域专家的合成和现实世界任务中验证了ComplLLM，证明该方法能够恢复已知的互补信息，并产生合理的互补信号解释以支持下游决策者

Conclusion: ComplLLM框架有效利用了多智能体系统中的互补性，通过微调LLM产生补充信号，为下游决策提供有价值的支持

Abstract: Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.

</details>


### [110] [Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark](https://arxiv.org/abs/2602.19502)
*Lalitha Pranathi Pulavarthy,Raajitha Muthyala,Aravind V Kuruvikkattil,Zhenan Yin,Rashmita Kudamala,Saptarshi Purkayastha*

Main category: cs.AI

TL;DR: 研究探索人类指导如何提升自主AI系统在临床预测任务中的表现，在AgentDS Healthcare基准测试的三个任务中取得优异成绩，揭示了人类指导在特征工程、多模态数据整合和模型选择中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管自主AI系统在数据科学工作流中能力不断增强，但临床预测任务需要领域专业知识，纯自动化方法难以提供。研究旨在探索人类指导如何提升自主AI系统在多模态临床预测中的表现。

Method: 采用人类指导的自主AI工作流方法，在三个临床预测任务中：人类分析师在关键决策点指导工作流，包括从临床记录、PDF账单收据和时间序列生命体征进行多模态特征工程、任务适当的模型选择以及临床信息验证策略。

Result: 在AgentDS Healthcare基准测试中：30天再入院预测（Macro-F1=0.8986）、急诊费用预测（MAE=$465.13）、出院准备评估（Macro-F1=0.7939）。整体排名第5，出院准备任务排名第3。消融研究显示人类指导决策累计提升+0.065 F1，多模态特征提取贡献最大单次提升（+0.041 F1）。

Conclusion: 研究提炼出三个可推广的经验：1）各管道阶段的领域信息特征工程产生复合增益，优于广泛自动搜索；2）多模态数据整合需要任务特定的人类判断，没有单一提取策略能通用；3）有意识的集成多样性与临床动机模型配置优于随机超参数搜索。这些发现为在需要可解释性、可重复性和临床有效性的医疗环境中部署自主AI提供了实用指导。

Abstract: Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.

</details>


### [111] [Classroom Final Exam: An Instructor-Tested Reasoning Benchmark](https://arxiv.org/abs/2602.19517)
*Chongyang Gao,Diji Yang,Shuyan Zhou,Xichen Yan,Luchuan Song,Shuo Li,Kezhen Chen*

Main category: cs.AI

TL;DR: CFE是一个多模态基准测试，用于评估大语言模型在20多个STEM领域的推理能力，基于真实的大学作业和考试题目，前沿模型表现仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个基于真实教育场景、覆盖广泛STEM领域、能够全面评估大语言模型推理能力的基准测试。现有基准往往使用合成数据或局限于特定领域，无法准确反映模型在复杂多步骤推理任务中的实际表现。

Method: 从大学课程中收集重复使用的真实作业和考试题目，由课程教师提供参考解答。将参考解答分解为推理流程进行分析，评估模型在中间状态推导和维护、步骤效率等方面的表现。

Result: 前沿模型在CFE基准上表现有限：Gemini-3.1-pro-preview总体准确率为59.69%，Gemini-3-flash-preview为55.46%。诊断分析显示，模型虽然能正确回答中间子问题，但在多步骤解决方案中难以可靠地推导和维护正确的中间状态，且生成的解决方案通常比教师提供的步骤更多，效率较低。

Conclusion: CFE基准揭示了当前大语言模型在复杂STEM推理任务中的局限性，特别是在多步骤推理的可靠性和效率方面。该基准为模型改进提供了重要方向，强调了提高中间状态管理能力和步骤效率的必要性。

Abstract: We introduce \CFE{} (\textbf{C}lassroom \textbf{F}inal \textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.

</details>


### [112] [Ada-RS: Adaptive Rejection Sampling for Selective Thinking](https://arxiv.org/abs/2602.19519)
*Yirou Ge,Yixi Li,Alec Chiu,Shivani Shekhar,Zijie Pan,Avinash Thangali,Yun-Shiuan Chuang,Chaitanya Kulkarni,Uma Kona,Linsey Pang,Prakhar Mehrotra*

Main category: cs.AI

TL;DR: Ada-RS是一种算法无关的样本过滤框架，通过自适应长度惩罚奖励对多个采样完成进行评分，应用随机拒绝采样保留高奖励候选，用于选择性高效推理，在延迟敏感部署中显著减少输出token和思考率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在成本和延迟敏感环境中部署时，虽然思维链能改善推理，但会在简单请求上浪费token。需要选择性思考机制来提高工具使用LLM的推理效率。

Method: 提出自适应拒绝采样（Ada-RS）框架：对给定上下文采样多个完成，使用自适应长度惩罚奖励进行评分，应用随机拒绝采样保留高奖励候选（或偏好对），用于下游优化。可集成到偏好对（如DPO）或分组策略优化（如DAPO）策略中。

Result: 在Qwen3-8B模型上使用LoRA，在合成工具调用导向的电商基准测试中，Ada-RS相比标准算法将平均输出token减少高达80%，思考率降低高达95%，同时保持或提高工具调用准确性，改善了准确性-效率边界。

Conclusion: 训练信号选择是延迟敏感部署中高效推理的有力杠杆，Ada-RS通过选择性采样优化实现了推理效率的显著提升。

Abstract: Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments.

</details>


### [113] [A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data](https://arxiv.org/abs/2602.19562)
*Joseph Bingham*

Main category: cs.AI

TL;DR: 该研究提出了一个计算框架，通过整合语言表达和从大规模众包图像中提取的感知表征，模拟人类指称解释的核心方面，在斯坦福重复指称游戏语料库上实现了超越人类的表现。


<details>
  <summary>Details</summary>
Motivation: 建立自然语言表达与视觉感知之间的稳定映射是认知科学和人工智能的基础问题。人类能够在嘈杂、模糊的感知环境中进行语言指称，但支持这种跨模态对齐的机制仍不清楚。本研究旨在通过计算模型来理解人类指称解释的核心机制。

Method: 提出了一个计算框架，结合尺度不变特征变换（SIFT）对齐和通用质量指数（UQI）来量化认知合理特征空间中的相似性，近似人类感知分类。同时采用语言预处理和查询转换操作来捕捉指称表达中的语用变异性。

Result: 在斯坦福重复指称游戏语料库（15,000个话语与七巧板刺激配对）上评估，该框架实现了稳健的指称接地。与人类对话者相比，达到稳定映射所需的话语减少了65%，并且能够从单个指称表达中正确识别目标对象的准确率达到41.66%（人类为20%）。

Conclusion: 相对简单的感知-语言对齐机制可以在经典认知基准上产生与人类竞争的行为，为接地通信、感知推理和跨模态概念形成的模型提供了见解。

Abstract: Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\% of the time (versus 20\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .

</details>


### [114] [Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent](https://arxiv.org/abs/2602.19837)
*Björn Hoppmann,Christoph Scholz*

Main category: cs.AI

TL;DR: 该论文是一篇关于元学习和元强化学习的综述性文章，系统性地形式化了这些概念，并梳理了从早期算法到DeepMind自适应智能体的发展历程。


<details>
  <summary>Details</summary>
Motivation: 人类能够有效利用先验知识适应新任务，而标准机器学习模型依赖任务特定训练难以实现这种能力。元学习通过从多个任务中获取可迁移知识，使模型能够用少量数据快速适应新挑战。

Method: 采用基于任务的元学习和元强化学习形式化框架，系统性地回顾和整理该领域的关键算法发展路径，特别关注通向DeepMind自适应智能体的里程碑式工作。

Result: 提供了元学习和元强化学习的统一形式化框架，梳理了该领域的重要算法发展脉络，为理解自适应智能体和其他通用方法提供了必要的概念基础。

Conclusion: 该综述通过系统性的形式化框架，整合了理解自适应智能体和通用方法所需的核心概念，展示了元学习在实现快速适应新任务方面的重要进展和潜力。

Abstract: Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.

</details>


### [115] [Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning](https://arxiv.org/abs/2602.19914)
*Thatchawin Leelawat,Lewis D Griffin*

Main category: cs.AI

TL;DR: 该研究将Watson & Holmes侦探桌游改编为AI推理基准，通过渐进式叙事证据、开放式问题和无约束语言响应评估AI推理能力，发现AI模型在9个月内从人类表现的下四分位数提升至前5%。


<details>
  <summary>Details</summary>
Motivation: 现有AI推理基准在评估AI推理能力与人类自然情境下推理的相似性方面存在局限，需要开发更贴近人类真实推理过程的评估方法。

Method: 将Watson & Holmes侦探桌游改编为基准测试，包含渐进式呈现的叙事证据、开放式问题和无约束语言响应；开发自动化评分系统并与人类评估者验证，确保可扩展和可复现的性能评估。

Result: 在2025年的9个月中，AI模型表现从人类比较组的下四分位数提升至约前5%；约一半改进来自连续模型发布的稳步进步，另一半与推理导向模型架构的显著跃升相关；AI与人类表现差异主要在于：模型在解决较长案件（1900-4000词）时表现下降，推理模型在证据稀缺的早期阶段具有归纳推理优势。

Conclusion: 该基准有效评估了AI推理能力的进步轨迹，揭示了AI推理与人类推理的相似性和差异，特别是推理导向架构带来的显著提升以及在处理长文本和早期归纳推理方面的特定模式。

Abstract: Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.

</details>


### [116] [Latent Introspection: Models Can Detect Prior Concept Injections](https://arxiv.org/abs/2602.20031)
*Theia Pearson-Vogel,Martin Vanek,Raymond Douglas,Jan Kulveit*

Main category: cs.AI

TL;DR: Qwen 32B模型具有潜在的内省能力，能够检测到早期上下文中注入的概念并识别具体是哪个概念，这种能力可以通过提示增强。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型是否具有内省能力，即模型能否意识到其内部状态或上下文中的特定操作，这对于理解模型的潜在推理机制和安全影响具有重要意义。

Method: 使用Qwen 32B模型进行概念注入实验，通过logit lens分析残差流中的检测信号，并设计提示策略来增强模型的内省能力，同时测量互信息等指标。

Result: 模型在采样输出中否认注入，但在残差流中显示出清晰的检测信号；通过准确提示AI内省机制，对注入的敏感性从0.3%大幅提升至39.2%，假阳性仅增加0.6%；九个注入与恢复概念间的互信息从0.62比特提升至1.05比特。

Conclusion: 模型具有令人惊讶的内省和引导意识能力，这种能力容易被忽视，对潜在推理和安全具有重要影响，表明可以通过适当的提示策略显著增强模型的内省能力。

Abstract: We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.

</details>


### [117] [CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching](https://arxiv.org/abs/2602.20094)
*Yuzhe Wang,Yaochen Zhu,Jundong Li*

Main category: cs.AI

TL;DR: 提出了CausalFlip基准测试，通过构建语义相似但因果答案相反的问题对，以及添加噪声前缀，来评估LLMs是否真正基于因果关系而非语义相关性进行推理。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂高风险决策场景中部署增加，需要确保其推理基于因果关系而非虚假相关性。传统推理基准的高性能可能源于记忆语义模式而非分析真实因果结构，因此需要专门评估因果推理能力的基准。

Method: 构建CausalFlip基准：1) 基于事件三元组构建因果判断问题，形成混淆、链式和碰撞关系；2) 为每个事件三元组创建语义相似但因果答案相反的问题对；3) 引入噪声前缀评估，在中间推理步骤前添加因果无关文本；4) 评估多种训练范式：仅答案训练、显式思维链监督、内部化因果推理方法。

Result: 显式思维链训练仍可能被虚假语义相关性误导，而内部化推理方法显著改善了因果基础，表明更好地激发基础LLMs的潜在因果推理能力是有前景的。

Conclusion: 需要新的LLM范式或训练算法来确保推理基于因果关系而非语义相关性，内部化因果推理方法能有效减少对相关性的显式依赖，提高因果推理的鲁棒性。

Abstract: As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.

</details>


### [118] [Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration](https://arxiv.org/abs/2602.20104)
*Hasan Amin,Ming Yin,Rajiv Khanna*

Main category: cs.AI

TL;DR: 提出自适应AI集成系统，通过交替使用对齐模型和互补模型解决人类-AI决策中的性能提升与信任建设之间的根本矛盾


<details>
  <summary>Details</summary>
Motivation: 传统单一AI模型在人类-AI决策中存在根本矛盾：互补性AI能提升性能但会降低信任，对齐性AI能建立信任但会强化次优人类行为。需要解决这一局限性以优化人类-AI协作效果。

Method: 提出人类中心的自适应AI集成系统，采用理性路由捷径机制，基于上下文线索在两种专家模型间切换：对齐模型（建立信任）和互补模型（提升性能）。该方法被证明是接近最优的。

Result: 理论分析阐明了自适应AI集成的有效性及其最大效益条件。模拟和真实世界数据实验表明，使用自适应AI集成的人类决策者比使用单一AI模型（包括优化独立性能或人类-AI团队性能的模型）获得显著更高的性能。

Conclusion: 自适应AI集成通过动态切换对齐和互补模型，有效解决了人类-AI决策中的性能-信任权衡问题，显著提升了人类-AI团队的整体决策性能。

Abstract: In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.

</details>


### [119] [ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models](https://arxiv.org/abs/2602.20117)
*Andre He,Nathaniel Weir,Kaj Bostrom,Allen Nie,Darion Cassel,Sam Bayless,Huzefa Rangwala*

Main category: cs.AI

TL;DR: ReSyn是一个生成多样化推理环境的管道，通过可验证奖励的强化学习训练推理语言模型，在多个基准测试中取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖解决方案标注（成本高），要么局限于少数手工制作的环境，需要一种能够大规模生成多样化推理环境的方法来扩展可验证奖励的强化学习

Method: 提出ReSyn管道，自动生成包含实例生成器和验证器的多样化推理环境，涵盖约束满足、算法谜题和空间推理等任务，使用这些环境通过可验证奖励的强化学习训练推理语言模型

Result: 使用ReSyn数据训练的Qwen2.5-7B-Instruct模型在推理基准测试和跨领域数学基准测试中均取得一致提升，在挑战性的BBEH基准上相对改进27%，消融实验显示验证器监督和任务多样性都有显著贡献

Conclusion: 大规模生成推理环境可以有效增强推理语言模型的推理能力，验证器监督和任务多样性是关键因素，为可验证奖励的强化学习提供了可扩展的解决方案

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs

</details>
