<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: SA-SFT：一种轻量级自增强方法，通过让LLM生成自我对话数据并与任务数据混合，在不修改优化或训练计划的情况下缓解灾难性遗忘，同时提升领域内性能。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）上针对特定任务数据进行微调时，灾难性遗忘是一个主要挑战，通常会降低模型的通用知识和推理能力。现有方法如层冻结或外部数据混合存在局限性，需要更简单有效的解决方案。

Method: SA-SFT（自增强监督微调）：在微调前，让LLM生成自我对话数据，然后将这些自生成的数据与任务数据混合，不改变优化或训练计划。该方法无需外部数据或额外调优，通过自对齐机制缓解风格诱导的参数漂移。

Result: 在50个评估场景中，SA-SFT保持了与原始模型相当的性能，并在40个案例中取得最佳结果，优于层冻结和外部数据混合等基线方法。该方法既能缓解灾难性遗忘，又能提升领域内性能。

Conclusion: 自增强提供了一种简单有效的机制，用于实现鲁棒的LLM适应而不引发灾难性遗忘。理论分析表明遗忘部分源于风格诱导的参数漂移，而通过自生成数据的自对齐能有效抵消这种效应。

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [2] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: 知识蒸馏显著提升小型语言模型效率，8B蒸馏模型比普通训练计算效率高2000倍，推理能力媲美10倍大小标准模型


<details>
  <summary>Details</summary>
Motivation: 为资源受限环境开发高效的小型语言模型，验证知识蒸馏作为构建先进、可访问AI的主要策略而非仅仅是压缩技术

Method: 对蒸馏模型与普通模型及专有模型进行性能与计算成本基准测试，提供效率的定量分析

Result: 蒸馏创造了更优的性能-计算曲线，8B蒸馏模型比普通训练计算效率高2000倍以上，推理能力达到或超过其10倍大小的标准模型

Conclusion: 知识蒸馏不仅是压缩技术，更是构建最先进、可访问AI的主要策略，为资源受限环境提供高效解决方案

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [3] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 该研究通过语言学特征分析发现查询形式影响LLM幻觉概率，识别出22个查询特征与幻觉风险的相关性，为查询改写和干预研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 传统上LLM幻觉被视为模型或解码策略的缺陷，但作者从语言学角度提出查询形式本身也会影响模型响应。研究旨在识别哪些查询特征会增加幻觉概率，为降低幻觉风险提供新视角。

Method: 构建22维查询特征向量，涵盖从句复杂性、词汇稀有性、指代、否定、可回答性和意图基础等语言学维度。使用369,837个真实世界查询进行大规模分析，探索查询特征与幻觉倾向的相关性。

Result: 分析揭示了"风险图谱"：深层从句嵌套和欠明确性等特征与较高幻觉倾向相关；清晰的意图基础和可回答性与较低幻觉率相关；领域特异性等特征则呈现混合、数据集和模型依赖的效果。

Conclusion: 研究建立了与幻觉风险相关的可观察查询特征表示，为引导性查询改写和未来干预研究铺平了道路，表明通过优化查询形式可以降低LLM幻觉风险。

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [4] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: QueryBandits：针对闭源大语言模型幻觉问题的模型无关上下文赌博机框架，通过在线学习选择最优查询重写策略，无需模型重训练或梯度调整


<details>
  <summary>Details</summary>
Motivation: 当前大多数缓解大语言模型幻觉的研究集中在开源模型的检测和参数编辑上，而闭源模型在机构部署中占绝大多数却缺乏相应研究。需要一种无需访问模型内部参数的方法来减少幻觉。

Method: 提出QueryBandits框架，这是一个模型无关的上下文赌博机系统。它通过经验验证和校准的奖励函数在线学习选择最优查询重写策略（如改写、扩展等）。框架利用语义特征学习在线策略，仅通过前向传递机制改变模型行为。

Result: 在16个QA场景中，Thompson Sampling策略的QueryBandits相比无重写基线获得87.5%胜率，分别比零次静态策略（改写和扩展）高出42.6%和60.3%。所有上下文赌博机在所有数据集上都优于普通赌博机，特征方差越大，臂选择方差也越大，证实了没有单一最优重写策略的发现。

Conclusion: QueryBandits通过纯前向传递机制有效减少闭源大语言模型的幻觉，无需重训练或基于梯度的调整。研究发现某些静态策略比无重写策略产生更高的累积遗憾，表明不灵活的重写策略可能加剧幻觉。该框架为闭源模型提供了实用的幻觉缓解方案。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [5] [How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity](https://arxiv.org/abs/2602.20372)
*Chundra Cathcart,Arne Rubehn,Katja Bocklage,Luca Ciucci,Kellen Parker van Dam,Alžběta Kučerová,Jekaterina Mažara,Carlo Y. Meloni,David Snee,Johann-Mattis List*

Main category: cs.CL

TL;DR: 该研究发现许多语言的数字系统在沟通效率上远低于预期，挑战了先前关于递归数字系统优化效率的观点。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为精确递归数字系统通过平衡数字词汇库大小和数字术语的平均形态句法复杂性来优化沟通效率，但作者认为这些研究未能充分考虑语言实际显示的复杂性程度。

Method: 使用来自52种遗传多样语言的语料，采用区分可预测和不可预测异形（形式变异）的标注方案进行分析。

Result: 研究表明世界上许多语言的数字系统效率明显低于预期水平，未能达到理论上的最优效率。

Conclusion: 这一发现对数字系统研究和更广泛的语言演化研究具有重要意义，表明实际语言系统可能并不总是追求最优沟通效率。

Abstract: Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.

</details>


### [6] [CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models](https://arxiv.org/abs/2602.20648)
*Anqi Li,Chenxiao Wang,Yu Lu,Renjun Xu,Lizhi Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: CARE是一个基于LLM的框架，用于从心理咨询转录本中自动预测多维联盟评分并生成可解释的推理，在CounselingWAI数据集上训练，显著提升了与客户感知联盟的相关性。


<details>
  <summary>Details</summary>
Motivation: 传统心理咨询联盟评估方法存在局限性：事后问卷负担重且延迟，现有计算方法产生粗糙评分、缺乏可解释推理、无法建模完整会话上下文。需要更准确、及时、可解释的联盟评估工具。

Method: 基于LLaMA-3.1-8B-Instruct骨干，在CounselingWAI数据集上使用推理增强监督进行微调，数据集包含9,516个专家策划的推理。框架自动预测多维联盟评分并生成可解释推理。

Result: CARE优于主流LLM，将咨询师评估与客户感知联盟的差距显著缩小，与客户评分的Pearson相关性提高70%以上。推理增强监督进一步提升预测准确性。自动和人工评估均验证了推理质量。

Conclusion: CARE作为AI辅助工具在心理健康护理中具有潜力，应用于真实中文在线咨询会话时，能识别联盟建设挑战、展示互动模式如何影响联盟发展，并提供可操作的见解。

Abstract: Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.

</details>


### [7] [ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition](https://arxiv.org/abs/2602.20727)
*Xindian Ma,Rundong Kong,Peng Zhang,Ruoxiang Huang,Yongyu Jiang*

Main category: cs.CL

TL;DR: ID-LoRA是一种新型参数高效微调框架，通过从预训练权重矩阵中提取和重用聚类参数组，形成多个共享单个可训练低秩矩阵的低秩组件，在减少可训练参数的同时保持模型能力。


<details>
  <summary>Details</summary>
Motivation: LoRA及其变体在大规模模型扩展时仍引入大量可训练参数开销，而过度降低秩又会在复杂多任务场景中显著降低性能，需要打破这种权衡。

Method: 从预训练权重矩阵中提取聚类参数组，重用这些组形成多个低秩组件，所有组件共享单个初始化的可训练低秩矩阵，从而减少可训练参数数量。

Result: 在五个基准测试（数学推理、代码生成、MMLU、常识问答、安全对齐）中，ID-LoRA优于全参数微调和现有PEFT基线（LoRA、DoRA、HydraLoRA），同时使用比标准LoRA少46%的可训练参数。在多任务场景中，在代码和MMLU任务上超越LoRA及其变体，仅需传统LoRA 54%的参数。

Conclusion: ID-LoRA成功打破了参数效率与性能之间的权衡，在显著减少可训练参数的同时保持甚至提升模型性能，为大规模语言模型的高效微调提供了新方案。

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

</details>


### [8] [Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization](https://arxiv.org/abs/2602.20743)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: 提出自适应文本匿名化任务，通过任务特定提示优化框架自动生成匿名化指令，在五个数据集上优于现有基线，实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 文本匿名化是高度上下文敏感的问题，现有方法依赖静态手动设计策略，缺乏适应不同需求的灵活性，且难以跨领域泛化。

Method: 提出自适应文本匿名化任务框架，通过任务特定提示优化自动构建语言模型的匿名化指令，适应不同隐私目标、领域和下游使用模式。

Result: 在五个不同领域、隐私约束和效用目标的数据集上评估，框架在所有设置中均优于现有基线，实现更好的隐私-效用权衡，计算效率高，开源模型性能与大型闭源模型相当。

Conclusion: 自适应文本匿名化框架能够自动适应特定隐私-效用需求，发现新颖的匿名化策略，探索隐私-效用权衡前沿的不同点，优于静态手动设计方法。

Abstract: Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.

</details>


### [9] [Explicit Grammar Semantic Feature Fusion for Robust Text Classification](https://arxiv.org/abs/2602.20749)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CL

TL;DR: 提出一种轻量级文本分类模型，通过显式编码句子级语法结构（句法组成、短语模式、复杂度指标）生成紧凑语法向量，并与冻结的上下文嵌入融合，构建统一特征表示，在资源受限环境下优于基线模型2%-15%


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的NLP模型计算密集，不适合资源受限环境。需要构建既能捕获语法结构特征又轻量化的分类模型，避免使用全参数化Transformer或重型深度学习架构

Method: 将句子级语法结构（句法组成、短语模式、复杂度指标）显式编码为紧凑语法向量，与冻结的上下文嵌入融合，形成统一特征表示。将语法作为显式归纳偏置而非可学习模块，使用DBNs、LSTMs、BiLSTMs、BERT和XLNET等模型进行训练评估

Result: 统一特征表示模型能同时捕获文本的语义和结构特性，在异构领域实现更有效学习，性能优于基线模型2%-15%。相比先前通过额外注意力层、树编码器或全微调注入语法结构的语法感知Transformer模型，该框架更轻量且在边缘设备上表现更好

Conclusion: 提出的框架通过将语法作为显式归纳偏置而非可学习模块，构建了轻量级但性能优越的文本分类模型，特别适合资源受限环境，为语法结构在NLP模型中的有效整合提供了新思路

Abstract: Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices

</details>


### [10] [SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing](https://arxiv.org/abs/2602.20751)
*Yifei Xu,Guilherme Potje,Shivam Shandilya,Tiancheng Yuan,Leonardo de Oliveira Nunes,Rakshanda Agarwal,Saeid Asgari,Adam Atkinson,Emre Kıcıman,Songwu Lu,Ranveer Chandra,Tusher Chakraborty*

Main category: cs.CL

TL;DR: SibylSense：一种通过可调记忆库自适应生成评估标准的推理时学习方法，用于改进开放生成任务的强化学习后训练


<details>
  <summary>Details</summary>
Motivation: 开放生成任务的RL后训练面临奖励函数设计的挑战：专家制定的评估标准成本高，提示生成的评估标准往往肤浅或不一致，固定池判别性评估标准容易饱和和漂移，导致奖励黑客攻击

Method: SibylSense采用推理时学习方法，通过可调记忆库自适应冻结的评估标准生成器。记忆库通过验证器基于项目奖励进行更新，这些奖励通过少量示例的参考-候选答案判别性差距来衡量。系统交替进行记忆调优和评估标准对抗性策略更新，生成满足评估标准的候选答案，缩小判别性差距，驱动评估标准生成器捕捉新的质量维度

Result: 在两个开放生成任务上的实验表明，SibylSense能产生更具判别性的评估标准，并在下游RL性能上优于静态和非自适应基线方法

Conclusion: SibylSense通过自适应评估标准生成解决了RL后训练中的奖励设计问题，提供了一种可扩展且鲁棒的评估标准构建方法，能持续改进开放生成任务的强化学习性能

Abstract: Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.

</details>


### [11] [Overton Pluralistic Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.20759)
*Yu Fu,Seongho Son,Ilija Bogunovic*

Main category: cs.CL

TL;DR: OP-GRPO是一个强化学习框架，通过双奖励系统使单个大语言模型能够生成多元视角的回应，无需显式提示或模块化编排，实现了"小模型，大视角覆盖"的效果。


<details>
  <summary>Details</summary>
Motivation: 现有对齐范式在捕捉人类价值观的多元性方面存在局限，无法充分反映真实世界中多样化的观点和立场。

Method: 采用两阶段方法：1) 训练相似性估计器（Sentence Transformer）用于评估生成回应的覆盖度；2) OP-GRPO训练，将相似性估计器集成到双奖励系统中，确保广泛覆盖真实人类视角并保持每个视角的独特性。

Result: Qwen2.5-3B-Instruct模型在自然语言推理基准上相对20B GPT-OSS基线获得37.4%准确率提升，相对模块化架构基线获得19.1%改进。GPT-4.1评估进一步证实了方法的鲁棒性。

Conclusion: OP-GRPO框架成功实现了隐式Overton多元主义，使单个语言模型能够生成具有多样视角的回应，在保持模型规模较小的同时显著提升了视角覆盖度。

Abstract: Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a "small models, big perspective coverage" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.

</details>


### [12] [Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation](https://arxiv.org/abs/2602.20816)
*Sayantan Dasgupta,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出一种新的尾部分布感知的KL散度变体，用于语言模型蒸馏，通过解耦教师模型前K个高概率预测与低概率预测的贡献，增强分布尾部信息的影响。


<details>
  <summary>Details</summary>
Motivation: 传统KL散度在语言模型蒸馏中主要受教师模型最高概率的token（模式）主导，而忽略了低概率但可能包含有用信息的分布尾部成分，这限制了蒸馏效果。

Method: 提出尾部分布感知散度，将教师模型的前K个最高概率预测与其余低概率预测的贡献解耦，保持与KL散度相同的计算复杂度，但减少教师模式的影响，增强分布尾部的作用。

Result: 实验表明，该方法在解码器模型的预训练和监督蒸馏中均能取得有竞争力的性能，且蒸馏过程高效，可在学术预算下处理大型数据集，无需工业级计算资源。

Conclusion: 通过解耦教师模型高概率模式与低概率尾部的贡献，提出的尾部分布感知散度改进了语言模型蒸馏，在保持计算效率的同时提升了性能，使大规模蒸馏更加可行。

Abstract: The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.

</details>


### [13] [FinAnchor: Aligned Multi-Model Representations for Financial Prediction](https://arxiv.org/abs/2602.20859)
*Zirui He,Huopu Zhang,Yanguang Liu,Sirui Wu,Mengnan Du*

Main category: cs.CL

TL;DR: FinAnchor框架通过锚定异构LLM嵌入表示，无需微调即可提升金融长文档预测性能


<details>
  <summary>Details</summary>
Motivation: 金融长文档预测面临信号稀疏、噪声干扰以及不同任务和时间段下最优LLM选择困难的问题，现有方法难以有效整合多个LLM的异构表示

Method: 提出FinAnchor框架：1) 选择锚定嵌入空间；2) 学习线性映射将其他模型的表示对齐到锚定空间；3) 聚合对齐后的特征形成统一表示用于下游预测

Result: 在多个金融NLP任务中，FinAnchor持续超越强单模型基线和标准集成方法，证明锚定异构表示对稳健金融预测的有效性

Conclusion: FinAnchor通过轻量级框架有效整合多个LLM的异构嵌入表示，无需微调底层模型即可显著提升金融长文档预测的鲁棒性和性能

Abstract: Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.

</details>


### [14] [Exa-PSD: a new Persian sentiment analysis dataset on Twitter](https://arxiv.org/abs/2602.20892)
*Seyed Himan Ghaderi,Saeed Sarbazi Azad,Mohammad Mehdi Jaziriyan,Ahmad Akbari*

Main category: cs.CL

TL;DR: 该论文介绍了Exa波斯语情感分析数据集，包含12,000条波斯语推文，标注为积极、中立、消极三类，使用预训练模型评估达到79.87 Macro F-score。


<details>
  <summary>Details</summary>
Motivation: 波斯语自然语言处理面临挑战，现有数据集多局限于特定主题（如产品、食品、酒店），而社交媒体中用户常使用讽刺、口语化表达，因此需要专门针对波斯语推文的情感分析数据集。

Method: 收集波斯语推文构建Exa数据集，包含12,000条推文，由5名波斯语母语标注者进行标注，分为积极、中立、消极三类。使用预训练的Pars Bert和Roberta模型作为基础模型进行评估。

Result: 数据集评估达到79.87 Macro F-score，表明该数据集和模型对于波斯语情感分析系统具有足够价值。

Conclusion: Exa波斯语情感分析数据集填补了波斯语社交媒体情感分析数据集的空白，为波斯语NLP研究提供了有价值的资源，预训练模型在该数据集上的良好表现证明了其实用性。

Abstract: Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.

</details>


### [15] [Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models](https://arxiv.org/abs/2602.20966)
*Paola Merlo,Chunyang Jiang,Giuseppe Samo,Vivi Nastase*

Main category: cs.CL

TL;DR: BLM任务是一种受智力测试启发的多选语言任务，具有多层结构，用于评估LLMs检测语言对象、属性和跨句系统模式的能力。


<details>
  <summary>Details</summary>
Motivation: 创建结构化、精心策划的数据集来回答关于大语言模型能力的核心问题：LLMs是否能检测语言对象及其属性？是否能检测和使用跨句的系统模式？更容易犯语言错误还是推理错误？这些错误如何相互作用？

Method: 设计了Blackbird语言矩阵任务，这是一种多选问题，具有多层结构（句内、输入序列内、候选答案内）。构建了BLM数据集，使用简单基线模型和定制模型进行实验，评估模型在分块和系统性方面的表现。

Result: BLM任务虽然具有挑战性，但可以在多种语言中以良好性能解决；简单基线模型表现尚可，定制模型性能更好；模型表示包含解决语言任务所需的语法对象和属性；解决方案通过检测跨句的系统模式实现。

Conclusion: 精心策划的结构化数据集支持对语言和大语言模型属性的多方面研究。BLM数据集因其结构化设计、包含学习上下文和预期答案、部分手工构建等特点，可用于可解释性研究，帮助理解LLMs的行为机制。

Abstract: This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?
  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.
  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.

</details>


### [16] [Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving](https://arxiv.org/abs/2602.20973)
*Yuliang Ji,Fuchen Shen,Jian Wu,Qiujie Xie,Yue Zhang*

Main category: cs.CL

TL;DR: 论文提出了PC-FOL数据集，专注于案例推理而非传统线性推理，用于评估大语言模型的数学推理能力，并揭示了线性推理与案例推理之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集主要关注线性推理，忽视了反证法、分情况证明等其他重要推理形式，这些对于全面评估大语言模型的推理能力至关重要。

Method: 1. 引入由专业数学家标注的新型一阶逻辑数据集PC-FOL，专注于案例推理问题；2. 所有实例都配备手动编写的自然语言证明；3. 在领先的大语言模型上进行实验；4. 基于图模型进行理论分析，解释两种推理问题之间的差异。

Result: 实验结果显示，在大语言模型上，线性推理与案例推理问题之间存在显著的性能差距。基于图模型的理论分析为观察到的这种差异提供了合理解释。

Conclusion: PC-FOL数据集揭示了自动自然语言数学证明生成领域的核心挑战，为未来研究铺平了道路。该工作强调了评估大语言模型数学推理能力时需要超越线性推理，考虑更复杂的推理形式。

Abstract: To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.

</details>


### [17] [Evaluating Proactive Risk Awareness of Large Language Models](https://arxiv.org/abs/2602.20976)
*Xuan Luo,Yubin Chen,Zhiyu Hou,Linpu Yu,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 论文提出了一种主动风险意识评估框架，用于衡量大语言模型能否在损害发生前预测潜在危害并提供警告，并在环境生态领域构建了Butterfly数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常决策中应用日益广泛，其安全责任已从应对显性恶意意图扩展到预测非故意但具有严重后果的风险。需要评估LLMs能否在损害发生前主动识别潜在危害。

Method: 提出了主动风险意识评估框架，构建了包含1,094个查询的Butterfly数据集，模拟可能产生潜在生态影响的普通解决方案寻求活动。在五个广泛使用的LLMs上进行实验，分析响应长度、语言和多模态的影响。

Result: 实验结果显示：在长度受限的响应下主动意识显著下降；跨语言相似性；在（多模态）物种保护方面存在持续盲点。这些发现突显了当前安全对齐与实际生态责任要求之间的关键差距。

Conclusion: 当前LLMs在主动风险意识方面存在不足，特别是在生态保护领域。研究强调了在LLM部署中需要建立主动安全保障机制，以应对现实世界的生态责任要求。

Abstract: As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.

</details>


### [18] [Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification](https://arxiv.org/abs/2602.21082)
*Vishal Patil,Shree Vaishnavi Bacha,Revanth Yamani,Yidan Sun,Mayank Kejriwal*

Main category: cs.CL

TL;DR: 该研究提出了一种混合方法，结合LLMs进行方面识别和传统机器学习进行情感分类，以大规模分析客户评论，并在470万条餐厅评论上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 客户评论已成为企业和顾客的重要信息来源，但有效分析数百万条非结构化评论仍然具有挑战性。虽然LLMs在自然语言理解方面表现出潜力，但由于计算成本和可扩展性问题，它们在大规模评论分析中的应用受到限制。

Method: 采用混合方法：使用ChatGPT分析餐厅评论样本进行方面识别，同时使用经典机器学习方法进行大规模情感分类。基于人工标注的评论开发情感分类器，并将其应用于从主要在线平台收集的470万条17年间的评论。

Result: 回归分析显示，机器标注的方面显著解释了不同餐饮体验方面、菜系和地理区域的餐厅总体评分的方差。该方法能够有效自动化大规模客户反馈的基于方面的情感分析。

Conclusion: 结合LLMs与传统机器学习方法可以有效自动化大规模客户反馈的基于方面的情感分析，为酒店业和其他服务行业的研究者和从业者提供了一个实用框架。

Abstract: Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.

</details>


### [19] [Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.21103)
*Sanket Badhe,Deep Shah*

Main category: cs.CL

TL;DR: PLD（提示级蒸馏）通过从教师模型中提取显式推理模式，组织成结构化指令列表作为学生模型的系统提示，使小模型在保持可解释性的同时达到前沿性能，且延迟开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：Chain-of-Thought提示准确但延迟高、推理成本大；微调小模型牺牲可解释性且引入显著资源开销。需要一种既能保持高性能又具有可解释性、低延迟的方法。

Method: 提出Prompt-Level Distillation（PLD）：从教师模型中提取显式推理模式，将其组织成结构化、表达性强的指令列表，作为学生模型的系统提示，实现知识蒸馏。

Result: 在StereoSet和Contract-NLI数据集上使用Gemma-3 4B评估，PLD将Macro F1分数分别从57%提升至90.0%和从67%提升至83%，使紧凑模型以可忽略的延迟开销达到前沿性能。

Conclusion: PLD方法使小模型在保持高性能的同时具有完全透明的决策过程，支持人类逻辑验证，适用于法律、金融、内容审核等受监管行业，以及高吞吐量用例和边缘设备。

Abstract: Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\% to 90.0\% and 67\% to 83\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.

</details>


### [20] [PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data](https://arxiv.org/abs/2602.21165)
*Samah Fodeh,Linhai Ma,Yan Wang,Srivani Talakokkul,Ganesh Puthiaraju,Afshan Khan,Ashley Hagaman,Sarah Lowe,Aimee Roundtree*

Main category: cs.CL

TL;DR: PVminer是一个领域适应的NLP框架，用于在安全的医患通信中结构化患者声音，通过多标签多类别预测任务整合患者特定BERT编码器和主题建模，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 患者生成的文本包含丰富的患者声音表达，反映沟通行为和社会健康决定因素。传统定性编码方法劳动密集且无法扩展到大规模数据，现有ML/NLP方法通常将患者中心沟通和社会健康决定因素视为独立任务，或依赖不适合患者语言的模型。

Method: PVminer将患者声音检测制定为多标签多类别预测任务，整合患者特定BERT编码器（PV-BERT-base和PV-BERT-large）、用于主题增强的无监督主题建模（PV-Topic-BERT），以及针对Code、Subcode和Combo级别标签的微调分类器。在微调和推理过程中融入主题表示以丰富语义输入。

Result: PVminer在分层任务中表现优异，优于生物医学和临床预训练基线，F1分数达到82.25%（Code）、80.14%（Subcode）和最高77.87%（Combo）。消融研究显示作者身份和基于主题的增强各自带来显著性能提升。

Conclusion: PVminer提供了一个有效的NLP框架，用于在医患通信中结构化患者声音，解决了现有方法的局限性。预训练模型、源代码和文档将公开发布，标注数据集可根据研究请求提供。

Abstract: Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.

</details>


### [21] [Multimodal Multi-Agent Empowered Legal Judgment Prediction](https://arxiv.org/abs/2601.12815)
*Zhaolu Kang,Junhao Gong,Qingxi Chen,Hao Zhang,Jiaxin Liu,Rong Fu,Zhiyuan Feng,Yuan Wang,Simon Fong,Kaiyue Zhou*

Main category: cs.CL

TL;DR: 提出JurisMMA框架用于法律判决预测，通过分解审判任务、标准化流程并组织为不同阶段，同时构建包含10万+中国司法记录的JurisMM多模态数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 传统法律判决预测方法依赖统计分析或基于角色的模拟，在处理多重指控、多样证据和缺乏适应性方面面临挑战，需要更有效的框架来推进法律系统发展。

Method: 提出JurisMMA框架，有效分解审判任务、标准化流程并组织为不同阶段；构建JurisMM大型数据集，包含10万+近期中国司法记录，涵盖文本和多模态视频-文本数据。

Result: 在JurisMM数据集和基准LawBench上的实验验证了框架的有效性，结果表明该框架不仅适用于法律判决预测，还能为更广泛的法律应用提供支持。

Conclusion: JurisMMA框架为法律判决预测提供了新方法，同时构建的JurisMM数据集为全面评估提供了基础，为未来法律方法和数据集的发展提供了新视角。

Abstract: Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [When Backdoors Go Beyond Triggers: Semantic Drift in Diffusion Models Under Encoder Attacks](https://arxiv.org/abs/2602.20193)
*Shenyang Chen,Liuwan Zhu*

Main category: cs.CR

TL;DR: 论文挑战了文本到图像模型后门攻击的传统评估范式，揭示了编码器侧投毒会导致持久、无触发器的语义损坏，从根本上重塑表示流形。


<details>
  <summary>Details</summary>
Motivation: 传统后门攻击评估主要关注触发器激活和视觉保真度，但忽视了编码器侧投毒可能导致的更深层次结构风险。本文旨在揭示这种攻击如何通过几何机制从根本上改变表示空间。

Method: 采用基于雅可比矩阵的几何分析，揭示后门作为低秩、目标中心变形的机制；引入SEMAD（语义对齐和漂移）诊断框架，量化内部嵌入漂移和下游功能错位；在扩散和对比范式上进行验证。

Result: 发现后门攻击作为低秩、目标中心变形，放大局部敏感性，导致失真在语义邻域内相干传播；SEMAD框架成功量化了结构退化；验证了编码器投毒的深层结构风险。

Conclusion: 编码器侧投毒会导致持久、无触发器的语义损坏，从根本上重塑表示流形；传统基于攻击成功率的评估不足，需要引入几何审计来全面评估后门攻击的结构风险。

Abstract: Standard evaluations of backdoor attacks on text-to-image (T2I) models primarily measure trigger activation and visual fidelity. We challenge this paradigm, demonstrating that encoder-side poisoning induces persistent, trigger-free semantic corruption that fundamentally reshapes the representation manifold. We trace this vulnerability to a geometric mechanism: a Jacobian-based analysis reveals that backdoors act as low-rank, target-centered deformations that amplify local sensitivity, causing distortion to propagate coherently across semantic neighborhoods. To rigorously quantify this structural degradation, we introduce SEMAD (Semantic Alignment and Drift), a diagnostic framework that measures both internal embedding drift and downstream functional misalignment. Our findings, validated across diffusion and contrastive paradigms, expose the deep structural risks of encoder poisoning and highlight the necessity of geometric audits beyond simple attack success rates.

</details>


### [23] [Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study](https://arxiv.org/abs/2602.20202)
*Jeel Piyushkumar Khatiwala,Daniel Kwaku Ntiamoah Addai,Weifeng Xu*

Main category: cs.CR

TL;DR: 该论文提出了一个结构化框架，通过自动化取证工件提取、LLM驱动分析优化和数字取证知识图谱验证，解决AI识别数字证据的可靠性问题，在13GB数据集上实现超过95%的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地集成到取证调查中，AI识别的数字证据的可靠性引发了重大担忧。需要解决确保AI识别证据可信度和取证完整性的挑战，减少分类错误，并推进可扩展、可审计的方法论。

Method: 提出了一个结构化框架，包含三个核心组件：1) 自动化取证工件提取；2) 通过LLM驱动分析优化数据；3) 使用数字取证知识图谱验证结果。框架通过确定性唯一标识符和取证交叉引用确保工件可追溯性和证据一致性。

Result: 在包含61个应用程序、2,864个数据库和5,870个表的13GB取证镜像数据集上评估，框架实现了超过95%的工件提取准确率，强有力地支持了监管链遵守，并在取证关系中保持了稳健的上下文一致性。

Conclusion: 该框架能够增强AI辅助数字取证的可靠性，减少错误，并建立法律上健全的范式。关键结果验证了框架在提高可靠性、减少错误方面的能力，为AI辅助数字取证提供了可扩展、可审计的解决方案。

Abstract: The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, refines data through LLM-driven analysis, and validates results using a Digital Forensic Knowledge Graph (DFKG). Evaluated on a 13 GB forensic image dataset containing 61 applications, 2,864 databases, and 5,870 tables, the framework ensures artifact traceability and evidentiary consistency through deterministic Unique Identifiers (UIDs) and forensic cross-referencing. We propose this methodology to address challenges in ensuring the credibility and forensic integrity of AI-identified evidence, reducing classification errors, and advancing scalable, auditable methodologies. A comprehensive case study on this dataset demonstrates the framework's effectiveness, achieving over 95 percent accuracy in artifact extraction, strong support of chain-of-custody adherence, and robust contextual consistency in forensic relationships. Key results validate the framework's ability to enhance reliability, reduce errors, and establish a legally sound paradigm for AI-assisted digital forensics.

</details>


### [24] [Post-Quantum Sanitizable Signatures from McEliece-Based Chameleon Hashing](https://arxiv.org/abs/2602.20657)
*Shahzad Ahmad,Stefan Rass,Zahra Seyedi*

Main category: cs.CR

TL;DR: 提出首个基于编码的透明后量子可净化签名方案，利用McEliece密码系统构建变色龙哈希函数，通过Goppa码的Patterson解码实现可控碰撞，实现完美透明性。


<details>
  <summary>Details</summary>
Motivation: 设计后量子安全的可净化签名方案，解决传统基于数论方案易受量子攻击的问题，为长期安全应用提供理论保证和实践路径。

Method: 基于McEliece密码系统构建变色龙哈希函数，指定净化者拥有Goppa码的陷门，通过Patterson解码实现可控碰撞查找，通过特定权重约束实现完美透明性。

Result: 建立了首个透明的、基于编码的后量子可净化签名方案，在随机预言机模型下证明了存在不可伪造性和不可变性，基于伴随解码的困难性假设。

Conclusion: 该工作为后量子可净化签名提供了首个基于编码的透明方案，具有强理论保证，为长期安全应用的实际部署开辟了道路。

Abstract: We introduce a novel post-quantum sanitizable signature scheme constructed upon a chameleon hash function derived from the McEliece cryptosystem. In this design, the designated sanitizer possesses the inherent trapdoor of a Goppa code, which facilitates controlled collision-finding via Patterson decoding. This mechanism enables authorized modification of specific message blocks while ensuring all other content remains immutably bound. We provide formal security definitions and rigorous proofs of existential unforgeability and immutability, grounded in the hardness of syndrome decoding in the random-oracle model, where a robust random oracle thwarts trivial linear hash collisions. A key innovation lies in our precise characterization of the transparency property: by imposing a specific weight constraint on the randomizers generated by the signer, we achieve perfect transparency, rendering sanitized signatures indistinguishable from freshly signed ones. This work establishes the first transparent, code-based, post-quantum sanitizable signature scheme, offering strong theoretical guarantees and a pathway for practical deployment in long-term secure applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [Diffusion Modulation via Environment Mechanism Modeling for Planning](https://arxiv.org/abs/2602.20422)
*Hanping Zhang,Yuhong Guo*

Main category: cs.AI

TL;DR: DMEMM：一种通过环境机制建模调制扩散模型的离线强化学习规划方法，解决了传统扩散规划中轨迹一致性不足的问题


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散模型的规划方法在离线强化学习中存在轨迹一致性不足的问题，未能充分考虑RL环境中状态转移的连贯性要求，导致生成轨迹与真实环境机制存在显著差异

Method: 提出DMEMM方法，通过建模关键RL环境机制（特别是转移动态和奖励函数）来调制扩散模型的训练过程，确保生成的轨迹在真实环境中具有一致性

Result: 实验结果表明DMEMM在离线强化学习规划任务中达到了最先进的性能水平

Conclusion: 通过将环境机制建模融入扩散模型训练，DMEMM有效解决了轨迹一致性问题，为基于扩散模型的离线RL规划提供了更可靠的解决方案

Abstract: Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.

</details>


### [26] [KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning](https://arxiv.org/abs/2602.20494)
*Haotian Si,Changhua Pei,Xiao He,Zeyan Li,Zhe Xie,Zexin Wang,Jiyao Hu,Zhaoyang Yu,Tieying Zhang,Dan Pei,Jianhui Li,Gaogang Xie*

Main category: cs.AI

TL;DR: 提出语义条件时间序列推理任务和两轮强化学习框架KairosVL，将语义理解与传统时序分析结合，提升复杂决策能力


<details>
  <summary>Details</summary>
Motivation: 针对时间序列分析日益复杂和决策导向的需求，传统纯数值建模方法已无法满足需要，需要结合上下文和语义理解来增强推理能力

Method: 提出两轮强化学习框架：第一轮强化模型对基本时间原语的感知能力，第二轮专注于语义条件推理；最终构建KairosVL模型

Result: KairosVL在合成和真实世界任务中均取得有竞争力的性能，实验表明该框架不仅提升性能，还保持内在推理能力并显著提高对未见场景的泛化能力

Conclusion: 该工作展示了将语义推理与时间建模结合的潜力，为现实世界时间序列智能提供了实用框架，满足了当前迫切需求

Abstract: Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.

</details>


### [27] [Identifying two piecewise linear additive value functions from anonymous preference information](https://arxiv.org/abs/2602.20638)
*Vincent Auriau,Khaled Belahcene,Emmanuel Malherbe,Vincent Mousseau,Marc Pirlot*

Main category: cs.AI

TL;DR: 提出一种同时向两位决策者提问以引出其偏好模型的方法，在不知道哪个回答对应哪个决策者的情况下，通过假设加性价值函数和分段线性边际价值函数来识别两个偏好模型


<details>
  <summary>Details</summary>
Motivation: 在同时向两位决策者提问时，虽然能获得两个回答，但不知道哪个回答对应哪个决策者。需要开发一种方法在这种信息缺失的情况下仍能准确引出两位决策者的偏好模型

Method: 假设偏好可以用加性价值函数表示，边际价值函数为分段线性且断点已知。提出一种引出程序，通过同时向两位决策者提问并接收两个回答（无噪声），在不知道回答对应关系的情况下识别出两个偏好模型

Result: 该方法能够在不知道回答对应关系的情况下成功识别出两位决策者的偏好模型，前提是边际价值函数为分段线性且断点已知

Conclusion: 提出的引出程序为解决同时引出多位决策者偏好模型的问题提供了一种有效方法，特别是在回答对应关系未知的情况下，为群体决策分析提供了新的技术手段

Abstract: Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.

</details>


### [28] [How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective](https://arxiv.org/abs/2602.20687)
*Bo Peng,Pi Bu,Keyu Pan,Xinrun Xu,Yinxiu Zhao,Miao Chen,Yang Du,Lin Li,Jun Song,Tong Xu*

Main category: cs.AI

TL;DR: NativeEmbodied是一个用于视觉语言模型驱动的具身智能体的基准测试，采用统一的原生低级动作空间，包含高级任务和低级技能解耦评估。


<details>
  <summary>Details</summary>
Motivation: 现有VLM驱动的具身智能体基准测试存在两个主要问题：1) 使用高级命令或离散化动作空间，这与真实世界控制场景不同；2) 主要关注高级任务，缺乏对低级和高级能力的联合评估分析。

Method: 构建NativeEmbodied基准测试，基于多样化模拟场景，包含三个代表性高级任务评估整体性能。同时解耦复杂任务所需技能，构建四种类型的低级任务，每种针对一个基础具身技能。通过任务和技能粒度的联合评估实现细粒度评估。

Result: 实验表明，当前最先进的视觉语言模型在多个基础具身技能上存在明显缺陷。进一步分析显示，这些瓶颈显著限制了高级任务的性能表现。

Conclusion: NativeEmbodied揭示了当前VLM驱动具身智能体的关键挑战，为未来研究提供了指导性见解，强调了在统一原生动作空间中进行多粒度评估的重要性。

Abstract: Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.

</details>


### [29] [PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding](https://arxiv.org/abs/2602.20696)
*Baolong Bi,Yuyao Ge,Shenghua Liu,Yuchen He,Siqian Tong,Lizhe Chen,Lingrui Mei,Zehao Li,Yiwei Wang,Yujun Cai,Ming-Hsuan Yang,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出PromptCD方法，通过构建正负引导提示对比模型响应，在测试时无需额外训练即可提升LLMs和VLMs的行为对齐表现。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法多在训练时进行，依赖高质量数据且计算成本高。对比解码虽能利用模型内部分布提升特定能力，但适用范围有限。需要一种更通用、成本效益高的测试时行为控制方法。

Method: 提出Polarity-Prompt Contrastive Decoding (PromptCD)：为目标行为构建正负引导提示对，对比模型响应（LLMs中的词级概率分布和VLMs中的视觉注意力模式），强化期望结果。该方法适用于多种增强目标，无需额外训练。

Result: LLMs在"3H"对齐目标（帮助性、诚实性、无害性）上获得一致显著改进，表明后训练模型可在测试时实现有意义的自我增强。VLMs方面，PromptCD通过强化行为一致的视觉基础显著提升VQA性能。

Conclusion: PromptCD是一种简单、通用、成本效益高的可靠行为控制策略，适用于多模态场景，为AI系统对齐提供了有效的测试时解决方案。

Abstract: Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the "3H" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.

</details>


### [30] [Counterfactual Simulation Training for Chain-of-Thought Faithfulness](https://arxiv.org/abs/2602.20710)
*Peter Hase,Christopher Potts*

Main category: cs.AI

TL;DR: 提出Counterfactual Simulation Training (CST)训练方法，通过奖励能够准确预测模型在反事实输入下输出的思维链，来提高LLM思维链的忠实性。


<details>
  <summary>Details</summary>
Motivation: 当前思维链推理存在忠实性问题，限制了通过分析思维链来理解模型输出原因的有效性。需要一种方法来提高思维链的忠实性，以便更可靠地理解模型决策过程。

Method: 提出CST训练方法，通过两种设置实现：1) 基于线索的反事实进行思维链监控，检测模型是否依赖虚假特征、奖励黑客行为或奉承；2) 基于通用模型的反事实模拟，鼓励模型在思维链中产生更忠实、可泛化的推理。

Result: 在参数高达235B的模型上实验显示：CST显著提高基于线索反事实的监控准确率（提升35个百分点）和通用反事实的可模拟性（提升2个百分点）；CST优于提示基准；用LLM重写不忠实思维链比单独使用RL效率高5倍；忠实性改进不会泛化到劝阻线索；更大模型不会天生产生更忠实思维链，但能从CST中获益更多。

Conclusion: CST能够普遍提高思维链的忠实性，在思维链监控方面具有有前景的应用价值。实验代码已开源。

Abstract: Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training

</details>


### [31] [Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning](https://arxiv.org/abs/2602.20722)
*Xu Wan,Yansheng Wang,Wenqi Huang,Mingyang Sun*

Main category: cs.AI

TL;DR: BAPO是一种离策略的RLVR框架，通过动态选择训练批次、重新评估历史困难样本和重用高质量样本来提高大语言模型后训练的数据效率，同时在数学、规划和视觉推理任务上平均比GRPO提升12.5%。


<details>
  <summary>Details</summary>
Motivation: 传统基于策略的RLVR框架存在经验浪费和奖励同质性问题，这直接阻碍了在大语言模型后训练中对困难样本的学习效率。

Method: 提出Batch Adaptation Policy Optimization (BAPO)，这是一种离策略的RLVR框架，通过动态选择训练批次，重新评估历史困难样本并重用高质量样本，同时保持策略改进的下界保证。

Result: BAPO在数学、规划和视觉推理任务上平均比GRPO提升12.5%，成功解决了基础模型持续无法解决的40.7%的问题。

Conclusion: BAPO通过改进数据效率和样本选择机制，有效解决了传统RLVR框架在困难样本学习上的局限性，显著提升了大语言模型后训练的性能。

Abstract: Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.

</details>


### [32] [Balancing Multiple Objectives in Urban Traffic Control with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2602.20728)
*Chenyang Zhao,Vinny Cahill,Ivana Dusparic*

Main category: cs.AI

TL;DR: 该论文将基于AI反馈的强化学习（RLAIF）扩展到多目标自适应系统，通过LLM生成偏好标签来平衡冲突目标，避免繁琐的奖励工程。


<details>
  <summary>Details</summary>
Motivation: 真实世界强化学习部署中，多目标系统的奖励设计是核心挑战。现有RLAIF工作主要关注单目标任务，而多目标系统中冲突目标间的权衡难以明确指定，策略容易偏向主导目标。需要探索RLAIF在多目标自适应系统中的应用。

Method: 扩展RLAIF范式到多目标自适应系统，利用大型语言模型（LLMs）生成偏好标签来替代人工标注。通过LLM评估行为结果对之间的偏好，学习反映不同用户优先级的平衡策略，避免复杂的奖励工程。

Result: 多目标RLAIF能够生成平衡不同用户优先级的策略，在冲突目标间实现有效权衡。该方法展示了在多目标领域实现用户对齐策略学习的可扩展路径。

Conclusion: 将RLAIF集成到多目标强化学习中，为具有固有冲突目标的领域提供了一条可扩展的用户对齐策略学习路径，能够产生反映不同用户优先级的平衡权衡，而无需繁琐的奖励工程。

Abstract: Reward design has been one of the central challenges for real world reinforcement learning (RL) deployment, especially in settings with multiple objectives. Preference-based RL offers an appealing alternative by learning from human preferences over pairs of behavioural outcomes. More recently, RL from AI feedback (RLAIF) has demonstrated that large language models (LLMs) can generate preference labels at scale, mitigating the reliance on human annotators. However, existing RLAIF work typically focuses only on single-objective tasks, leaving the open question of how RLAIF handles systems that involve multiple objectives. In such systems trade-offs among conflicting objectives are difficult to specify, and policies risk collapsing into optimizing for a dominant goal. In this paper, we explore the extension of the RLAIF paradigm to multi-objective self-adaptive systems. We show that multi-objective RLAIF can produce policies that yield balanced trade-offs reflecting different user priorities without laborious reward engineering. We argue that integrating RLAIF into multi-objective RL offers a scalable path toward user-aligned policy learning in domains with inherently conflicting objectives.

</details>


### [33] [CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference](https://arxiv.org/abs/2602.20732)
*Chao Fei,Guozhong Li,Chenxi Liu,Panos Kalnis*

Main category: cs.AI

TL;DR: CHESS是一个算法-系统协同设计的KV缓存管理系统，通过上下文感知的分层选择策略动态重建解码上下文，仅使用1%的KV缓存即可超越完整KV的质量，实现高达4.56倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM需要低延迟的准确推理，但随着上下文增长，解码主要受KV缓存约束。现有剪枝方法大多是上下文无关的：其令牌选择忽略了逐步相关性和局部语义，这影响了质量。此外，它们的不规则访问和选择开销仅带来有限的实时加速。

Method: CHESS采用算法-系统协同设计。算法层面：引入上下文感知的分层选择策略，动态重建当前解码的连贯上下文。系统层面：粗粒度选择消除了昂贵的数据移动，充分实现理论稀疏性的实际加速。

Result: 广泛评估表明，CHESS仅使用1%的KV缓存即可超越完整KV的质量，提供低延迟稳定推理，吞吐量提升高达4.56倍，并持续优于其他强基线。

Conclusion: CHESS通过算法-系统协同设计有效解决了长上下文LLM中KV缓存管理的挑战，在保持质量的同时显著提升了推理效率。

Abstract: Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \textbf{CHESS}, an \textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \textbf{1\%} of the KV cache, delivers low-latency stable inference with up to \textbf{4.56$\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.

</details>


### [34] [PyVision-RL: Forging Open Agentic Vision Models via RL](https://arxiv.org/abs/2602.20739)
*Shitian Zhao,Shaoheng Lin,Ming Li,Haoquan Zhang,Wenshuo Peng,Kaipeng Zhang,Chen Wei*

Main category: cs.AI

TL;DR: PyVision-RL框架通过过采样-过滤-排序策略和累积工具奖励防止交互崩溃，维持多模态模型的多轮工具使用，显著提升视频理解效率


<details>
  <summary>Details</summary>
Motivation: 解决多模态智能体强化学习中的交互崩溃问题，即模型倾向于减少工具使用和多轮推理，限制了智能体行为的优势

Method: 提出PyVision-RL框架，结合过采样-过滤-排序的rollout策略和累积工具奖励；采用统一训练流程开发PyVision-Image和PyVision-Video；视频推理中采用按需上下文构建，选择性采样任务相关帧以减少视觉token使用

Result: 实验显示PyVision-RL在保持强性能的同时显著提升效率，证明持续交互和按需视觉处理对可扩展多模态智能体的重要性

Conclusion: 持续交互和按需视觉处理是构建可扩展多模态智能体的关键要素，PyVision-RL框架通过防止交互崩溃和优化视觉处理实现了这一目标

Abstract: Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.

</details>


### [35] [POMDPPlanners: Open-Source Package for POMDP Planning](https://arxiv.org/abs/2602.20810)
*Yaacov Pariente,Vadim Indelman*

Main category: cs.AI

TL;DR: 开源Python包POMDPPlanners用于POMDP规划算法的实证评估，集成了先进算法、基准环境、自动超参数优化、持久缓存和并行仿真功能。


<details>
  <summary>Details</summary>
Motivation: 现有的POMDP规划工具在风险敏感设置和可扩展实证研究方面存在不足，需要统一的框架来支持不确定性决策的可复现研究。

Method: 开发开源Python包，集成先进规划算法、基准环境（含安全关键变体）、Optuna自动超参数优化、持久缓存与故障恢复、可配置并行仿真。

Result: 创建了POMDPPlanners工具包，显著减少了大规模仿真研究的开销，支持不确定性决策的可扩展、可复现研究。

Conclusion: POMDPPlanners填补了现有工具在风险敏感设置下的空白，为不确定性决策研究提供了高效、可复现的实证评估框架。

Abstract: We present POMDPPlanners, an open-source Python package for empirical evaluation of Partially Observable Markov Decision Process (POMDP) planning algorithms. The package integrates state-of-the-art planning algorithms, a suite of benchmark environments with safety-critical variants, automated hyperparameter optimization via Optuna, persistent caching with failure recovery, and configurable parallel simulation -- reducing the overhead of extensive simulation studies. POMDPPlanners is designed to enable scalable, reproducible research on decision-making under uncertainty, with particular emphasis on risk-sensitive settings where standard toolkits fall short.

</details>


### [36] [Qwen-BIM: developing large language model for BIM-based design with domain-specific benchmark and dataset](https://arxiv.org/abs/2602.20812)
*Jia-Rui Lin,Yun-Hong Cai,Xiang-Rui Ni,Shaojie Zhou,Peng Pan*

Main category: cs.AI

TL;DR: 该论文针对BIM设计领域缺乏LLM评估基准和数据集的问题，提出了首个BIM设计专用LLM评估基准、BIM文本数据生成方法及微调策略，开发了Qwen-BIM模型，在仅14B参数下性能媲美671B通用LLM。


<details>
  <summary>Details</summary>
Motivation: 建筑行业数字化转型背景下，BIM设计成为智能建造关键驱动力。尽管大型语言模型在促进BIM设计方面显示潜力，但缺乏特定数据集和评估基准严重制约了LLM性能表现。

Method: 1) 提出BIM设计评估基准及量化指标；2) 开发从BIM生成文本数据的方法，构建BIM衍生数据集用于LLM评估和微调；3) 设计适应BIM设计的LLM微调策略。

Result: 提出的领域专用基准能有效全面评估LLM能力，显示通用LLM在领域任务中仍不称职。基于该基准和数据集开发的Qwen-BIM相比基础LLM平均G-Eval分数提升21.0%，仅14B参数性能即可媲美671B通用LLM在BIM设计任务中的表现。

Conclusion: 本研究通过引入全面基准和高质量数据集，开发了首个BIM设计专用LLM，为各领域开发BIM相关LLM奠定了坚实基础。

Abstract: As the construction industry advances toward digital transformation, BIM (Building Information Modeling)-based design has become a key driver supporting intelligent construction. Despite Large Language Models (LLMs) have shown potential in promoting BIM-based design, the lack of specific datasets and LLM evaluation benchmarks has significantly hindered the performance of LLMs. Therefore, this paper addresses this gap by proposing: 1) an evaluation benchmark for BIM-based design together with corresponding quantitative indicators to evaluate the performance of LLMs, 2) a method for generating textual data from BIM and constructing corresponding BIM-derived datasets for LLM evaluation and fine-tuning, and 3) a fine-tuning strategy to adapt LLMs for BIM-based design. Results demonstrate that the proposed domain-specific benchmark effectively and comprehensively assesses LLM capabilities, highlighting that general LLMs are still incompetent for domain-specific tasks. Meanwhile, with the proposed benchmark and datasets, Qwen-BIM is developed and achieves a 21.0% average increase in G-Eval score compared to the base LLM model. Notably, with only 14B parameters, performance of Qwen-BIM is comparable to that of general LLMs with 671B parameters for BIM-based design tasks. Overall, this study develops the first domain-specific LLM for BIM-based design by introducing a comprehensive benchmark and high-quality dataset, which provide a solid foundation for developing BIM-related LLMs in various fields.

</details>


### [37] [Pressure Reveals Character: Behavioural Alignment Evaluation at Depth](https://arxiv.org/abs/2602.20813)
*Nora Petrova,John Burden*

Main category: cs.AI

TL;DR: 该论文提出了一个包含904个场景的全面对齐基准测试，涵盖六个类别，通过多轮交互压力测试语言模型在实际压力下的行为表现，发现对齐表现呈现类似g因子的统一结构特征。


<details>
  <summary>Details</summary>
Motivation: 现有对齐评估主要关注模型声称会做什么，而非在实际压力下的真实行为。随着对齐失败导致的实际危害增加，缺乏包含现实多轮场景的全面评估框架。

Method: 1. 构建包含904个场景的基准测试，涵盖诚实性、安全性、非操纵性、鲁棒性、可纠正性和阴谋性六个类别；2. 通过人类评估验证场景的现实性；3. 使用冲突指令、模拟工具访问和多轮升级等压力测试方法；4. 评估24个前沿模型，使用经过人类标注验证的LLM作为评判者。

Result: 1. 即使表现最佳的模型在特定类别中也存在差距；2. 大多数模型在所有类别中表现出一致的弱点；3. 因子分析显示对齐表现呈现统一结构（类似认知研究中的g因子），在一个类别得分高的模型在其他类别也倾向于得分高。

Conclusion: 该研究提出了一个现实的多轮对齐评估基准，揭示了语言模型对齐表现的统一结构特征。研究公开了基准测试和交互式排行榜，计划在观察到持续弱点的领域扩展场景，并随着新模型发布而更新。

Abstract: Evaluating alignment in language models requires testing how they behave under realistic pressure, not just what they claim they would do. While alignment failures increasingly cause real-world harm, comprehensive evaluation frameworks with realistic multi-turn scenarios remain lacking. We introduce an alignment benchmark spanning 904 scenarios across six categories -- Honesty, Safety, Non-Manipulation, Robustness, Corrigibility, and Scheming -- validated as realistic by human raters. Our scenarios place models under conflicting instructions, simulated tool access, and multi-turn escalation to reveal behavioural tendencies that single-turn evaluations miss. Evaluating 24 frontier models using LLM judges validated against human annotations, we find that even top-performing models exhibit gaps in specific categories, while the majority of models show consistent weaknesses across the board. Factor analysis reveals that alignment behaves as a unified construct (analogous to the g-factor in cognitive research) with models scoring high on one category tending to score high on others. We publicly release the benchmark and an interactive leaderboard to support ongoing evaluation, with plans to expand scenarios in areas where we observe persistent weaknesses and to add new models as they are released.

</details>


### [38] [Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs](https://arxiv.org/abs/2602.20878)
*Dhita Putri Pratama,Soyeon Caren Han,Yihao Ding*

Main category: cs.AI

TL;DR: 该论文提出了Vision-Language Causal Graphs (VLCGs)结构化表示和ViLCaR诊断基准，用于评估大视觉语言模型的因果推理能力，发现当前模型主要缺乏结构化指导而非推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在视觉问答任务中表现良好，但往往依赖虚假相关性而非真正的因果推理。现有评估主要关注答案正确性，难以区分失败是由于推理能力有限还是未能识别因果相关信息。

Method: 提出Vision-Language Causal Graphs (VLCGs)结构化表示，明确编码因果相关的对象、属性、关系和场景基础假设。基于此构建ViLCaR诊断基准，包含因果归因、因果推理和问答任务，以及图对齐的评估指标。

Result: 实验表明，注入结构化相关信息显著提高了归因和推理一致性，优于零样本和标准上下文学习。当前LVLM在因果推理中的限制主要源于结构化指导不足而非推理能力缺乏。

Conclusion: 通过结构化因果表示和诊断评估，揭示了当前大视觉语言模型因果推理的主要瓶颈在于缺乏结构化指导，而非推理能力本身不足，为改进模型提供了方向。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.

</details>


### [39] [HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG](https://arxiv.org/abs/2602.20926)
*Yuqi Huang,Ning Liao,Kai Yang,Anning Hu,Shengchao Hu,Xiaoxing Wang,Junchi Yan*

Main category: cs.AI

TL;DR: HELP框架通过超节点扩展和逻辑路径引导的证据定位策略，在保持检索精度的同时大幅提升图增强检索生成系统的效率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在知识边界和幻觉问题，传统检索增强生成方法忽略了多跳推理所需的结构依赖性，而基于图的RAG方法在准确性和效率之间存在权衡，需要昂贵图遍历且易受LLM生成摘要的语义噪声影响

Method: 提出HELP框架，包含两个核心策略：1) 超节点扩展：迭代地将知识三元组链式连接成连贯推理路径，抽象为超节点以捕获复杂结构依赖；2) 逻辑路径引导的证据定位：利用预计算的图-文本相关性将这些路径直接映射到语料库，避免昂贵随机游走和语义失真

Result: 在多个简单和多跳问答基准测试中取得有竞争力的性能，相比领先的基于图的RAG基线方法实现高达28.8倍的加速

Conclusion: HELP框架通过平衡准确性和实际效率，有效解决了图增强检索生成中的结构依赖捕获和效率瓶颈问题，为知识密集型任务提供了可靠且高效的解决方案

Abstract: Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\times$ speedup over leading Graph-based RAG baselines.

</details>


### [40] [Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence](https://arxiv.org/abs/2602.20934)
*ChengYou Li,XiaoDong Liu,XiangBao Meng,XinYu Zhao*

Main category: cs.AI

TL;DR: 论文提出AgentOS框架，将LLM重新定义为受结构化操作系统逻辑管理的"推理内核"，通过深度上下文管理将上下文窗口概念化为可寻址语义空间，为构建弹性、可扩展、自演化的认知环境提供系统级架构路线图。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究主要关注扩展上下文窗口或优化提示工程，但微观层面的token处理与宏观层面的系统智能之间的理论桥梁仍然零散。需要从静态推理引擎向动态自主认知系统进行根本性转变，解决多智能体编排中的认知漂移问题。

Method: 提出AgentOS整体概念框架，将LLM重新定义为"推理内核"，采用深度上下文管理将上下文窗口概念化为可寻址语义空间而非被动缓冲区。引入语义切片和时间对齐机制来缓解认知漂移，并将经典操作系统抽象（如内存分页、中断处理和进程调度）映射到LLM原生构造上。

Result: 该框架为构建弹性、可扩展、自演化的认知环境提供了严格的路线图，系统化解构了从离散序列到连贯认知状态的转变过程，为多智能体编排提供了理论支撑。

Conclusion: AGI发展的下一个前沿在于系统级协调的架构效率，AgentOS框架通过将操作系统原理应用于LLM架构，为实现从静态推理引擎到动态自主认知系统的范式转变提供了理论基础。

Abstract: The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a "Reasoning Kernel" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.

</details>


### [41] [LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification](https://arxiv.org/abs/2602.21044)
*Yanrui Wu,Lingling Zhang,Xinyu Zhang,Jiayu Chang,Pengyu Li,Xu Jiang,Jingtao Hu,Jun Liu*

Main category: cs.AI

TL;DR: LogicGraph：首个评估大语言模型多路径逻辑推理能力的基准，通过神经符号框架构建包含深度多路径推理和逻辑干扰的问题，揭示模型倾向于过早固定单一推理路径而无法探索替代方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估主要关注收敛性逻辑推理（产生单一正确证明），但许多现实世界推理问题存在多个有效推导路径，需要模型探索多样化的逻辑路径而非固守单一路线。当前缺乏系统评估多路径逻辑推理能力的基准。

Method: 提出LogicGraph基准，采用神经符号框架构建，通过后向逻辑生成和语义实例化流程产生求解器验证的推理问题。这些问题具有高深度多路径推理和固有逻辑干扰特性，每个实例都关联一组详尽的最小证明集合。同时提出无参考评估框架来严格评估模型在收敛和发散两种机制下的性能。

Result: 对最先进语言模型的实验揭示了一个共同局限性：模型倾向于过早固定单一推理路径而无法探索替代方案，且随着推理深度的增加，覆盖差距显著扩大。LogicGraph暴露了这种发散差距，并为未来改进提供了可操作的见解。

Conclusion: LogicGraph是首个系统评估多路径逻辑推理能力的基准，揭示了当前大语言模型在发散推理方面的局限性。该基准为未来模型改进提供了重要方向，代码和数据将开源发布。

Abstract: Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.

</details>


### [42] [Tool Building as a Path to "Superintelligence"](https://arxiv.org/abs/2602.21061)
*David Koplow,Tomer Galanti,Tomaso Poggio*

Main category: cs.AI

TL;DR: 论文提出基准测试测量LLMs在逻辑分布外推理中的步成功概率γ，发现前沿模型在GF(2)电路重构任务中表现部分鲁棒性，工具调用精度是实现超级智能的关键。


<details>
  <summary>Details</summary>
Motivation: 研究Diligent Learner框架下LLMs通过测试时搜索实现超级智能的可能性，需要测量步成功概率γ，特别是在逻辑分布外推理任务中评估模型的信息整合能力。

Method: 设计基准测试测量γ值，构建GF(2)电路重构任务类别，这些任务随着推理步骤增加而变难，从信息论角度看需要仔细整合所有信息才能可靠解决。

Result: 小型LLMs的γ值随深度增加超线性下降，而前沿模型在该任务上表现出部分鲁棒性；成功推理依赖于精确的工具调用，工具设计是实现超级智能的关键能力。

Conclusion: LLMs通过Diligent Learner框架实现通用超级智能需要精确的工具调用能力，工具设计是核心瓶颈，前沿模型在复杂逻辑推理任务中已展现部分鲁棒性。

Abstract: The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.

</details>


### [43] [CG-DMER: Hybrid Contrastive-Generative Framework for Disentangled Multimodal ECG Representation Learning](https://arxiv.org/abs/2602.21154)
*Ziwei Niu,Hao Sun,Shujun Bian,Xihong Yang,Lanfen Lin,Yuxin Liu,Yueming Jin*

Main category: cs.AI

TL;DR: CG-DMER是一个对比生成框架，通过空间-时间掩码建模和表示解耦对齐策略，解决ECG信号与临床报告多模态融合中的空间-时间依赖性和模态特定偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态ECG分析方法存在两个主要问题：1）在单模态层面，现有模型以导联无关的方式处理ECG，忽略了导联间的空间-时间依赖性，限制了细粒度诊断模式的建模能力；2）在多模态层面，现有方法直接将ECG信号与临床报告对齐，由于报告的自由文本特性引入了模态特定偏差。

Method: 提出CG-DMER框架，包含两个关键设计：1）空间-时间掩码建模，通过在空间和时间维度上应用掩码并重建缺失信息，更好地捕捉细粒度时间动态和导联间空间依赖性；2）表示解耦和对齐策略，引入模态特定和模态共享编码器，确保模态不变和模态特定表示之间的清晰分离，减少不必要的噪声和模态特定偏差。

Result: 在三个公共数据集上的实验表明，CG-DMER在多种下游任务中实现了最先进的性能。

Conclusion: CG-DMER通过创新的空间-时间建模和表示解耦技术，有效解决了ECG多模态分析中的关键挑战，为心血管疾病诊断提供了更准确的多模态表示学习方法。

Abstract: Accurate interpretation of electrocardiogram (ECG) signals is crucial for diagnosing cardiovascular diseases. Recent multimodal approaches that integrate ECGs with accompanying clinical reports show strong potential, but they still face two main concerns from a modality perspective: (1) intra-modality: existing models process ECGs in a lead-agnostic manner, overlooking spatial-temporal dependencies across leads, which restricts their effectiveness in modeling fine-grained diagnostic patterns; (2) inter-modality: existing methods directly align ECG signals with clinical reports, introducing modality-specific biases due to the free-text nature of the reports. In light of these two issues, we propose CG-DMER, a contrastive-generative framework for disentangled multimodal ECG representation learning, powered by two key designs: (1) Spatial-temporal masked modeling is designed to better capture fine-grained temporal dynamics and inter-lead spatial dependencies by applying masking across both spatial and temporal dimensions and reconstructing the missing information. (2) A representation disentanglement and alignment strategy is designed to mitigate unnecessary noise and modality-specific biases by introducing modality-specific and modality-shared encoders, ensuring a clearer separation between modality-invariant and modality-specific representations. Experiments on three public datasets demonstrate that CG-DMER achieves state-of-the-art performance across diverse downstream tasks.

</details>


### [44] [NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning](https://arxiv.org/abs/2602.21172)
*Ishaan Rawal,Shubh Gupta,Yihan Hu,Wei Zhan*

Main category: cs.AI

TL;DR: NoRD模型通过消除推理标注需求，在仅使用<60%数据的情况下实现与现有VLA模型竞争的性能，训练token减少3倍


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action模型面临两大昂贵需求：大规模数据集收集和密集推理标注，这限制了自动驾驶系统的效率

Method: 提出NoRD模型，采用Dr. GRPO算法（专门设计用于缓解LLM中的难度偏差），克服标准GRPO在小规模无推理数据集上的局限性

Result: 在Waymo和NAVSIM基准测试中达到竞争性性能，仅需<60%的训练数据和零推理标注，训练token减少3倍

Conclusion: NoRD通过消除推理标注需求并大幅减少数据需求，为更高效的自动驾驶系统提供了可行路径

Abstract: Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $<$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.

</details>
