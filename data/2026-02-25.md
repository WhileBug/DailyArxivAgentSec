<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [DCInject: Persistent Backdoor Attacks via Frequency Manipulation in Personal Federated Learning](https://arxiv.org/abs/2602.18489)
*Nahom Birhan,Daniel Wesego,Dereje Shenkut,Frank Liu,Daniel Takabi*

Main category: cs.CR

TL;DR: æœ¬æ–‡æå‡ºDCInjectï¼Œä¸€ç§é’ˆå¯¹ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ çš„è‡ªé€‚åº”é¢‘åŸŸåé—¨æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡ç§»é™¤é›¶é¢‘åˆ†é‡å¹¶æ›¿æ¢ä¸ºé«˜æ–¯åˆ†å¸ƒæ ·æœ¬ï¼Œåœ¨ä¿æŒæ¸…æ´ç²¾åº¦çš„åŒæ—¶å®ç°é«˜æ”»å‡»æˆåŠŸç‡


<details>
  <summary>Details</summary>
Motivation: å°½ç®¡ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ ï¼ˆPFLï¼‰è¢«è®¤ä¸ºå¯¹åé—¨æ”»å‡»ä¼ æ’­å…·æœ‰å¤©ç„¶æŠµæŠ—åŠ›ï¼Œä½†æœ¬æ–‡æ­ç¤ºPFLä»ç„¶å­˜åœ¨å®‰å…¨æ¼æ´ï¼Œéœ€è¦å¼€å‘æ›´éšè”½æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•æ¥éªŒè¯å…¶å®‰å…¨æ€§å‡è®¾

Method: æå‡ºDCInjectæ”»å‡»æ–¹æ³•ï¼šåœ¨é¢‘åŸŸä¸­ç§»é™¤éƒ¨åˆ†é›¶é¢‘ï¼ˆDCï¼‰åˆ†é‡ï¼Œå¹¶ç”¨é«˜æ–¯åˆ†å¸ƒæ ·æœ¬æ›¿æ¢ï¼Œå½¢æˆè‡ªé€‚åº”é¢‘åŸŸåé—¨æ”»å‡»ï¼Œé€‚ç”¨äºå‚æ•°è§£è€¦çš„ä¸ªæ€§åŒ–è”é‚¦å­¦ä¹ åœºæ™¯

Result: åœ¨å››ä¸ªæ•°æ®é›†ï¼ˆCIFAR-10/100ã€GTSRBã€SVHNï¼‰ä¸Šå®ç°é«˜æ”»å‡»æˆåŠŸç‡ï¼šCIFAR-10 96.83%ã€SVHN 99.38%ã€GTSRB 100%ï¼ŒåŒæ—¶ä¿æŒæ¸…æ´ç²¾åº¦ï¼›åœ¨I-BAUé˜²å¾¡ä¸‹ä»ä¿æŒ90.30%æ”»å‡»æˆåŠŸç‡ï¼ˆBadNetä»…58.56%ï¼‰

Conclusion: DCInjectæ”»å‡»æ­ç¤ºäº†PFLå®‰å…¨å‡è®¾çš„å…³é”®æ¼æ´ï¼Œé¢‘åŸŸæ”»å‡»æ–¹æ³•æ¯”ä¼ ç»Ÿç©ºé—´åŸŸæ”»å‡»æ›´æœ‰æ•ˆä¸”å…·æœ‰æ›´å¼ºçš„é˜²å¾¡æŠµæŠ—èƒ½åŠ›ï¼Œå¯¹è”é‚¦å­¦ä¹ å®‰å…¨è¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰

Abstract: Personalized federated learning (PFL) creates client-specific models to handle data heterogeneity. Previously, PFL has been shown to be naturally resistant to backdoor attack propagation across clients. In this work, we reveal that PFL remains vulnerable to backdoor attacks through a novel frequency-domain approach. We propose DCInject, an adaptive frequency-domain backdoor attack for PFL, which removes portions of the zero-frequency (DC) component and replaces them with Gaussian-distributed samples in the frequency domain. Our attack achieves superior attack success rates while maintaining clean accuracy across four datasets (CIFAR-10/100, GTSRB, SVHN) compared to existing spatial-domain attacks, evaluated under parameter decoupling based personalization. DCInject achieves superior performance with ASRs of 96.83% (CIFAR-10), 99.38% (SVHN), and 100% (GTSRB) while maintaining clean accuracy. Under I-BAU defense, DCInject demonstrates strong persistence, retaining 90.30% ASR vs BadNet's 58.56% on VGG-16, exposing critical vulnerabilities in PFL security assumptions. Our code is available at https://github.com/NahomMA/DCINject-PFL

</details>


### [2] [Trojan Horses in Recruiting: A Red-Teaming Case Study on Indirect Prompt Injection in Standard vs. Reasoning Models](https://arxiv.org/abs/2602.18514)
*Manuel Wirth*

Main category: cs.CR

TL;DR: è¯¥ç ”ç©¶æŒ‘æˆ˜äº†"æ¨ç†æ¨¡å‹æ›´å®‰å…¨"çš„å‡è®¾ï¼Œé€šè¿‡çº¢é˜Ÿæµ‹è¯•å‘ç°æ¨ç†æ¨¡å‹åœ¨é—´æ¥æç¤ºæ³¨å…¥æ”»å‡»ä¸­è¡¨ç°å‡ºå±é™©çš„äºŒå…ƒæ€§ï¼šæ—¢èƒ½ç­–ç•¥æ€§é‡æ„ç®€å•æ”»å‡»ä½¿å…¶æ›´å…·è¯´æœåŠ›ï¼Œåˆä¼šåœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶å‘ç”Ÿ"å…ƒè®¤çŸ¥æ³„æ¼"ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€å¤§è¯­è¨€æ¨¡å‹åœ¨äººåŠ›èµ„æºç­‰è‡ªåŠ¨åŒ–å†³ç­–ç®¡é“ä¸­çš„é›†æˆï¼Œé—´æ¥æç¤ºæ³¨å…¥çš„å®‰å…¨å½±å“å˜å¾—è‡³å…³é‡è¦ã€‚è™½ç„¶æ™®éå‡è®¾"æ¨ç†"æˆ–"æ€ç»´é“¾"æ¨¡å‹å› å…¶è‡ªæˆ‘çº æ­£èƒ½åŠ›è€Œå…·æœ‰å®‰å…¨ä¼˜åŠ¿ï¼Œä½†æ–°å…´ç ”ç©¶è¡¨æ˜è¿™äº›èƒ½åŠ›å¯èƒ½å¯¼è‡´æ›´å¤æ‚çš„å¯¹é½å¤±è´¥ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡çº¢é˜Ÿæ¡ˆä¾‹ç ”ç©¶æŒ‘æˆ˜"é€šè¿‡æ¨ç†å®ç°å®‰å…¨"çš„å‰æã€‚

Method: é‡‡ç”¨å®šæ€§çº¢é˜Ÿæ¡ˆä¾‹ç ”ç©¶æ–¹æ³•ï¼Œä½¿ç”¨Qwen 3 30Bæ¶æ„ã€‚é€šè¿‡å‘æ ‡å‡†æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹å’Œæ¨ç†å¢å¼ºæ¨¡å‹åŒæ—¶æäº¤"ç‰¹æ´›ä¼Šæœ¨é©¬"ç®€å†ï¼Œè§‚å¯Ÿä¸åŒçš„å¤±è´¥æ¨¡å¼ã€‚ç ”ç©¶è®¾è®¡äº†ç®€å•å’Œå¤æ‚çš„æ”»å‡»åœºæ™¯æ¥æµ‹è¯•æ¨¡å‹çš„å®‰å…¨æ€§è¡¨ç°ã€‚

Result: ç ”ç©¶å‘ç°å¤æ‚çš„æƒè¡¡ï¼šæ ‡å‡†æ¨¡å‹åœ¨ç®€å•æ”»å‡»ä¸­é‡‡ç”¨è„†å¼±çš„å¹»è§‰æ¥åˆç†åŒ–æ”»å‡»ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­è¿‡æ»¤æ‰ä¸åˆé€»è¾‘çš„çº¦æŸï¼›è€Œæ¨ç†æ¨¡å‹è¡¨ç°å‡ºå±é™©çš„äºŒå…ƒæ€§ã€‚å¯¹äºç®€å•æ”»å‡»ï¼Œæ¨ç†æ¨¡å‹ä½¿ç”¨é«˜çº§ç­–ç•¥é‡æ„ä½¿å…¶æå…·è¯´æœåŠ›ï¼›å¯¹äºå¤æ‚é€»è¾‘æŒ‡ä»¤ï¼Œæ¨ç†æ¨¡å‹è¡¨ç°å‡º"å…ƒè®¤çŸ¥æ³„æ¼"â€”â€”å¤„ç†å¤æ‚å¯¹æŠ—æŒ‡ä»¤çš„è®¤çŸ¥è´Ÿè·å¯¼è‡´æ³¨å…¥é€»è¾‘æ— æ„ä¸­æ‰“å°åœ¨æœ€ç»ˆè¾“å‡ºä¸­ï¼Œä½¿æ”»å‡»æ¯”æ ‡å‡†æ¨¡å‹æ›´å®¹æ˜“è¢«äººç±»æ£€æµ‹åˆ°ã€‚

Conclusion: è¯¥ç ”ç©¶æŒ‘æˆ˜äº†æ¨ç†æ¨¡å‹æ›´å®‰å…¨çš„å‡è®¾ï¼Œæ­ç¤ºäº†æ¨ç†èƒ½åŠ›å¯èƒ½å¸¦æ¥çš„æ–°å®‰å…¨é£é™©ã€‚æ¨ç†æ¨¡å‹åœ¨å¤„ç†é—´æ¥æç¤ºæ³¨å…¥æ—¶è¡¨ç°å‡ºå¤æ‚çš„æƒè¡¡ï¼šæ—¢èƒ½ç­–ç•¥æ€§åœ°å¢å¼ºæ”»å‡»è¯´æœåŠ›ï¼Œåˆä¼šåœ¨è®¤çŸ¥è¿‡è½½æ—¶æ³„æ¼æ”»å‡»é€»è¾‘ã€‚è¿™è¡¨æ˜éœ€è¦é‡æ–°è¯„ä¼°æ¨ç†æ¨¡å‹çš„å®‰å…¨å‡è®¾ï¼Œå¹¶å¼€å‘æ›´å…¨é¢çš„å®‰å…¨è¯„ä¼°æ–¹æ³•ã€‚

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated decision-making pipelines, specifically within Human Resources (HR), the security implications of Indirect Prompt Injection (IPI) become critical. While a prevailing hypothesis posits that "Reasoning" or "Chain-of-Thought" Models possess safety advantages due to their ability to self-correct, emerging research suggests these capabilities may enable more sophisticated alignment failures. This qualitative Red-Teaming case study challenges the safety-through-reasoning premise using the Qwen 3 30B architecture. By subjecting both a standard instruction-tuned model and a reasoning-enhanced model to a "Trojan Horse" curriculum vitae, distinct failure modes are observed. The results suggest a complex trade-off: while the Standard Model resorted to brittle hallucinations to justify simple attacks and filtered out illogical constraints in complex scenarios, the Reasoning Model displayed a dangerous duality. It employed advanced strategic reframing to make simple attacks highly persuasive, yet exhibited "Meta-Cognitive Leakage" when faced with logically convoluted commands. This study highlights a failure mode where the cognitive load of processing complex adversarial instructions causes the injection logic to be unintentionally printed in the final output, rendering the attack more detectable by humans than in Standard Models.

</details>


### [3] [Poster: Privacy-Preserving Compliance Checks on Ethereum via Selective Disclosure](https://arxiv.org/abs/2602.18539)
*Supriya Khadka,Dhiman Goswami,Sanchari Das*

Main category: cs.CR

TL;DR: åŸºäºä»¥å¤ªåŠçš„é€‰æ‹©æ€§æŠ«éœ²æ¡†æ¶ï¼Œä½¿ç”¨zk-SNARKså®ç°å±æ€§éªŒè¯ä¸èº«ä»½æŠ«éœ²çš„è§£è€¦ï¼Œæ»¡è¶³åˆè§„è¦æ±‚åŒæ—¶ä¿æŠ¤éšç§


<details>
  <summary>Details</summary>
Motivation: æ•°å­—èº«ä»½éªŒè¯é€šå¸¸è¿«ä½¿ç”¨æˆ·åœ¨éšç§æ–¹é¢åšå‡ºå¦¥åï¼Œå¿…é¡»æŠ«éœ²æ•æ„Ÿä¸ªäººæ•°æ®æ¥è¯æ˜ç®€å•çš„èµ„æ ¼æ ‡å‡†ã€‚éšç€åŒºå—é“¾åº”ç”¨ä¸å—ç›‘ç®¡ç¯å¢ƒé›†æˆï¼Œè¿™ç§è¿‡åº¦æŠ«éœ²å¸¦æ¥äº†æ•°æ®æ³„éœ²å’Œç›‘æ§çš„é‡å¤§é£é™©ã€‚

Method: æå‡ºåŸºäºä»¥å¤ªåŠçš„é€šç”¨é€‰æ‹©æ€§æŠ«éœ²æ¡†æ¶ï¼Œåˆ©ç”¨å®¢æˆ·ç«¯zk-SNARKsæŠ€æœ¯ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿè¯æ˜ç‰¹å®šèµ„æ ¼è°“è¯è€Œæ— éœ€æ­ç¤ºåº•å±‚èº«ä»½æ–‡æ¡£ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ZK-Complianceï¼Œå®ç°äº†å¹´é¾„éªŒè¯çš„æˆæƒã€éªŒè¯ã€æ’¤é”€åŠŸèƒ½ç”Ÿå‘½å‘¨æœŸã€‚

Result: åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸¥æ ¼çš„åˆè§„è¦æ±‚å¯ä»¥åœ¨å®¢æˆ·ç«¯å»¶è¿Ÿå¯å¿½ç•¥ä¸è®¡ï¼ˆ<200æ¯«ç§’ï¼‰çš„æƒ…å†µä¸‹å¾—åˆ°æ»¡è¶³ï¼ŒåŒæ—¶ä¿æŒå…¬å…±åŒºå—é“¾çš„å‡åæ€§è´¨ã€‚

Conclusion: è¯¥æ¡†æ¶æˆåŠŸè§£å†³äº†åŒºå—é“¾åº”ç”¨ä¸­éšç§ä¸åˆè§„çš„å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡é€‰æ‹©æ€§æŠ«éœ²æœºåˆ¶å®ç°äº†å±æ€§éªŒè¯ä¸èº«ä»½æŠ«éœ²çš„è§£è€¦ï¼Œä¸ºå—ç›‘ç®¡ç¯å¢ƒä¸­çš„åŒºå—é“¾åº”ç”¨æä¾›äº†å¯è¡Œçš„éšç§ä¿æŠ¤è§£å†³æ–¹æ¡ˆã€‚

Abstract: Digital identity verification often forces a privacy trade-off, where users must disclose sensitive personal data to prove simple eligibility criteria. As blockchain applications integrate with regulated environments, this over-disclosure creates significant risks of data breaches and surveillance. This work proposes a general Selective Disclosure Framework built on Ethereum, designed to decouple attribute verification from identity revelation. By utilizing client-side zk-SNARKs, the framework enables users to prove specific eligibility predicates without revealing underlying identity documents. We present a case study, ZK-Compliance, which implements a functional Grant, Verify, Revoke lifecycle for age verification. Preliminary results indicate that strict compliance requirements can be satisfied with negligible client-side latency (< 200 ms) while preserving the pseudonymous nature of public blockchains.

</details>


### [4] [Orbital Escalation: Modeling Satellite Ransomware Attacks Using Game Theory](https://arxiv.org/abs/2602.18624)
*EfrÃ©n LÃ³pez-Morales*

Main category: cs.CR

TL;DR: é¦–ä¸ªé’ˆå¯¹å«æ˜Ÿå‹’ç´¢è½¯ä»¶æ”»å‡»çš„åšå¼ˆè®ºæ¡†æ¶â€”â€”è½¨é“å‡çº§åšå¼ˆï¼Œé€šè¿‡åŠ¨æ€è§„åˆ’æ±‚è§£é˜²å¾¡è€…æœ€ä¼˜ç­–ç•¥å’Œæ”»å‡»è€…é¢„æœŸæ”¶ç›Šï¼ŒåŒ…å«GPS IIIå«æ˜Ÿæ¡ˆä¾‹ç ”ç©¶


<details>
  <summary>Details</summary>
Motivation: å‹’ç´¢è½¯ä»¶å°šæœªè¿›å…¥è½¨é“ï¼Œä½†æ­¤ç±»æ”»å‡»çš„æ¡ä»¶å·²ç»å­˜åœ¨ã€‚å«æ˜Ÿæ‰€æœ‰è€…ã€æ”¿ç­–åˆ¶å®šè€…å’Œç ”ç©¶äººå‘˜éœ€è¦æ­£å¼æ¡†æ¶æ¥åº”å¯¹èˆªå¤©å™¨è¢«å‹’ç´¢çš„æƒ…å†µ

Method: æå‡ºè½¨é“å‡çº§åšå¼ˆæ¨¡å‹ï¼Œæ”»å‡»è€…é€šè¿‡è½¨é“é€šé“é€æ­¥å‡çº§å‹’ç´¢è¦æ±‚ï¼Œé˜²å¾¡è€…é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼ˆå¦‚å°è¯•æ¢å¤ç¨‹åºï¼‰ã€‚ä½¿ç”¨åŠ¨æ€è§„åˆ’æ±‚è§£é˜²å¾¡è€…æœ€ä¼˜ç­–ç•¥å’Œæ”»å‡»è€…é¢„æœŸæ”¶ç›Šï¼Œè€ƒè™‘çœŸå®è½¨é“çº¦æŸ

Result: å»ºç«‹äº†é¦–ä¸ªå«æ˜Ÿå‹’ç´¢è½¯ä»¶åšå¼ˆè®ºæ¡†æ¶ï¼Œé€šè¿‡GPS IIIå«æ˜Ÿæ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†æ¨¡å‹åœ¨è™šæ„ä½†å¯è¡Œçš„å‹’ç´¢æ”»å‡»ä¸­çš„åº”ç”¨ï¼Œèƒ½å¤Ÿæ¨å¯¼å‡ºæ¯ä¸€æ­¥çš„æœ€ä½³ç­–ç•¥

Conclusion: è¿™ä¸€åŸºç¡€æ¨¡å‹ä¸ºå«æ˜Ÿæ‰€æœ‰è€…ã€æ”¿ç­–åˆ¶å®šè€…å’Œç ”ç©¶äººå‘˜æä¾›äº†æ­£å¼æ¡†æ¶ï¼Œå¸®åŠ©ä»–ä»¬æ›´å¥½åœ°å‡†å¤‡åº”å¯¹èˆªå¤©å™¨è¢«å‹’ç´¢çš„æƒ…å†µ

Abstract: Ransomware has yet to reach orbit, but the conditions for such an attack already exist. This paper presents the first game-theoretic framework for modeling ransomware against satellites: the orbital escalation game. In this model, the attacker escalates ransom demands across orbital passes, while the defender chooses their best strategy, e.g., attempt a restore procedure. Using dynamic programming, we solve the defender's optimal strategy and the attacker's expected payoff under real orbital constraints. Additionally, we provide a GPS III satellite case study that demonstrates how our orbital escalation game can be applied in the context of a fictional but feasible ransomware attack to derive the best strategies at every step. In conclusion, this foundational model offers satellite owners, policy makers and researchers, a formal framework to better prepare their responses when a spacecraft is held for ransom.

</details>


### [5] [Media Integrity and Authentication: Status, Directions, and Futures](https://arxiv.org/abs/2602.18681)
*Jessica Young,Sam Vaughan,Andrew Jenks,Henrique Malvar,Christian Paquin,Paul England,Thomas Roca,Juan LaVista Ferres,Forough Poursabzi,Neil Coles,Ken Archer,Eric Horvitz*

Main category: cs.CR

TL;DR: è¯¥è®ºæ–‡ç»¼è¿°äº†AIç”Ÿæˆåª’ä½“ä¸çœŸå®åª’ä½“é‰´åˆ«çš„æŒ‘æˆ˜ï¼Œè¯„ä¼°äº†æ¥æºè¿½è¸ªã€æ•°å­—æ°´å°å’ŒæŒ‡çº¹è¯†åˆ«ä¸‰ç§è®¤è¯æ–¹æ³•ï¼Œåˆ†æäº†è·¨æ¨¡æ€æ“ä½œã€å¨èƒæ¨¡å‹å’Œç°å®å·¥ä½œæµï¼Œç‰¹åˆ«å…³æ³¨äº†ç¤¾ä¼šæŠ€æœ¯åè½¬æ”»å‡»ï¼Œå¹¶æå‡ºäº†åŸºäºå®‰å…¨é£åœ°çš„é«˜ç½®ä¿¡åº¦æ¥æºè®¤è¯æŠ€æœ¯æ–¹å‘ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€AIç”Ÿæˆåª’ä½“çš„å¿«é€Ÿå‘å±•ï¼ŒåŒºåˆ†AIç”Ÿæˆå†…å®¹ä¸çœŸå®ç›¸æœº/éº¦å…‹é£æ•è·å†…å®¹æˆä¸ºé‡è¦æŒ‘æˆ˜ã€‚éœ€è¦å»ºç«‹å¯é çš„åª’ä½“å®Œæ•´æ€§å’Œè®¤è¯æ–¹æ³•æ¥åº”å¯¹è¿™ä¸€æ–°å…´å¨èƒï¼Œé˜²æ­¢è™šå‡ä¿¡æ¯ä¼ æ’­ã€‚

Method: è®ºæ–‡é¦–å…ˆå®šä¹‰äº†ä¸‰ç§ä¸»è¦è®¤è¯æ–¹æ³•ï¼šæ¥æºè¿½è¸ªã€æ•°å­—æ°´å°å’ŒæŒ‡çº¹è¯†åˆ«ã€‚ç„¶åæ·±å…¥åˆ†æäº†ä¸‰ç§ä»£è¡¨æ€§æŠ€æœ¯ï¼šåŠ å¯†å®‰å…¨æ¥æºè¿½è¸ªã€ä¸å¯æ„ŸçŸ¥æ°´å°å’Œè½¯å“ˆå¸ŒæŒ‡çº¹è¯†åˆ«ã€‚é€šè¿‡è·¨æ¨¡æ€åˆ†æã€å¨èƒæ¨¡å‹è¯„ä¼°å’Œç°å®å·¥ä½œæµï¼ˆæ•è·ã€ç¼–è¾‘ã€åˆ†å‘ã€éªŒè¯ï¼‰çš„è€ƒå¯Ÿï¼Œè¯„ä¼°äº†è¿™äº›æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚

Result: åˆ†ææ­ç¤ºäº†ç¤¾ä¼šæŠ€æœ¯åè½¬æ”»å‡»çš„é£é™©ï¼Œå³æ”»å‡»è€…å¯ä»¥åè½¬å®Œæ•´æ€§ä¿¡å·ï¼Œä½¿çœŸå®å†…å®¹çœ‹èµ·æ¥åƒåˆæˆçš„ï¼Œåä¹‹äº¦ç„¶ã€‚è¿™å‡¸æ˜¾äº†éœ€è¦æ—¢èƒ½æŠµæŠ—æŠ€æœ¯æ”»å‡»åˆèƒ½æŠµæŠ—å¿ƒç†ç¤¾ä¼šæ“çºµçš„éªŒè¯ç³»ç»Ÿçš„é‡è¦æ€§ã€‚

Conclusion: è®ºæ–‡æå‡ºäº†å®ç°é«˜ç½®ä¿¡åº¦æ¥æºè®¤è¯çš„æŠ€æœ¯æ–¹å‘ï¼ŒåŒ…æ‹¬åˆ©ç”¨å®‰å…¨é£åœ°åŠ å¼ºè¾¹ç¼˜è®¾å¤‡å®‰å…¨ã€‚å¼ºè°ƒäº†å¼€å‘æ—¢èƒ½åº”å¯¹æŠ€æœ¯æ”»å‡»åˆèƒ½æŠµå¾¡ç¤¾ä¼šå·¥ç¨‹æ“çºµçš„å¼¹æ€§éªŒè¯ç³»ç»Ÿçš„å¿…è¦æ€§ã€‚

Abstract: We provide background on emerging challenges and future directions with media integrity and authentication methods, focusing on distinguishing AI-generated media from authentic content captured by cameras and microphones. We evaluate several approaches, including provenance, watermarking, and fingerprinting. After defining each method, we analyze three representative technologies: cryptographically secured provenance, imperceptible watermarking, and soft-hash fingerprinting. We analyze how these tools operate across modalities and evaluate relevant threat models, attack categories, and real-world workflows spanning capture, editing, distribution, and verification. We consider sociotechnical reversal attacks that can invert integrity signals, making authentic content appear synthetic and vice versa, highlighting the value of verification systems that are resilient to both technical and psychosocial manipulation. Finally, we outline techniques for delivering high-confidence provenance authentication, including directions for strengthening edge-device security using secure enclaves.

</details>


### [6] [MANATEE: Inference-Time Lightweight Diffusion Based Safety Defense for LLMs](https://arxiv.org/abs/2602.18782)
*Chun Yan Ryan Kan,Tommy Tran,Vedant Yadav,Ava Cai,Kevin Zhu,Ruizhe Li,Maheep Chaudhary*

Main category: cs.CR

TL;DR: MANATEEæ˜¯ä¸€ç§åŸºäºå¯†åº¦ä¼°è®¡çš„æ¨ç†æ—¶é˜²å¾¡æ–¹æ³•ï¼Œé€šè¿‡æ‰©æ•£è¿‡ç¨‹å°†å¼‚å¸¸éšè—çŠ¶æ€æŠ•å½±åˆ°è‰¯æ€§è¡¨ç¤ºæµå½¢ä¸Šï¼Œæœ‰æ•ˆé˜²å¾¡LLMå¯¹æŠ—æ€§è¶Šç‹±æ”»å‡»


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰LLMå¯¹æŠ—æ€§è¶Šç‹±æ”»å‡»é˜²å¾¡æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼šäºŒå…ƒåˆ†ç±»å™¨åœ¨å¯¹æŠ—æ€§è¾“å…¥è¶…å‡ºå­¦ä¹ å†³ç­–è¾¹ç•Œæ—¶å¤±æ•ˆï¼›é‡å¤å¾®è°ƒè®¡ç®—æˆæœ¬é«˜ä¸”å¯èƒ½é™ä½æ¨¡å‹èƒ½åŠ›

Method: MANATEEä½¿ç”¨è‰¯æ€§è¡¨ç¤ºæµå½¢ä¸Šçš„å¯†åº¦ä¼°è®¡ï¼Œå­¦ä¹ è‰¯æ€§éšè—çŠ¶æ€çš„å¾—åˆ†å‡½æ•°ï¼Œé€šè¿‡æ‰©æ•£è¿‡ç¨‹å°†å¼‚å¸¸è¡¨ç¤ºæŠ•å½±åˆ°å®‰å…¨åŒºåŸŸï¼Œæ— éœ€æœ‰å®³è®­ç»ƒæ•°æ®å’Œæ¶æ„ä¿®æ”¹

Result: åœ¨Mistral-7B-Instructã€Llama-3.1-8B-Instructå’ŒGemma-2-9B-itä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMANATEEåœ¨æŸäº›æ•°æ®é›†ä¸Šå¯å°†æ”»å‡»æˆåŠŸç‡é™ä½é«˜è¾¾100%ï¼ŒåŒæ—¶ä¿æŒå¯¹è‰¯æ€§è¾“å…¥çš„æ¨¡å‹æ•ˆç”¨

Conclusion: MANATEEæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ—¶é˜²å¾¡æ–¹æ³•ï¼Œé€šè¿‡å¯†åº¦ä¼°è®¡å’Œæ‰©æ•£æŠ•å½±æŠ€æœ¯é˜²å¾¡LLMå¯¹æŠ—æ€§è¶Šç‹±æ”»å‡»ï¼Œæ— éœ€æœ‰å®³æ•°æ®æˆ–æ¶æ„ä¿®æ”¹ï¼Œåœ¨ä¿æŒæ¨¡å‹æ•ˆç”¨çš„åŒæ—¶æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡

Abstract: Defending LLMs against adversarial jailbreak attacks remains an open challenge. Existing defenses rely on binary classifiers that fail when adversarial input falls outside the learned decision boundary, and repeated fine-tuning is computationally expensive while potentially degrading model capabilities. We propose MANATEE, an inference-time defense that uses density estimation over a benign representation manifold. MANATEE learns the score function of benign hidden states and uses diffusion to project anomalous representations toward safe regions--requiring no harmful training data and no architectural modifications. Experiments across Mistral-7B-Instruct, Llama-3.1-8B-Instruct, and Gemma-2-9B-it demonstrate that MANATEE reduce Attack Success Rate by up to 100\% on certain datasets, while preserving model utility on benign inputs.

</details>


### [7] [SiGRRW: A Single-Watermark Robust Reversible Watermarking Framework with Guiding Strategy](https://arxiv.org/abs/2602.19097)
*Zikai Xu,Bin Liu,Weihai Li,Lijunxian Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: æå‡ºSiGRRWå•æ°´å°é²æ£’å¯é€†æ°´å°æ¡†æ¶ï¼Œé€šè¿‡å¼•å¯¼å›¾åƒç­–ç•¥å®ç°å•æ¬¡æ°´å°åŒæ—¶å…·å¤‡é²æ£’æ€§å’Œå¯é€†æ€§ï¼Œé€‚ç”¨äºç”Ÿæˆæ¨¡å‹å’Œè‡ªç„¶å›¾åƒ


<details>
  <summary>Details</summary>
Motivation: å½“å‰é²æ£’å¯é€†æ°´å°æ–¹æ¡ˆé€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ— æ³•åœ¨å•æ¬¡æ°´å°ä¸­åŒæ—¶å®ç°é²æ£’æ€§å’Œå¯é€†æ€§ï¼Œä¸”ä¸¤ä¸ªæ°´å°ä¹‹é—´çš„åŠŸèƒ½å¹²æ‰°å¯¼è‡´å®¹é‡å’Œä¸å¯æ„ŸçŸ¥æ€§ç­‰å¤šæ–¹é¢æ€§èƒ½ä¸‹é™

Method: æå‡ºSiGRRWå•æ°´å°æ¡†æ¶ï¼Œå¼•å…¥æ–°é¢–çš„å¼•å¯¼ç­–ç•¥ç”Ÿæˆå¼•å¯¼å›¾åƒä½œä¸ºåµŒå…¥å’Œæ¢å¤çš„æŒ‡å¯¼ï¼›é€šè¿‡å¼•å¯¼æ®‹å·®å¯é€†åœ°åµŒå…¥æ°´å°ï¼Œè¯¥æ®‹å·®å¯ä»è½½ä½“å›¾åƒå’Œæ°´å°å›¾åƒè®¡ç®—ï¼›æ¡†æ¶å¯ä½œä¸ºç”Ÿæˆæ¨¡å‹è¾“å‡ºé˜¶æ®µçš„å³æ’å³ç”¨æ°´å°å±‚ï¼Œæˆ–ç›´æ¥åº”ç”¨äºè‡ªç„¶å›¾åƒ

Result: å®éªŒè¡¨æ˜SiGRRWç›¸æ¯”ç°æœ‰RRWæ–¹æ¡ˆæœ‰æ•ˆæå‡äº†ä¸å¯æ„ŸçŸ¥æ€§å’Œé²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒè½½ä½“å›¾åƒçš„æ— æŸæ¢å¤ï¼Œå®¹é‡æ˜¾è‘—é«˜äºä¼ ç»Ÿæ–¹æ¡ˆ

Conclusion: SiGRRWæ¡†æ¶è§£å†³äº†ä¼ ç»Ÿä¸¤é˜¶æ®µRRWæ–¹æ¡ˆçš„åŠŸèƒ½å¹²æ‰°é—®é¢˜ï¼Œåœ¨å•æ¬¡æ°´å°ä¸­åŒæ—¶å®ç°äº†é²æ£’æ€§ã€å¯é€†æ€§ã€é«˜å®¹é‡å’Œè‰¯å¥½ä¸å¯æ„ŸçŸ¥æ€§ï¼Œä¸ºç”Ÿæˆæ¨¡å‹å’Œè‡ªç„¶å›¾åƒæä¾›äº†ç»Ÿä¸€çš„é²æ£’å¯é€†æ°´å°è§£å†³æ–¹æ¡ˆ

Abstract: Robust reversible watermarking (RRW) enables copyright protection for images while overcoming the limitation of distortion introduced by watermark itself. Current RRW schemes typically employ a two-stage framework, which fails to achieve simultaneous robustness and reversibility within a single watermarking, and functional interference between the two watermarks results in performance degradation in multiple terms such as capacity and imperceptibility. We propose SiGRRW, a single-watermark RRW framework, which is applicable to both generative models and natural images. We introduce a novel guiding strategy to generate guiding images, serving as the guidance for embedding and recovery. The watermark is reversibly embedded with the guiding residual, which can be calculated from both cover images and watermark images. The proposed framework can be deployed either as a plug-and-play watermarking layer at the output stage of generative models, or directly applied to natural images. Extensive experiments demonstrate that SiGRRW effectively enhances imperceptibility and robustness compared to existing RRW schemes while maintaining lossless recovery of cover images, with significantly higher capacity than conventional schemes.

</details>


### [8] [ReVision : A Post-Hoc, Vision-Based Technique for Replacing Unacceptable Concepts in Image Generation Pipeline](https://arxiv.org/abs/2602.19149)
*Gurjot Singh,Prabhjot Singh,Aashima Sharma,Maninder Singh,Ryan Ko*

Main category: cs.CR

TL;DR: ReVisionï¼šä¸€ç§æ— éœ€è®­ç»ƒã€åŸºäºæç¤ºçš„åå¤„ç†å®‰å…¨æ¡†æ¶ï¼Œé€šè¿‡VLMè¾…åŠ©çš„ç©ºé—´é—¨æ§æœºåˆ¶å®ç°ç²¾ç¡®çš„å±€éƒ¨è¯­ä¹‰ç¼–è¾‘ï¼Œæœ‰æ•ˆæŠ‘åˆ¶å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„è¿è§„å†…å®¹ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰å›¾åƒç”Ÿæˆæ¨¡å‹å®‰å…¨ç¼“è§£ç­–ç•¥ï¼ˆå¦‚æç¤ºè¿‡æ»¤ã€å®‰å…¨æ„ŸçŸ¥è®­ç»ƒ/å¾®è°ƒï¼‰å­˜åœ¨æ˜“è¢«ç»•è¿‡ä¸”ä¼šé™ä½ç”Ÿæˆè´¨é‡çš„é—®é¢˜ï¼Œéœ€è¦ä¸€ç§è®­ç»ƒæ— å…³ã€åå¤„ç†çš„æœ€åé˜²çº¿æ¥ç²¾ç¡®ç¼–è¾‘è¿è§„å†…å®¹ã€‚

Method: æå‡ºReVisionæ¡†æ¶ï¼š1) ä½¿ç”¨Gemini-2.5-Flashä½œä¸ºé€šç”¨è¿è§„æ¦‚å¿µæ£€æµ‹å™¨ï¼›2) å¼•å…¥VLMè¾…åŠ©çš„ç©ºé—´é—¨æ§æœºåˆ¶å®ç°å®ä¾‹ä¸€è‡´å®šä½ï¼›3) è¿›è¡Œå±€éƒ¨è¯­ä¹‰ç¼–è¾‘æ›¿æ¢ä¸å®‰å…¨å†…å®¹ï¼Œä¸ä¿®æ”¹åº•å±‚ç”Ÿæˆå™¨ã€‚

Result: åœ¨245å¼ å›¾åƒçš„åŸºå‡†æµ‹è¯•ä¸­ï¼š1) CLIPå®‰å…¨æç¤ºå¯¹é½åº¦å¹³å‡æå‡+0.121ï¼›2) å¤šæ¦‚å¿µèƒŒæ™¯ä¿çœŸåº¦æ˜¾è‘—æ”¹å–„ï¼ˆLPIPSä»0.166é™è‡³0.058ï¼‰ï¼›3) ç±»åˆ«ç‰¹å®šæ£€æµ‹å™¨è¿‘ä¹å®Œå…¨æŠ‘åˆ¶ï¼ˆå¦‚NudeNetä»70.51é™è‡³0ï¼‰ï¼›4) äººå·¥å®¡æ ¸ä¸­è¿è§„å†…å®¹å¯è¯†åˆ«ç‡ä»95.99%é™è‡³10.16%ã€‚

Conclusion: ReVisionä½œä¸ºè®­ç»ƒæ— å…³çš„åå¤„ç†å®‰å…¨æ¡†æ¶ï¼Œé€šè¿‡ç²¾ç¡®çš„å®ä¾‹çº§å®šä½å’Œè¯­ä¹‰ç¼–è¾‘ï¼Œèƒ½æœ‰æ•ˆæŠ‘åˆ¶å›¾åƒç”Ÿæˆä¸­çš„è¿è§„å†…å®¹ï¼ŒåŒæ—¶ä¿æŒåœºæ™¯å®Œæ•´æ€§ï¼Œä¸ºéƒ¨ç½²æä¾›äº†å®ç”¨çš„æœ€åé˜²çº¿è§£å†³æ–¹æ¡ˆã€‚

Abstract: Image-generative models are widely deployed across industries. Recent studies show that they can be exploited to produce policy-violating content. Existing mitigation strategies primarily operate at the pre- or mid-generation stages through techniques such as prompt filtering and safety-aware training/fine-tuning. Prior work shows that these approaches can be bypassed and often degrade generative quality. In this work, we propose ReVision, a training-free, prompt-based, post-hoc safety framework for image-generation pipeline. ReVision acts as a last-line defense by analyzing generated images and selectively editing unsafe concepts without altering the underlying generator. It uses the Gemini-2.5-Flash model as a generic policy-violating concept detector, avoiding reliance on multiple category-specific detectors, and performs localized semantic editing to replace unsafe content. Prior post-hoc editing methods often rely on imprecise spatial localization, that undermines usability and limits deployability, particularly in multi-concept scenes. To address this limitation, ReVision introduces a VLM-assisted spatial gating mechanism that enforces instance-consistent localization, enabling precise edits while preserving scene integrity. We evaluate ReVision on a 245-image benchmark covering both single- and multi-concept scenarios. Results show that ReVision (i) improves CLIP-based alignment toward safe prompts by +$0.121$ on average; (ii) significantly improves multi-concept background fidelity (LPIPS $0.166 \rightarrow 0.058$); (iii) achieves near-complete suppression on category-specific detectors (e.g., NudeNet $70.51 \rightarrow 0$); and (iv) reduces policy-violating content recognizability in a human moderation study from $95.99\%$ to $10.16\%$.

</details>


### [9] [KUDA: Knowledge Unlearning by Deviating Representation for Large Language Models](https://arxiv.org/abs/2602.19275)
*Ce Fang,Zhikun Zhang,Min Chen,Qing Liu,Lu Zhou,Zhe Liu,Yunjun Gao*

Main category: cs.CR

TL;DR: KUDAæ˜¯ä¸€ç§æ–°çš„LLMçŸ¥è¯†é—å¿˜æ–¹æ³•ï¼Œé€šè¿‡å› æœè¿½è¸ªå®šä½çŸ¥è¯†å­˜å‚¨å±‚ï¼Œè®¾è®¡è¡¨ç¤ºåç¦»ç›®æ ‡å®ç°çŸ¥è¯†ç§»é™¤ï¼Œé‡‡ç”¨æ¾å¼›é›¶ç©ºé—´æŠ•å½±è§£å†³é—å¿˜ä¸ä¿ç•™çš„ä¼˜åŒ–å†²çªï¼Œåœ¨WMDPå’ŒMUSEåŸºå‡†ä¸Šä¼˜äºç°æœ‰åŸºçº¿ã€‚


<details>
  <summary>Details</summary>
Motivation: LLMé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒè·å¾—ä¸°å¯ŒçŸ¥è¯†ï¼Œä½†ä¹Ÿæ”¾å¤§äº†è®­ç»ƒæ•°æ®ä¸­æ•æ„Ÿã€ç‰ˆæƒæˆ–æœ‰å®³å†…å®¹çš„é£é™©ã€‚ç°æœ‰LLMé—å¿˜æ–¹æ³•å¾€å¾€å¯¼è‡´æ¨¡å‹ç”Ÿæˆéšæœºæˆ–ä¸è¿è´¯ç­”æ¡ˆï¼Œå› ä¸ºå®ƒä»¬æ— æ³•ç²¾ç¡®æ”¹å˜ç¼–ç çŸ¥è¯†ã€‚éœ€è¦å®ç°çŸ¥è¯†å±‚é¢çš„æœ‰æ•ˆé—å¿˜ã€‚

Method: æå‡ºKUDAæ–¹æ³•ï¼š1)ä½¿ç”¨å› æœè¿½è¸ªå®šä½ç›®æ ‡çŸ¥è¯†å­˜å‚¨çš„ç‰¹å®šå±‚ï¼›2)è®¾è®¡æ–°çš„é—å¿˜ç›®æ ‡ï¼Œè¯±å¯¼æ¨¡å‹è¡¨ç¤ºåç¦»åŸå§‹ä½ç½®ä»¥ç ´åä¸ç›®æ ‡çŸ¥è¯†çš„å…³è”ï¼›3)é‡‡ç”¨æ¾å¼›é›¶ç©ºé—´æŠ•å½±æœºåˆ¶ç¼“è§£å¯¹ä¿ç•™çŸ¥è¯†è¡¨ç¤ºç©ºé—´çš„å¹²æ‰°ï¼Œè§£å†³é—å¿˜ä¸ä¿ç•™çš„ä¼˜åŒ–å†²çªã€‚

Result: åœ¨WMDPå’ŒMUSEç­‰ä»£è¡¨æ€§åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒKUDAåœ¨æœ‰æ•ˆå¹³è¡¡çŸ¥è¯†ç§»é™¤å’Œæ¨¡å‹æ•ˆç”¨ä¿ç•™æ–¹é¢ä¼˜äºå¤§å¤šæ•°ç°æœ‰åŸºçº¿æ–¹æ³•ã€‚

Conclusion: KUDAé€šè¿‡ç²¾ç¡®çš„çŸ¥è¯†å®šä½ã€è¡¨ç¤ºåç¦»æœºåˆ¶å’Œä¼˜åŒ–å†²çªè§£å†³ç­–ç•¥ï¼Œå®ç°äº†LLMçŸ¥è¯†å±‚é¢çš„æœ‰æ•ˆé—å¿˜ï¼Œä¸ºé™ä½LLMé£é™©æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æŠ€æœ¯æ–¹æ¡ˆã€‚

Abstract: Large language models (LLMs) acquire a large amount of knowledge through pre-training on vast and diverse corpora. While this endows LLMs with strong capabilities in generation and reasoning, it amplifies risks associated with sensitive, copyrighted, or harmful content in training data.LLM unlearning, which aims to remove specific knowledge encoded within models, is a promising technique to reduce these risks. However, existing LLM unlearning methods often force LLMs to generate random or incoherent answers due to their inability to alter the encoded knowledge precisely. To achieve effective unlearning at the knowledge level of LLMs, we propose Knowledge Unlearning by Deviating representAtion (KUDA). We first utilize causal tracing to locate specific layers for target knowledge storage. We then design a new unlearning objective that induces the model's representations to deviate from its original position in the phase of knowledge removal, thus disrupting the ability to associate with the target knowledge. To resolve the optimization conflicts between forgetting and retention, we employ a relaxation null-space projection mechanism to mitigate the disruption to the representation space of retaining knowledge. Extensive experiments on representative benchmarks, WMDP and MUSE, demonstrate that KUDA outperforms most existing baselines by effectively balancing knowledge removal and model utility retention.

</details>


### [10] [Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments](https://arxiv.org/abs/2602.19450)
*Kunal Mukherjee*

Main category: cs.CR

TL;DR: è®ºæ–‡æå‡ºTEE-RedBenchè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹è¯•LLMä½œä¸ºTEEå®‰å…¨é¡¾é—®æ—¶çš„é£é™©ï¼Œå‘ç°ChatGPTå’ŒClaudeå­˜åœ¨å¹»è§‰ã€è¿‡åº¦æ‰¿è¯ºç­‰é—®é¢˜ï¼Œå¹¶æå‡º"LLM-in-the-loop"è§£å†³æ–¹æ¡ˆå‡å°‘80.62%çš„å¤±è´¥ç‡ã€‚


<details>
  <summary>Details</summary>
Motivation: å°½ç®¡å¯ä¿¡æ‰§è¡Œç¯å¢ƒï¼ˆTEEsï¼‰æ—¨åœ¨ä¿æŠ¤æ•æ„Ÿè®¡ç®—å…å—æ“ä½œç³»ç»Ÿæ”»å‡»ï¼Œä½†å®é™…éƒ¨ç½²ä»é¢ä¸´å¾®æ¶æ„æ³„æ¼ã€ä¾§ä¿¡é“æ”»å‡»ç­‰é£é™©ã€‚åŒæ—¶ï¼Œå®‰å…¨å›¢é˜Ÿè¶Šæ¥è¶Šå¤šåœ°ä¾èµ–LLMä½œä¸ºTEEå®‰å…¨é¡¾é—®ï¼Œè¿™å¸¦æ¥äº†ç¤¾ä¼šæŠ€æœ¯é£é™©ï¼šLLMå¯èƒ½äº§ç”Ÿå¹»è§‰ã€è¿‡åº¦æ‰¿è¯ºå®‰å…¨ä¿è¯ï¼Œæˆ–åœ¨å¯¹æŠ—æ€§æç¤ºä¸‹è¡¨ç°ä¸å®‰å…¨ã€‚

Method: æå‡ºTEE-RedBenchè¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š(1) LLMä»‹å¯¼å®‰å…¨å·¥ä½œçš„TEEç‰¹å®šå¨èƒæ¨¡å‹ï¼›(2) æ¶µç›–SGXå’ŒTrustZoneæ¶æ„ã€è®¤è¯ä¸å¯†é’¥ç®¡ç†ã€å¨èƒå»ºæ¨¡ã€éæ“ä½œç¼“è§£æŒ‡å—çš„ç»“æ„åŒ–æç¤ºå¥—ä»¶ï¼Œä»¥åŠæ”¿ç­–çº¦æŸçš„æ»¥ç”¨æ¢æµ‹ï¼›(3) è”åˆæµ‹é‡æŠ€æœ¯æ­£ç¡®æ€§ã€åŸºç¡€æ€§ã€ä¸ç¡®å®šæ€§æ ¡å‡†ã€æ‹’ç»è´¨é‡å’Œå®‰å…¨å¸®åŠ©æ€§çš„æ³¨é‡Šè§„åˆ™ã€‚å¯¹ChatGPT-5.2å’ŒClaude Opus-4.6è¿›è¡Œçº¢é˜Ÿæµ‹è¯•ã€‚

Result: ç ”ç©¶å‘ç°ä¸€äº›å¤±è´¥å¹¶éçº¯ç²¹ç‰¹å¼‚çš„ï¼Œåœ¨LLMåŠ©æ‰‹é—´çš„å¯è½¬ç§»æ€§é«˜è¾¾12.02%ã€‚æå‡ºçš„"LLM-in-the-loop"è¯„ä¼°ç®¡é“ï¼ˆæ”¿ç­–é—¨æ§ã€æ£€ç´¢åŸºç¡€ã€ç»“æ„åŒ–æ¨¡æ¿ã€è½»é‡çº§éªŒè¯æ£€æŸ¥ï¼‰ç»„åˆä½¿ç”¨æ—¶ï¼Œèƒ½å°†å¤±è´¥ç‡é™ä½80.62%ã€‚

Conclusion: LLMä½œä¸ºTEEå®‰å…¨é¡¾é—®å­˜åœ¨æ˜¾è‘—é£é™©ï¼Œéœ€è¦ç³»ç»Ÿæ€§çš„è¯„ä¼°å’Œç¼“è§£æªæ–½ã€‚TEE-RedBenchæä¾›äº†å®ç”¨çš„è¯„ä¼°æ¡†æ¶ï¼Œ"LLM-in-the-loop"æ–¹æ³•èƒ½æœ‰æ•ˆå‡å°‘LLMåœ¨å®‰å…¨å…³é”®ä»»åŠ¡ä¸­çš„å¤±è´¥ç‡ï¼Œä¸ºå®‰å…¨å›¢é˜Ÿæä¾›æ›´å¯é çš„è¾…åŠ©å·¥å…·ã€‚

Abstract: Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting.
  We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs. We introduce TEE-RedBench, a TEE-grounded evaluation methodology comprising (i) a TEE-specific threat model for LLM-mediated security work, (ii) a structured prompt suite spanning SGX and TrustZone architecture, attestation and key management, threat modeling, and non-operational mitigation guidance, along with policy-bound misuse probes, and (iii) an annotation rubric that jointly measures technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that some failures are not purely idiosyncratic, transferring up to 12.02% across LLM assistants, and we connect these outcomes to secure architecture by outlining an "LLM-in-the-loop" evaluation pipeline: policy gating, retrieval grounding, structured templates, and lightweight verification checks that, when combined, reduce failures by 80.62%.

</details>


### [11] [Hardware-Friendly Randomization: Enabling Random-Access and Minimal Wiring in FHE Accelerators with Low Total Cost](https://arxiv.org/abs/2602.19550)
*Ilan Rosenfeld,Noam Kleinburd,Hillel Chapman,Dror Reuven*

Main category: cs.CR

TL;DR: æå‡ºä¸€ç§åŸºäºç§å­çš„RLWEå‚æ•°aç”Ÿæˆæ–¹æ¡ˆï¼Œé€šè¿‡å®¢æˆ·ç«¯ä¼ è¾“ç§å­å¹¶åœ¨ç¡¬ä»¶åŠ é€Ÿå™¨ä¸­åŠ¨æ€ç”Ÿæˆaï¼Œæ˜¾è‘—é™ä½é€šä¿¡å¼€é”€å’Œå†…å­˜å ç”¨ï¼ŒåŒæ—¶è§£å†³ç¡¬ä»¶å®ç°ä¸­çš„å¸ƒçº¿ã€åŠŸè€—ç­‰æŒ‘æˆ˜ã€‚


<details>
  <summary>Details</summary>
Motivation: RLWEé—®é¢˜ä¸­å‡åŒ€éšæœºå¤šé¡¹å¼açš„ä¼ è¾“åœ¨å®¢æˆ·ç«¯åˆ°æœåŠ¡å™¨ä»¥åŠç¡¬ä»¶åŠ é€Ÿå™¨è¾“å…¥è¿‡ç¨‹ä¸­ä¼šäº§ç”Ÿæ˜¾è‘—çš„é€šä¿¡å¼€é”€ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿½æ±‚é«˜åŠ é€Ÿå› å­çš„FHEåŠ é€Ÿå™¨ä¸­ã€‚ç°æœ‰æŠ€æœ¯é€šè¿‡ç§å­ç”Ÿæˆaå¯ä»¥å‡å°‘å¼€é”€ï¼Œä½†ç¡¬ä»¶å®ç°é¢ä¸´å¸ƒçº¿å¯†åº¦ã€åŠŸè€—ã€è®¡ç®—é¢ç§¯å’Œè°ƒåº¦çµæ´»æ€§ç­‰æŒ‘æˆ˜ã€‚

Method: æå‡ºå…·ä½“çš„æ–¹æ¡ˆå’Œå‚æ•°ï¼Œåœ¨å®¢æˆ·ç«¯é€šè¿‡ç¡®å®šæ€§è¿‡ç¨‹ä»ç§å­ç”Ÿæˆaï¼Œä»…ä¼ è¾“ç§å­ï¼Œåœ¨åŠ é€Ÿå™¨ä¸­åŠ¨æ€ç”Ÿæˆaã€‚è¯¥æ–¹æ¡ˆæ”¯æŒå¹¶è¡Œç”Ÿæˆå‡åŒ€åˆ†å¸ƒæ ·æœ¬ï¼Œæ”¾å®½å¸ƒçº¿è¦æ±‚ï¼Œå…è®¸å¯¹RNS limbsçš„æ— é™åˆ¶éšæœºè®¿é—®ï¼Œå¹¶ä¼˜åŒ–å®¢æˆ·ç«¯å¼€é”€ã€‚

Result: æ–¹æ¡ˆåœ¨ä¿æŒé€šä¿¡å»¶è¿Ÿå’Œå†…å­˜å ç”¨å‡å°‘çš„åŒæ—¶ï¼Œå®¢æˆ·ç«¯å¼€é”€æä½ï¼ˆå¯†é’¥ç”Ÿæˆè¿‡ç¨‹å°äº3%ï¼‰ã€‚æ¶ˆé™¤äº†å¯¹åšé‡‘å±å±‚è¿›è¡Œéšæœºæ€§åˆ†å¸ƒçš„éœ€æ±‚ï¼Œé˜²æ­¢PRNGå­ç³»ç»ŸåŠŸè€—éšåŠ é€Ÿå› å­å‘ˆæŒ‡æ•°å¢é•¿ï¼Œåœ¨é«˜ååé…ç½®ä¸­æ¯ä¸ªåŠ é€Ÿå™¨èŠ¯ç‰‡å¯èŠ‚çœæ•°åç“¦åŠŸè€—ã€‚

Conclusion: æå‡ºçš„å…·ä½“æ–¹æ¡ˆæœ‰æ•ˆè§£å†³äº†RLWEä¸­aå¤šé¡¹å¼ä¼ è¾“çš„é€šä¿¡å¼€é”€é—®é¢˜ï¼Œé€šè¿‡ç§å­ç”Ÿæˆå’ŒåŠ¨æ€ç”Ÿæˆæœºåˆ¶ï¼Œåœ¨ç¡¬ä»¶å®ç°ä¸­å¹³è¡¡äº†æ€§èƒ½ã€åŠŸè€—å’Œçµæ´»æ€§è¦æ±‚ï¼Œä¸ºé«˜æ•ˆFHEåŠ é€Ÿå™¨è®¾è®¡æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚

Abstract: The Ring-Learning With Errors (RLWE) problem forms the backbone of highly efficient Fully Homomorphic Encryption (FHE) schemes. A significant component of the RLWE public key and ciphertext of the form $(b,a)$ is the uniformly random polynomial $a \in R_q$ . While essential for security, the communication overhead of transmitting $a$ from client to server, and inputting it into a hardware accelerator, can be substantial, especially for FHE accelerators aiming at high acceleration factors. A known technique in reducing this overhead generates $a$ from a small seed on the client side via a deterministic process, transmits only the seed, and generates $a$ on-the-fly within the accelerator. Challenges in the hardware implementation of an accelerator include wiring (density and power), compute area, compute power as well as flexibility in scheduling of on-the-fly generation instructions. This extended abstract proposes a concrete scheme and parameters wherein these practical challenges are addressed. We detail the benefits of our approach, which maintains the reduction in communication latency and memory footprint, while allowing parallel generation of uniformly distributed samples, relaxed wiring requirements, unrestricted randomaccess to RNS limbs, and results in an extremely low overhead on the client side (i.e. less than 3%) during the key generation process. The proposed scheme eliminates the need for thick metal layers for randomness distribution and prevents the power consumption of the PRNG subsystem from scaling prohibitively with the acceleration factor, potentially saving tens of Watts per accelerator chip in high-throughput configurations.

</details>


### [12] [Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains](https://arxiv.org/abs/2602.19555)
*Xiaochong Jiang,Shiqi Yang,Wenting Yang,Yichen Liu,Cheng Ji*

Main category: cs.CR

TL;DR: ç³»ç»ŸåŒ–åˆ†æåŸºäºLLMçš„æ™ºèƒ½ä½“ç³»ç»Ÿåœ¨è¿è¡Œæ—¶é¢ä¸´çš„å®‰å…¨é£é™©ï¼Œæå‡ºç»Ÿä¸€æ¡†æ¶åˆ†ç±»å¨èƒï¼Œå¹¶å€¡å¯¼é›¶ä¿¡ä»»è¿è¡Œæ—¶æ¶æ„


<details>
  <summary>Details</summary>
Motivation: åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“ç³»ç»Ÿä»æ–‡æœ¬ç”Ÿæˆæ‰©å±•åˆ°è‡ªä¸»æ£€ç´¢ä¿¡æ¯å’Œè°ƒç”¨å·¥å…·ï¼Œè¿™ç§è¿è¡Œæ—¶æ‰§è¡Œæ¨¡å‹å°†æ”»å‡»é¢ä»æ„å»ºæ—¶å·¥ä»¶è½¬ç§»åˆ°æ¨ç†æ—¶ä¾èµ–ï¼Œä½¿æ™ºèƒ½ä½“é¢ä¸´é€šè¿‡ä¸å¯ä¿¡æ•°æ®å’Œæ¦‚ç‡èƒ½åŠ›è§£æçš„æ“çºµé£é™©ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹çº§æ¼æ´ï¼Œè€Œç”±å¾ªç¯å’Œç›¸äº’ä¾èµ–çš„è¿è¡Œæ—¶è¡Œä¸ºäº§ç”Ÿçš„å®‰å…¨é£é™©ä»ç„¶åˆ†æ•£ï¼Œéœ€è¦ç³»ç»ŸåŒ–åˆ†æã€‚

Method: æå‡ºç»Ÿä¸€çš„è¿è¡Œæ—¶æ¡†æ¶æ¥ç³»ç»ŸåŒ–åˆ†æè¿™äº›é£é™©ï¼Œå°†å¨èƒåˆ†ç±»ä¸ºæ•°æ®ä¾›åº”é“¾æ”»å‡»ï¼ˆç¬æ—¶ä¸Šä¸‹æ–‡æ³¨å…¥å’ŒæŒä¹…æ€§å†…å­˜æ±¡æŸ“ï¼‰å’Œå·¥å…·ä¾›åº”é“¾æ”»å‡»ï¼ˆå‘ç°ã€å®ç°å’Œè°ƒç”¨ï¼‰ã€‚è¯†åˆ«"ç—…æ¯’æ™ºèƒ½ä½“å¾ªç¯"ç°è±¡ï¼Œå³æ™ºèƒ½ä½“ä½œä¸ºè‡ªæˆ‘ä¼ æ’­ç”Ÿæˆè •è™«çš„è½½ä½“è€Œæ— éœ€åˆ©ç”¨ä»£ç çº§ç¼ºé™·ã€‚å€¡å¯¼é›¶ä¿¡ä»»è¿è¡Œæ—¶æ¶æ„ï¼Œå°†ä¸Šä¸‹æ–‡è§†ä¸ºä¸å¯ä¿¡æ§åˆ¶æµï¼Œå¹¶é€šè¿‡åŠ å¯†æ¥æºè€Œéè¯­ä¹‰æ¨ç†æ¥çº¦æŸå·¥å…·æ‰§è¡Œã€‚

Result: å»ºç«‹äº†åŸºäºLLMçš„æ™ºèƒ½ä½“ç³»ç»Ÿè¿è¡Œæ—¶å®‰å…¨é£é™©çš„ç³»ç»ŸåŒ–åˆ†ç±»æ¡†æ¶ï¼Œè¯†åˆ«äº†æ•°æ®ä¾›åº”é“¾å’Œå·¥å…·ä¾›åº”é“¾ä¸¤ç±»ä¸»è¦æ”»å‡»å‘é‡ï¼Œå‘ç°äº†ç—…æ¯’æ™ºèƒ½ä½“å¾ªç¯è¿™ä¸€æ–°å‹å¨èƒæ¨¡å¼ï¼Œä¸ºé›¶ä¿¡ä»»è¿è¡Œæ—¶æ¶æ„æä¾›äº†ç†è®ºåŸºç¡€ã€‚

Conclusion: æ™ºèƒ½ä½“ç³»ç»Ÿçš„å®‰å…¨é£é™©éœ€è¦ä»ä¼ ç»Ÿçš„æ¨¡å‹çº§æ¼æ´åˆ†æè½¬å‘è¿è¡Œæ—¶è¡Œä¸ºåˆ†æã€‚æå‡ºçš„ç»Ÿä¸€æ¡†æ¶ä¸ºç†è§£å’Œç®¡ç†è¿™äº›é£é™©æä¾›äº†ç³»ç»ŸåŒ–æ–¹æ³•ï¼Œè€Œé›¶ä¿¡ä»»è¿è¡Œæ—¶æ¶æ„é€šè¿‡å°†ä¸Šä¸‹æ–‡è§†ä¸ºä¸å¯ä¿¡æ§åˆ¶æµå¹¶ä½¿ç”¨åŠ å¯†æ¥æºçº¦æŸå·¥å…·æ‰§è¡Œï¼Œä¸ºæ„å»ºæ›´å®‰å…¨çš„æ™ºèƒ½ä½“ç³»ç»Ÿæä¾›äº†è®¾è®¡åŸåˆ™ã€‚

Abstract: Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference.

</details>


### [13] [Efficient Multi-Party Secure Comparison over Different Domains with Preprocessing Assistance](https://arxiv.org/abs/2602.19604)
*Kaiwen Wang,Xiaolin Chang,Yuehan Dong,Ruichen Zhang*

Main category: cs.CR

TL;DR: æœ¬æ–‡æå‡ºäº†é¦–ä¸ªæ”¯æŒnæ–¹å‚ä¸ã€åœ¨ğ”½_på’Œâ„¤_{2^k}ä¸Šå®ç°å®Œç¾å®‰å…¨çš„ç»é”€å•†è¾…åŠ©LTBitså’ŒMSBæå–åè®®ï¼Œé€šè¿‡å……åˆ†åˆ©ç”¨ç»é”€å•†ç”Ÿæˆä¸°å¯Œç›¸å…³éšæœºæ€§çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æå‡äº†å®‰å…¨æ¯”è¾ƒåè®®çš„åœ¨çº¿é˜¶æ®µæ€§èƒ½ã€‚


<details>
  <summary>Details</summary>
Motivation: å®‰å…¨æ¯”è¾ƒæ˜¯å¤šæ–¹è®¡ç®—ä¸­çš„åŸºç¡€åŸè¯­ï¼Œä½†ç°æœ‰åè®®çš„é¢„å¤„ç†é˜¶æ®µæ€§èƒ½ç“¶é¢ˆä¸¥é‡ï¼Œä¸»è¦æºäºç”Ÿæˆç›¸å…³éšæœºæ€§çš„é«˜æˆæœ¬ã€‚è™½ç„¶è¿‘æœŸæ¡†æ¶å¼•å…¥äº†è¢«åŠ¨ã€éå…±è°‹çš„ç»é”€å•†æ¥åŠ é€Ÿé¢„å¤„ç†ï¼Œä½†ä»å­˜åœ¨ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼š1) ç°æœ‰ç»é”€å•†è¾…åŠ©æ–¹æ³•ä»…å°†ç»é”€å•†ä½œä¸ºä¼ ç»Ÿé¢„å¤„ç†çš„æ›¿ä»£ï¼Œæœªé‡æ–°è®¾è®¡æ¯”è¾ƒåè®®ä»¥ä¼˜åŒ–åœ¨çº¿é˜¶æ®µï¼›2) å¤§å¤šæ•°åè®®é’ˆå¯¹ç‰¹å®šä»£æ•°åŸŸã€æ•Œæ‰‹æ¨¡å‹æˆ–å‚ä¸æ–¹é…ç½®ï¼Œç¼ºä¹å¹¿æ³›é€šç”¨æ€§ã€‚

Method: æå‡ºäº†é¦–ä¸ªç»é”€å•†è¾…åŠ©çš„næ–¹LTBitsï¼ˆå°äºä½ï¼‰å’ŒMSBï¼ˆæœ€é«˜æœ‰æ•ˆä½ï¼‰æå–åè®®ï¼Œæ”¯æŒğ”½_på’Œâ„¤_{2^k}ä¸¤ç§ä»£æ•°åŸŸï¼Œåœ¨åè®®å±‚é¢å®ç°å®Œç¾å®‰å…¨æ€§ã€‚é€šè¿‡å……åˆ†åˆ©ç”¨ç»é”€å•†ç”Ÿæˆä¸°å¯Œç›¸å…³éšæœºæ€§çš„èƒ½åŠ›ï¼Œğ”½_pæ„é€ å®ç°äº†å¸¸æ•°è½®åœ¨çº¿å¤æ‚åº¦ï¼Œâ„¤_{2^k}æ„é€ å®ç°äº†O(log_n k)è½®æ¬¡ä¸”å…·æœ‰å¯è°ƒåˆ†æ”¯å› å­ã€‚æ‰€æœ‰åè®®éƒ½é€šè¿‡æ‰©å±•çš„ABBæ¨¡å‹ä½œä¸ºé»‘ç›’æ„é€ å®ç°ï¼Œç¡®ä¿è·¨MPCåç«¯å’Œæ•Œæ‰‹æ¨¡å‹çš„å¯ç§»æ¤æ€§ã€‚

Result: å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„MPCæ¡†æ¶ç›¸æ¯”ï¼Œæœ¬æ–‡åè®®å®ç°äº†1.79å€åˆ°19.4å€çš„åŠ é€Ÿï¼Œçªæ˜¾äº†åè®®åœ¨æ¯”è¾ƒå¯†é›†å‹MPCåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚

Conclusion: æœ¬æ–‡é€šè¿‡å……åˆ†åˆ©ç”¨ç»é”€å•†çš„èƒ½åŠ›é‡æ–°è®¾è®¡äº†å®‰å…¨æ¯”è¾ƒåè®®ï¼Œè§£å†³äº†ç°æœ‰ç»é”€å•†è¾…åŠ©æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ç°äº†åœ¨ğ”½_på’Œâ„¤_{2^k}ä¸Šçš„é«˜æ•ˆã€é€šç”¨ä¸”å®‰å…¨çš„æ¯”è¾ƒåè®®ï¼Œä¸ºéšç§ä¿æŠ¤æœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æç­‰åº”ç”¨æä¾›äº†é‡è¦çš„æ€§èƒ½æ”¹è¿›ã€‚

Abstract: Secure comparison is a fundamental primitive in multi-party computation, supporting privacy-preserving applications such as machine learning and data analytics. A critical performance bottleneck in comparison protocols is their preprocessing phase, primarily due to the high cost of generating the necessary correlated randomness. Recent frameworks introduce a passive, non-colluding dealer to accelerate preprocessing. However, two key issues still remain. First, existing dealer-assisted approaches treat the dealer as a drop-in replacement for conventional preprocessing without redesigning the comparison protocol to optimize the online phase. Second, most protocols are specialized for particular algebraic domains, adversary models, or party configurations, lacking broad generality. In this work, we present the first dealer-assisted $n$-party LTBits (Less-Than-Bits) and MSB (Most Significant Bit) extraction protocols over both $\mathbb{F}_p$ and $\mathbb{Z}_{2^k}$, achieving perfect security at the protocol level. By fully exploiting the dealer's capability to generate rich correlated randomness, our $\mathbb{F}_p$ construction achieves constant-round online complexity and our $\mathbb{Z}_{2^k}$ construction achieves $O(\log_n k)$ rounds with tunable branching factor. All protocols are formulated as black-box constructions via an extended ABB model, ensuring portability across MPC backends and adversary models. Experimental results demonstrate $1.79\times$ to $19.4\times$ speedups over state-of-the-art MPC frameworks, highlighting the practicality of our protocols for comparison-intensive MPC applications.

</details>


### [14] [AegisSat: Securing AI-Enabled SoC FPGA Satellite Platforms](https://arxiv.org/abs/2602.19777)
*Huimin Li,Vusal Novruzov,Nikhilesh Singh,Lichao Wu,Mohamadreza Rostami,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: æå‡ºAegisSatå®‰å…¨æ¡†æ¶ï¼Œä¿æŠ¤åŸºäºSoC FPGAçš„AIå«æ˜Ÿç³»ç»Ÿå…å—æ¶æ„æ›´æ–°æ”»å‡»ï¼Œç¡®ä¿å¹³å°å®Œæ•´æ€§å’ŒéŸ§æ€§


<details>
  <summary>Details</summary>
Motivation: SoC FPGAåœ¨AIå«æ˜Ÿç³»ç»Ÿä¸­çš„å¹¿æ³›åº”ç”¨å¸¦æ¥äº†å®‰å…¨æŒ‘æˆ˜ï¼Œæ¶æ„æ›´æ–°å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€æœåŠ¡ä¸­æ–­æˆ–ä»»åŠ¡ç»“æœè¢«ç¯¡æ”¹ï¼Œè€Œå¤ªç©ºç¯å¢ƒæ— æ³•ç‰©ç†è®¿é—®ä¸”éœ€é•¿æœŸå¯é è¿è¡Œ

Method: æå‡ºAegisSatç»¼åˆå®‰å…¨æ¡†æ¶ï¼ŒåŒ…å«ï¼š(1)åŸºäºå¯†ç å­¦çš„å®‰å…¨å¯åŠ¨æœºåˆ¶å»ºç«‹å¯ä¿¡è®¡ç®—åŸºï¼›(2)ä¸¥æ ¼çš„è¿è¡Œæ—¶èµ„æºéš”ç¦»ï¼›(3)åœ¨è½¨é‡é…ç½®å’ŒAIæ¨¡å‹æ›´æ–°çš„è®¤è¯ç¨‹åºï¼›(4)å¼ºå¤§çš„å›æ»šèƒ½åŠ›ä»¥æ¢å¤å¯åŠ¨å’Œæ›´æ–°å¤±è´¥

Result: åœ¨å½“ä»£SoC FPGAè®¾å¤‡ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†è¿™äº›æœºåˆ¶çš„é›†æˆå¯è¡Œæ€§ï¼Œå±•ç¤ºäº†é˜²å¾¡æ·±åº¦æ¡†æ¶çš„å®é™…åº”ç”¨

Conclusion: AegisSatæ¡†æ¶å¯¹äºå¤ªç©ºåº”ç”¨è‡³å…³é‡è¦ï¼Œå¢å¼ºäº†åŸºäºSoC FPGAçš„å«æ˜Ÿç³»ç»Ÿçš„å¯ä¿¡åº¦ï¼Œå®ç°äº†åœ¨è½¨å®‰å…¨ã€éŸ§æ€§çš„AIæ“ä½œ

Abstract: The increasing adoption of System-on-Chip Field-Programmable Gate Arrays (SoC FPGAs) in AI-enabled satellite systems, valued for their reconfigurability and in-orbit update capabilities, introduces significant security challenges. Compromised updates can lead to performance degradation, service disruptions, or adversarial manipulation of mission outcomes. To address these risks, this paper proposes a comprehensive security framework, AegisSat. It ensures the integrity and resilience of satellite platforms by (i) integrating cryptographically-based secure boot mechanisms to establish a trusted computing base; (ii) enforcing strict runtime resource isolation; (iii) employing authenticated procedures for in-orbit reconfiguration and AI model updates to prevent unauthorized modifications; and (iv) providing robust rollback capabilities to recover from boot and update failures and maintain system stability. To further support our claims, we conducted experiments demonstrating the integration of these mechanisms on contemporary SoC FPGA devices. This defense-in-depth framework is crucial for space applications, where physical access is impossible and systems must operate reliably over extended periods, thereby enhancing the trustworthiness of SoC FPGA-based satellite systems and enabling secure and resilient AI operations in orbit.

</details>


### [15] [SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models](https://arxiv.org/abs/2602.19818)
*Hillel Ohayon,Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: æå‡ºåŸºäºæœºå™¨å­¦ä¹ çš„è½»é‡çº§æ‰«æå™¨ï¼Œæ— éœ€ç­–ç•¥ç”Ÿæˆæˆ–ä»£ç æ’æ¡©å³å¯æ£€æµ‹æ¶æ„Pickleæ–‡ä»¶ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰æ–¹æ³•


<details>
  <summary>Details</summary>
Motivation: Hugging Faceç­‰æ¨¡å‹ä»“åº“ä½¿ç”¨Python pickleæ ¼å¼åˆ†å‘æœºå™¨å­¦ä¹ å·¥ä»¶ï¼Œåœ¨æ¨¡å‹åŠ è½½æ—¶æš´éœ²è¿œç¨‹ä»£ç æ‰§è¡Œé£é™©ã€‚ç°æœ‰é˜²å¾¡æ–¹æ³•å¦‚PickleBalléœ€è¦å¤æ‚çš„ç³»ç»Ÿè®¾ç½®å’ŒéªŒè¯è‰¯æ€§æ¨¡å‹ï¼Œå¯æ‰©å±•æ€§å’Œæ³›åŒ–æ€§æœ‰é™ã€‚

Method: æå‡ºè½»é‡çº§æœºå™¨å­¦ä¹ æ‰«æå™¨ï¼Œä»Pickleå­—èŠ‚ç ä¸­é™æ€æå–ç»“æ„å’Œè¯­ä¹‰ç‰¹å¾ï¼Œåº”ç”¨ç›‘ç£å’Œæ— ç›‘ç£æ¨¡å‹å°†æ–‡ä»¶åˆ†ç±»ä¸ºè‰¯æ€§æˆ–æ¶æ„ã€‚æ„å»ºå¹¶å‘å¸ƒäº†åŒ…å«727ä¸ªPickleæ–‡ä»¶çš„æ ‡æ³¨æ•°æ®é›†ã€‚

Result: åœ¨è‡ªæœ‰æ•°æ®é›†ä¸Šè¾¾åˆ°90.01% F1åˆ†æ•°ï¼Œä¼˜äºç°æœ‰æ‰«æå™¨ï¼ˆ7.23%-62.75%ï¼‰ï¼›åœ¨PickleBallæ•°æ®ï¼ˆOODï¼‰ä¸Šè¾¾åˆ°81.22% F1åˆ†æ•°ï¼Œä¼˜äºPickleBallæ–¹æ³•ï¼ˆ76.09%ï¼‰ï¼›åœ¨9ä¸ªä¸“é—¨è®¾è®¡çš„é€ƒé¿æ€§æ¶æ„æ¨¡å‹ä¸Šæ­£ç¡®åˆ†ç±»å…¨éƒ¨9ä¸ªï¼Œæ˜¯å”¯ä¸€èƒ½åšåˆ°çš„æ–¹æ³•ã€‚

Conclusion: æ•°æ®é©±åŠ¨çš„æ£€æµ‹æ–¹æ³•å¯ä»¥æœ‰æ•ˆä¸”é€šç”¨åœ°ç¼“è§£åŸºäºPickleçš„æ¨¡å‹æ–‡ä»¶æ”»å‡»ï¼Œæ— éœ€ä¾èµ–ç‰¹å®šåº“çš„ç­–ç•¥ç”Ÿæˆï¼Œå…·æœ‰æ›´å¥½çš„å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

Abstract: Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks.

</details>


### [16] [Quantum approaches to learning parity with noise](https://arxiv.org/abs/2602.19819)
*Daniel Shiu*

Main category: cs.CR

TL;DR: è¯¥è®ºæ–‡æ¢è®¨äº†é‡å­æ–¹æ³•åœ¨è§£å†³LPNé—®é¢˜ä¸Šçš„æ½œåœ¨åº”ç”¨ï¼Œé€šè¿‡äºŒè¿›åˆ¶åŸŸé‚»åŸŸæ„é€ æ¥è¿‘Simonæ‰¿è¯ºçš„å‡½æ•°ï¼Œè¿è¡ŒSimonç®—æ³•ç”Ÿæˆæ–°çš„LPNæ ·æœ¬ï¼Œå¯èƒ½å®ç°é—®é¢˜è§„æ¨¡çš„è¿­ä»£ç¼©å‡ã€‚


<details>
  <summary>Details</summary>
Motivation: LPNé—®é¢˜æ˜¯åé‡å­å¯†ç å­¦ï¼ˆå¦‚HQCå’ŒClassic McElieceï¼‰å®‰å…¨æ€§çš„å…³é”®åŸºç¡€ã€‚ç»å…¸æ”»å‡»æ–¹æ³•ï¼ˆä¿¡æ¯é›†è§£ç ï¼‰å¯¹ç›¸å…³å‚æ•°åŒ–å…·æœ‰æŒ‡æ•°å¤æ‚åº¦ï¼Œå› æ­¤ç ”ç©¶é‡å­æ–¹æ³•æ˜¯å¦æä¾›æ›¿ä»£é€”å¾„å…·æœ‰é‡è¦æ„ä¹‰ã€‚

Method: å—Regevå°†æ ¼é—®é¢˜ä¸éšè—äºŒé¢ä½“å­ç¾¤é—®é¢˜å…³è”çš„å¯å‘ï¼Œä½¿ç”¨äºŒè¿›åˆ¶åŸŸé‚»åŸŸæ„é€ æ¥è¿‘Simonæ‰¿è¯ºçš„å‡½æ•°ï¼Œå…¶å·®å€¼ç­‰äºç§˜å¯†å¥‡å¶å‘é‡ã€‚è¿è¡ŒSimonç®—æ³•ç”Ÿæˆæ–°çš„LPNæ ·æœ¬ï¼Œå¯èƒ½é€šè¿‡å¿½ç•¥å˜é‡å®ç°é—®é¢˜çš„è¿­ä»£ç¼©å‡ã€‚

Result: è®ºæ–‡æœªå£°ç§°è¯¥æ–¹æ³•å¿…ç„¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä½†è¡¨æ˜è¿™äº›é‡å­æ–¹æ³•å€¼å¾—æ·±å…¥ç ”ç©¶ã€‚é€šè¿‡Simonç®—æ³•ç”Ÿæˆæ–°æ ·æœ¬å¯èƒ½ä¸ºLPNé—®é¢˜æä¾›æ–°çš„é‡å­æ”»å‡»é€”å¾„ã€‚

Conclusion: é‡å­æ–¹æ³•ä¸ºLPNé—®é¢˜æä¾›äº†å€¼å¾—æ¢ç´¢çš„æ–°æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡Simonç®—æ³•ç”Ÿæˆæ–°æ ·æœ¬å®ç°é—®é¢˜ç¼©å‡çš„ç­–ç•¥ï¼Œå°½ç®¡å…¶å®é™…ç«äº‰åŠ›å°šéœ€è¿›ä¸€æ­¥ç ”ç©¶éªŒè¯ã€‚

Abstract: The learning parity with noise (LPN) problem is a well-established computational challenge whose difficulty is critical to the security of several post-quantum cryptographic primitives such as HQC and Classic McEliece. Classically, the best-known attacks involve information set decoding methods which are exponential in complexity for parameterisations of interest. In this paper we investigate whether quantum methods might offer alternative approaches. The line of inquiry is inspired by Regev's relating of certain lattice problems to the hidden dihedral subgroup problem. We use neighbourhoods of binary fields to produce a function close to fulfilling Simon's promise with difference equal to the secret parity vector. Although unlikely to recover the secret parity vector directly, running Simon's algorithm essentially produces new LPN samples. This gives the hope that we might be able to produce enough new samples to ignore one or more variables and iteratively reduce the problem.
  We make no claim that these methods will necessarily be competitive with existing approaches, merely that they warrant deeper investigation.

</details>


### [17] [An Explainable Memory Forensics Approach for Malware Analysis](https://arxiv.org/abs/2602.19831)
*Silvia Lucia Sanna,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: æå‡ºä¸€ç§å¯è§£é‡Šçš„AIè¾…åŠ©å†…å­˜å–è¯æ–¹æ³•ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è§£é‡Šå†…å­˜åˆ†æç»“æœå¹¶è‡ªåŠ¨æå–IoCï¼Œåœ¨Windowså’ŒAndroidæ¶æ„è½¯ä»¶ä¸ŠéªŒè¯ï¼Œç»“åˆä¸“å®¶çŸ¥è¯†æé«˜å¯å¤ç°æ€§å’Œé™ä½æ“ä½œå¤æ‚åº¦ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰è‡ªåŠ¨åŒ–æ¶æ„è½¯ä»¶å†…å­˜æ£€æµ‹æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå†…å­˜åˆ†æä»ä¸¥é‡ä¾èµ–ä¸“å®¶å¯¹å¤æ‚å·¥å…·è¾“å‡ºçš„æ£€æŸ¥ï¼Œéœ€è¦æ›´æ™ºèƒ½ã€å¯è§£é‡Šçš„è¾…åŠ©å·¥å…·æ¥æå‡åˆ†ææ•ˆç‡å’Œå¯è®¿é—®æ€§ã€‚

Method: æå‡ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯è§£é‡ŠAIè¾…åŠ©å†…å­˜å–è¯æ–¹æ³•ï¼Œåˆ©ç”¨LLMså°†å†…å­˜åˆ†æç»“æœè½¬åŒ–ä¸ºäººç±»å¯è¯»å½¢å¼ï¼Œè‡ªåŠ¨æå–æœ‰æ„ä¹‰çš„IoCï¼Œæ”¯æŒWindowså’ŒAndroidå¹³å°ï¼Œæ¯”è¾ƒå®Œæ•´RAMè·å–ä¸ç›®æ ‡è¿›ç¨‹å†…å­˜è½¬å‚¨ï¼Œå¹¶è®¾è®¡äººæœºååŒå·¥ä½œæµç¨‹ã€‚

Result: åœ¨æŸäº›æƒ…å†µä¸‹æ£€æµ‹åˆ°çš„IoCæ¯”ç°æœ‰æœ€å…ˆè¿›å·¥å…·æ›´å¤šï¼ŒLLMsèƒ½å¤Ÿè§£é‡Šåˆ†æç»“æœã€å…³è”å–è¯å·¥ä»¶ã€è®ºè¯æ¶æ„è½¯ä»¶åˆ†ç±»ï¼ŒäººæœºååŒå·¥ä½œæµç¨‹æé«˜äº†å¯å¤ç°æ€§å¹¶é™ä½äº†æ“ä½œå¤æ‚åº¦ã€‚

Conclusion: AIé©±åŠ¨çš„å†…å­˜å–è¯æ–¹æ³•å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼ŒLLMsèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒä¸“å®¶å’Œéä¸“å®¶åˆ†æå¸ˆï¼ŒäººæœºååŒå·¥ä½œæµç¨‹å¢å¼ºäº†ç°ä»£æ¶æ„è½¯ä»¶è°ƒæŸ¥çš„å®ç”¨æ€§å’Œå¯æ“ä½œæ€§ã€‚

Abstract: Memory forensics is an effective methodology for analyzing living-off-the-land malware, including threats that employ evasion, obfuscation, anti-analysis, and steganographic techniques. By capturing volatile system state, memory analysis enables the recovery of transient artifacts such as decrypted payloads, executed commands, credentials, and cryptographic keys that are often inaccessible through static or traditional dynamic analysis. While several automated models have been proposed for malware detection from memory, their outputs typically lack interpretability, and memory analysis still relies heavily on expert-driven inspection of complex tool outputs, such as those produced by Volatility. In this paper, we propose an explainable, AI-assisted memory forensics approach that leverages general-purpose large language models (LLMs) to interpret memory analysis outputs in a human-readable form and to automatically extract meaningful Indicators of Compromise (IoCs), in some circumstances detecting more IoCs than current state-of-the-art tools. We apply the proposed methodology to both Windows and Android malware, comparing full RAM acquisition with target-process memory dumping and highlighting their complementary forensic value. Furthermore, we demonstrate how LLMs can support both expert and non-expert analysts by explaining analysis results, correlating artifacts, and justifying malware classifications. Finally, we show that a human-in-the-loop workflow, assisted by LLMs during kernel-assisted setup and analysis, improves reproducibility and reduces operational complexity, thereby reinforcing the practical applicability of AI-driven memory forensics for modern malware investigations.

</details>


### [18] [LLM-enabled Applications Require System-Level Threat Monitoring](https://arxiv.org/abs/2602.19844)
*Yedi Zhang,Haoyu Wang,Xianglin Yang,Jin Song Dong,Jun Sun*

Main category: cs.CR

TL;DR: è¯¥ç«‹åœºè®ºæ–‡ä¸»å¼ å¯¹LLMåº”ç”¨è¿›è¡Œç³»ç»Ÿæ€§å®‰å…¨å¨èƒç›‘æ§ï¼Œå°†å…¶è§†ä¸ºå¯é éƒ¨ç½²çš„å…ˆå†³æ¡ä»¶


<details>
  <summary>Details</summary>
Motivation: LLMä½œä¸ºæ ¸å¿ƒæ¨ç†ç»„ä»¶é‡å¡‘è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼Œä½†å…¶éç¡®å®šæ€§ã€å­¦ä¹ é©±åŠ¨å’Œéš¾ä»¥éªŒè¯çš„ç‰¹æ€§å¼•å…¥äº†æ–°çš„å¯é æ€§æŒ‘æˆ˜å’Œå®‰å…¨æ”»å‡»é¢ï¼Œéœ€è¦ä»äº‹ä»¶å“åº”è§†è§’å¤„ç†è¿™äº›é£é™©

Method: æå‡ºå°†å®‰å…¨å¨èƒç›‘æ§ä½œä¸ºç³»ç»Ÿçº§å¨èƒæ£€æµ‹æœºåˆ¶ï¼Œèƒ½å¤Ÿæ£€æµ‹å’Œæƒ…å¢ƒåŒ–éƒ¨ç½²åçš„å®‰å…¨ç›¸å…³å¼‚å¸¸ï¼Œè¶…è¶Šä¼ ç»Ÿçš„æµ‹è¯•æˆ–æŠ¤æ é˜²å¾¡æ–¹æ³•

Result: è¯†åˆ«å‡ºå¯ä¿¡éƒ¨ç½²çš„ä¸»è¦éšœç¢ä¸æ˜¯è¿›ä¸€æ­¥æé«˜æ¨¡å‹èƒ½åŠ›ï¼Œè€Œæ˜¯å»ºç«‹ç³»ç»Ÿçº§å¨èƒç›‘æ§æœºåˆ¶ï¼Œè¿™ä¸€æ–¹é¢åœ¨æµ‹è¯•æˆ–æŠ¤æ é˜²å¾¡ä¹‹å¤–å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢

Conclusion: ä¸»å¼ å¯¹LLMåº”ç”¨è¿›è¡Œç³»ç»Ÿå’Œå…¨é¢çš„å®‰å…¨å¨èƒç›‘æ§ï¼Œå°†å…¶è§†ä¸ºå¯é æ“ä½œçš„å‰ææ¡ä»¶ï¼Œå¹¶ä¸ºä¸“é—¨çš„äº‹ä»¶å“åº”æ¡†æ¶å¥ å®šåŸºç¡€

Abstract: LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.

</details>


### [19] [Can You Tell It's AI? Human Perception of Synthetic Voices in Vishing Scenarios](https://arxiv.org/abs/2602.20061)
*Zoha Hayat Bhatti,Bakhtawar Ahtisham,Seemal Tausif,Niklas George,Nida ul Habib Bajwa,Mobin Javed*

Main category: cs.CR

TL;DR: å‚ä¸è€…æ— æ³•å¯é åŒºåˆ†AIç”Ÿæˆä¸äººç±»å½•åˆ¶çš„è¯­éŸ³è¯ˆéª—éŸ³é¢‘ï¼Œå‡†ç¡®ç‡ä»…37.5%ï¼Œä½äºéšæœºæ°´å¹³ï¼Œè¡¨æ˜ç°æœ‰å£°éŸ³çœŸå®æ€§å¯å‘å¼åˆ¤æ–­åœ¨å½“ä»£è¯­éŸ³è¯ˆéª—åœºæ™¯ä¸­ä¸å¯é ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå•†ä¸šè¯­éŸ³åˆæˆç³»ç»Ÿçš„å‘å±•ï¼ŒAIç”Ÿæˆçš„è¯­éŸ³è¯ˆéª—(vishing)å˜å¾—é«˜åº¦é€¼çœŸï¼Œå¼•å‘äº†å¤§è§„æ¨¡æ¬ºéª—çš„ç´§è¿«æ‹…å¿§ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šä¸ªä½“åœ¨ç°å®è¯ˆéª—æƒ…å¢ƒä¸­èƒ½å¦å¯é åŒºåˆ†AIç”Ÿæˆä¸äººç±»å½•åˆ¶çš„å£°éŸ³ï¼Œä»¥åŠå…¶åˆ¤æ–­èƒŒåçš„æ„ŸçŸ¥ç­–ç•¥æ˜¯ä»€ä¹ˆã€‚

Method: è¿›è¡Œäº†å—æ§åœ¨çº¿ç ”ç©¶ï¼Œ22åå‚ä¸è€…è¯„ä¼°16ä¸ªè¯­éŸ³è¯ˆéª—é£æ ¼éŸ³é¢‘ç‰‡æ®µï¼ˆ8ä¸ªAIç”Ÿæˆï¼Œ8ä¸ªäººç±»å½•åˆ¶ï¼‰ï¼Œå¯¹æ¯ä¸ªç‰‡æ®µåˆ†ç±»ä¸ºäººç±»æˆ–AIå¹¶æŠ¥å‘Šç½®ä¿¡åº¦ã€‚ä½¿ç”¨ä¿¡å·æ£€æµ‹ç†è®ºåˆ†æåˆ¤åˆ«èƒ½åŠ›ï¼Œå¹¶å¯¹315ä¸ªç¼–ç æ‘˜å½•è¿›è¡Œå®šæ€§åˆ†æã€‚

Result: å‚ä¸è€…è¡¨ç°å¾ˆå·®ï¼šå¹³å‡å‡†ç¡®ç‡37.5%ï¼Œä½äºäºŒå…ƒåˆ†ç±»ä»»åŠ¡çš„éšæœºæ°´å¹³ã€‚åœ¨åˆºæ¿€å±‚é¢ï¼Œè¯¯åˆ†ç±»æ˜¯åŒå‘çš„ï¼š75%çš„AIç”Ÿæˆç‰‡æ®µè¢«å¤šæ•°æ ‡è®°ä¸ºäººç±»ï¼Œ62.5%çš„äººç±»å½•åˆ¶ç‰‡æ®µè¢«å¤šæ•°æ ‡è®°ä¸ºAIã€‚ä¿¡å·æ£€æµ‹ç†è®ºåˆ†ææ˜¾ç¤ºåˆ¤åˆ«èƒ½åŠ›æ¥è¿‘é›¶(d'â‰ˆ0)ï¼Œè¡¨æ˜æ— æ³•å¯é åŒºåˆ†åˆæˆä¸äººç±»å£°éŸ³ã€‚å®šæ€§åˆ†ææ˜¾ç¤ºå‚ä¸è€…ä¾èµ–å‰¯è¯­è¨€å’Œæƒ…æ„Ÿå¯å‘å¼ï¼ˆåœé¡¿ã€å¡«å……è¯ã€å£°éŸ³å˜åŒ–ã€èŠ‚å¥ã€æƒ…æ„Ÿè¡¨è¾¾ï¼‰ï¼Œä½†è¿™äº›ä¼ ç»Ÿä¸äººç±»çœŸå®æ€§ç›¸å…³çš„è¡¨é¢çº¿ç´¢å¸¸è¢«AIæ ·æœ¬å¤åˆ¶ã€‚è¯¯åˆ†ç±»å¸¸ä¼´éšä¸­é«˜ç½®ä¿¡åº¦ï¼Œè¡¨æ˜æ„ŸçŸ¥æ ¡å‡†é”™è¯¯è€Œéä¸ç¡®å®šæ€§ã€‚

Conclusion: åŸºäºå£°éŸ³å¯å‘å¼çš„çœŸå®æ€§åˆ¤æ–­åœ¨å½“ä»£è¯­éŸ³è¯ˆéª—åœºæ™¯ä¸­ä¸å¯é ã€‚ç ”ç©¶ç»“æœå¯¹å®‰å…¨å¹²é¢„ã€ç”¨æˆ·æ•™è‚²å’ŒAIä»‹å¯¼æ¬ºéª—ç¼“è§£å…·æœ‰é‡è¦å¯ç¤ºï¼Œè¡¨æ˜éœ€è¦å¼€å‘æ–°çš„æ£€æµ‹ç­–ç•¥å’Œé˜²æŠ¤æªæ–½æ¥åº”å¯¹é«˜åº¦é€¼çœŸçš„AIç”Ÿæˆè¯­éŸ³å¨èƒã€‚

Abstract: Large Language Models and commercial speech synthesis systems now enable highly realistic AI-generated voice scams (vishing), raising urgent concerns about deception at scale. Yet it remains unclear whether individuals can reliably distinguish AI-generated speech from human-recorded voices in realistic scam contexts and what perceptual strategies underlie their judgments. We conducted a controlled online study in which 22 participants evaluated 16 vishing-style audio clips (8 AI-generated, 8 human-recorded) and classified each as human or AI while reporting confidence. Participants performed poorly: mean accuracy was 37.5%, below chance in a binary classification task. At the stimulus level, misclassification was bidirectional: 75% of AI-generated clips were majority-labeled as human, while 62.5% of human-recorded clips were majority-labeled as AI. Signal Detection Theory analysis revealed near-zero discriminability (d' approx 0), indicating inability to reliably distinguish synthetic from human voices rather than simple response bias. Qualitative analysis of 315 coded excerpts revealed reliance on paralinguistic and emotional heuristics, including pauses, filler words, vocal variability, cadence, and emotional expressiveness. However, these surface-level cues traditionally associated with human authenticity were frequently replicated by AI-generated samples. Misclassifications were often accompanied by moderate to high confidence, suggesting perceptual miscalibration rather than uncertainty. Together, our findings demonstrate that authenticity judgments based on vocal heuristics are unreliable in contemporary vishing scenarios. We discuss implications for security interventions, user education, and AI-mediated deception mitigation.

</details>
